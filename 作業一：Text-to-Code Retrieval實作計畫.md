# 注意事項
1. 實作時注意程式碼模組化並提升可重用性、可讀性
2. 先確保基礎功能性再追求進階功能
	滿足以下條件之一就用git進行備份（commit時描寫完成了什麼里程碑）
	* 基礎功能完成
	* 進階功能完成
3. TF-IDF和BM25的數學模型與向量計算需用Numpy完成，文本預處理時可以呼叫專用函式


# 後續優化方向（進階功能）
1. TF-IDF和BM25都有不同的變種公式，可嘗試套用不同公式進行計算




# 稀疏檢索(Sparse Retrieval) 
說明：
	核心思想是基於關鍵字匹配(Keyword Matching)
	藉由計算文件與查詢間共享了多少**相同**的詞彙，以及這些詞彙的重要性判斷**文件與查詢的相關性**
	會將文件和查詢表示成一個非常高維度、但很稀疏的向量
	向量的維度等於詞彙庫的大小（常見的即One-Hot Encoding，依據詞彙庫大小可能有幾萬甚至幾十萬維）
	稀疏(Sparse)的意思即向量中絕大部分的元素都是0，只有對應到文件中出現過的詞彙的位置才會有非零值

	
## 通用步驟
* 說明
	TF-IDF和BM25在相同的步驟，為了提升程式的重用性，以模組化的方式進行，並於實作TF-IDF和BM25時分別呼叫
	
* 文本預處理(Text Preprocessing)
	* 目標
		將原始、雜亂的文本清理成乾淨、標準化的詞彙單元(Tokens)
		詞彙單元(Token)指的是在進行文本分析時，將一段連續的文字切分成的**最小**有意義單位
	* 作法		
		* 讀取資料
			載入code_snippets.csv裡所有的程式碼片段
			
		* 標準化
			將所有文本轉換為小寫，這樣"Apple"和"apple"會被視為同一個詞
			
		* 標記化
			將句子或程式碼片段切分成一個個Token
			考慮到此處的程式碼為python，以所有非字母、非數字的字元作為分割符號
			另外考慮到運算符號本身存在意義，需要另外保留運算符（+-*/等）作為獨立的Token
			
			額外處理：
				建立語意映射：需要在處理查詢時，建立一個從自然語言詞彙到**程式碼運算符（+-*/等）**的映射規則
					"add", "sum", "plus", "addition" -> +
					"concatenate", "join" -> + (在字串情境下)
					"assign", "set to" -> =
					
				處理查詢：當一個查詢進來時，例如：
					查詢: "calculate the sum of price and tax"
					查詢處理器在進行標記化時，除了切分出['calculate', 'the', 'sum', 'of', 'price', 'and', 'tax']之外還要根據映射規則，額外加入+這個Token
				最終查詢的Tokens:['calculate', 'sum', 'price', 'tax', '+']
		

	* 進階（在確保基礎完成的前提下實作）
		
		* 切割token
			* 說明	
				單一的詞彙有時候會失去上下文的語意
				將詞彙配對在一起可以保留更精確的意義，減少歧義
				
			* 作法
				切割token時新增將多個單字（例如兩個）組合成一個的詞彙單元（n-grams）
				當n=2時，稱之為bigrams（二元語法）
				當n=3時，稱之為trigrams（三元語法）
				依序嘗試加入n=2、n=3
			
		* 詞形還原（查詢擴充）
			* 說明
				考慮到作為查詢尋對象的程式碼可能用各種型態的詞彙作為單字，初始不進行詞形還原，待嘗試提升性能時才嘗試加入
				
				另外考慮到Python的變數和函數名雖然不是完整的句子，但通常會採用有意義的英文單字組合而成（例如get_user_data, process_item）
				詞形還原後的結果是真實的單字（如 retrieve, call, process），這使得它們更有可能直接匹配到程式碼識別碼中的單字
				而詞幹提取產生的非單字詞根（如 retriev）則很難匹配到任何東西
				
			* 作法：當使用者輸入一個詞時，我們同時用原始詞和還原後的詞去搜尋
				1. 接收查詢
					假設使用者輸入plotting
				2. 擴充查詢詞彙
					查詢系統需自動將其擴充為一個詞彙集合{plotting, plot}
				3. 建立查詢向量
					在建立查詢的TF-IDF或BM25向量時，同時考慮plotting和plot這兩個詞彙單元
					可以給予原始詞更高的權重
				4. 進行搜尋
					用這個擴充後的向量去和文件進行匹配。
		
		* 去除停用詞（作為提升效果的備選選項）
			針對這份作業，**不應該去除停用詞**，因為**有些停用詞在python中可能被作為變數使用**
			另外，TF-IDF和BM25這樣的模型，其設計本身就帶有懲罰高頻詞的機制，頻繁出現的詞彙權重會降低
			不過為了以防萬一，**當效果達到瓶頸時*，可以嘗試啟用去除停用詞	
		

* 建立詞彙庫與相關統計數據
	* 說明
		目標是只遍歷一次所有文件，就計算出後續TF-IDF和BM25模型所需要的所有基礎數據
		
	* 準備工作區
		* 說明
			在開始處理任何文件之前，先準備好幾個空的容器用來存放成果
		* 變數	
			* 文件頻率記錄
				用來記錄每個詞彙出現在了多少份不同的文件中
			* 處理後的文件檔案櫃
				用來存放每一份程式碼片段經過**文本預處理**後的結果
			* 總長度計數器
				初始值為0，用來累加所有文件的總詞彙數
			
	* 逐一處理每份文件
		* 說明
			對每一份文件進行預處理
		* 遍歷所有文件，對其進行：
			1. 呼叫**文本預處理**模組，對文本進行預處理
			2. 更新總長度計數器
				數一下這份文件被切成了多少個詞彙單元，把這個數字加到計數器上
			3. 存入處理後的文件檔案櫃
				將切分好的詞彙單元列表存進檔案櫃中，並**標記好它對應的是哪一份原始文件**
			4. 更新文件頻率記錄本
				**一個詞彙在一份文件中是不是最多計一次，重複不算**
				先找出這份文件中不重複的詞彙有哪些
				然後針對每一個不重複的詞彙，在對應的條目+1
				如果這是個全新的詞彙，就在記錄本上新增這個條目，初始化為0後+1

	* 總整理
		* 說明
			當所有文件都處理完畢後，進行最後的整理
			
		* 製作最終詞彙庫
			處理完所有的文件後，文件頻率記錄本上面記錄的所有詞彙就是完整的詞彙庫
			為了方便電腦處理，需要給每個詞彙分配一個獨一無二的編號（例如，從0開始的索引）

		* 計算平均文件長度
			用**總長度計數器**除以**處理的文件總數**
			得到BM25演算法需要的**平均文件長度**

	* 產出成果
		* 說明 
			至此完成適用於TF-IDF以及BM25的準備流程
		* 輸出
			* 詞彙字典(Vocabulary)
			* 文件頻率統計(Document Frequencies)
			* 所有處理好的文件(Tokenized Documents)
			* 平均文件長度(Average Document Length)

## TF-IDF
* 說明
	TF-IDF的核心目標是將每一份**文件**和**查詢**，都轉換成一個能代表**其內容重要性**的數字向量（稀疏矩陣），換句話說就是計算一個詞彙在某份文件中的重要性
	有了這些向量就能計算文件與查詢之間的**相似度**
	利用通用步驟產出的成果，將所有文件和查詢轉換為TF-IDF向量，並計算查詢與文件之間的餘弦相似度，最終找出前10名最相關的文件
	
* 輸入
	* 詞彙字典
	* 文件頻率統計
	* 所有處理好的文件
	* 處理好的查詢（呼叫**文本預處理**模組，對文本進行預處理）

* 實作步驟
	1. 計算IDF
		* 說明
			根據文件頻率統計，為詞彙庫中的每一個詞計算其IDF分數
			IDF衡量的是一個**詞彙t在所有文件中的普遍程度**
			對於詞彙庫中的每一個詞，它都只有一個固定的IDF值
			假設詞彙庫有V個詞，那麼IDF就是一個**長度為V的一維陣列，也就是一個向量**
			**IDF的值與任何特定文件都無關**，只跟這個詞在**整個文件集合中的統計數據**有關
			
		* 作法
			遍歷詞彙庫中的所有詞彙，套用TF-IDF的IDF公式
			$$ IDF_{TF-IDF}(t) = \log\left(\frac{N}{df_t + 1}\right) $$
			其中N是文件總數，df_t是詞彙t的文件頻率
			
	2. 建立文件的TF-IDF向量
		* 說明
			將每一個處理好的文件轉換成一個數字向量
			TF衡量的是一個詞彙t在**某一份特定文件d**中出現的頻率
			為了儲存每一份文件中每一個詞彙的TF值，需要一個二維的結構，也就是一個**矩陣**	
			行(Rows)代表每一份文件，列(Columns)代表詞彙庫中的每一個詞彙
			矩陣中在(d, t)位置的值，就是**詞彙t在文件d中的TF值**
			假設有N份文件和V個詞彙，那麼TF就會是一個NxV的矩陣

		* 作法
			a. 建立一個大小為**文件總數x詞彙庫大小**的NumPy零矩陣
			b. 遍歷每一份處理好的文件，計算其中每個詞彙的詞頻(TF)
			c. 對於文件中的每個詞，其TF-IDF分數為**TF*IDF**
				將TF矩陣和IDF向量進行元素級的乘法
				這個操作會將IDF向量中的每一個值，分別乘到TF矩陣對應的每一列上
				最終得到的TF-IDF結果，也是一個和TF矩陣大小相同的**NxV矩陣**


	3. 處理查詢並計算相似度
		* 說明
			遍歷每一個查詢，將其轉換為TF-IDF向量，並與所有文件向量計算餘弦相似度
		* 作法：
			針對每一個查詢，重複以下步驟：
			a. 將查詢也轉換成一個TF-IDF向量
			b. 使用NumPy計算這個查詢向量與所有文件向量（上一步建立的TF-IDF矩陣）之間的餘弦相似度，這會涉及到向量的點積和**範數(norm)**計算
			c. 根據相似度分數對所有文件進行降序排序
			d. 選取分數最高的前**10**個文件的ID

	4. 輸出
		針對每個查詢，輸出一個包含前10個最相關文件ID的列表

## BM25
* 說明
	BM25的意義和TF-IDF 一樣，都是用來計算一個查詢(Query)和一份文件(Document)之間的相關性分數
	不同之處在於，BM25進行了2處改進：
		1.  解決詞頻無限膨脹的問題 => 詞頻飽和度
			在TF-IDF中，一個詞彙在一份文件中出現的次數越多（TF越高），它的分數貢獻就越大，這個增長可以是線性的，也可以是對數的，這在現實中並不完全合理
			BM25認為一個詞彙的出現次數對文件相關性的貢獻有一個上限，或者說會**飽和**
			例如一個詞從出現1次變成5次，對文件的相關性有巨大提升
			但如果一個詞從出現50次變成100次，雖然次數翻倍，但對**這份文件就是在講這個主題**的判斷來說提升效果已經非常微弱了
		2. 文件長度正規化
			長文件因為包含的詞彙更多，更容易獲得更高的原始分數，因此需要進行正規化
			TF-IDF的正規化方式（例如，除以文件總詞數）有時候過於粗暴，可能會過度懲罰那些內容豐富的長文件
			BM25採用了一種更聰明、更靈活的正規化方法
			它不只看文件本身有多長，而是看**這份文件的長度與所有文件的平均長度**之間的關係
			如果一份文件比平均長度長很多，BM25會對它進行懲罰，因為它有更多機會**偶然**匹配到查詢中的詞彙
			如果一份文件比平均長度短，懲罰就會減輕，甚至沒有		
	總之，BM25針對詞頻的分數貢獻可以無限增加以及文件長度正規化的公平性進行了調整

* 目標
	根據BM25公式，為每個查詢計算所有文件的相關性分數，並找出前10名
	
* 輸入
	* 詞彙字典
	* 文件頻率統計 
	* 所有處理好的文件，以及它們各自的長度
	* 平均文件長度 
	* 處理好的查詢（呼叫**文本預處理**模組，對文本進行預處理）

* 實作步驟
	1. 計算IDF 
		* 說明
			與TF-IDF類似，但使用BM25專用的IDF公式
		* 作法
			遍歷詞彙庫，套用BM25的IDF公式
			$$ IDF_{BM25}(t) = \log\left(\frac{N - df_t + 0.5}{df_t + 0.5} + 1\right) $$

	2. 處理查詢並計算文件分數
		* 說明
			遍歷每一個查詢，再針對查詢中的每個詞計算它對所有文件的分數貢獻，最後加總

		* 作法：針對每一個查詢，重複以下步驟
			a. 準備一個列表或字典，用來存放每個文件的累計分數，初始值皆為0
			b. 遍歷查詢中的每一個詞彙q_i
			c. 取得q_i的BM25 IDF分數（該詞彙在**整個文件集合**中的重要性）
			d. 遍歷所有文件d，計算q_i在文件d中的詞頻f(q_i, d)
			e. 套用完整的BM25評分公式的右半部分，計算q_i對文件d的分數貢獻。別忘了需要文件d的長度|d|和平均文件長度avgdl
				換句話說就是結合q_i的IDF分數以及其在文件d中出現的次數以及文件d的長度，計算開詞彙對於其在BM25分數的貢獻
				綜合一個查詢詞彙q_i的4個方面的資訊：
					全域稀有度：
						這個詞彙q_i有多稀有？(由IDF分數提供)
					局部重要性：
						這個詞彙q_i在這份文件d中出現了幾次？(由詞頻提供)
					文件長度特徵：
						這份文件d本身有多長？它的長度跟所有文件的平均長度比起來是長還是短？
					演算法調控：
						我們希望詞頻飽和度多快？文件長度的影響多大？(由k1和b兩個參數控制)
			f. 將IDF*分數貢獻的結果，累加到文件d的總分上
			g. 當查詢中的所有詞彙都處理完畢後，就得到了每個文件對於該查詢的最終BM25分數
			h. 根據最終分數對所有文件進行降序排序，選出前10名
	3. 輸出
		針對每個查詢，輸出一個包含前10個最相關文件ID的列表


# 密集檢索(Dense Retrieval)
	說明：
		基於深度學習的現代檢索方法，核心思想是基於**語義相似度匹配(Semantic Similarity Matching)**
		不關心**字面上**的詞彙是否完全一樣，而是去**理解查詢和文件背後的意思是否相近**

		它使用神經網路模型(如CodeBERT)將文件和查詢轉換成一個維度相對較低（通常是幾百到一千多維）但很密集的向量，這個向量被稱為嵌入(Embedding)
		密集(Dense)的意思是向量中幾乎所有的元素都是非零的浮點數，每個維度都捕捉了輸入文本某個層面的語義特徵
		選用mean pooling over last hidden state作為文件與查詢的最終embedding，以獲得較穩定的語義表示


	* 作法：
		使用一個強大的預訓練語言模型（暫定使用CodeBERT，優化時可嘗試修改），將查詢和程式碼進行轉換（稱編）成一個低維度、密集的數字向量，稱為嵌入(Embedding)
		在由這些向量構成的語義空間中，只要兩個向量在空間中的位置相近（例如，餘弦相似度高），就認為它們在語義上是相關的
		
		由於作業要求實作並比較兩種密集檢索的方式 ：
			直接使用預訓練模型
			使用train_queries.csv對預訓練模型進行微調
		因此分兩部分進行實作
		
		* 第一部分：直接使用預訓練模型
			* 說明
				此部分的程式碼亦需要模組化，可以在使用預訓練模型、使用微調後的模型時重複使用
				為了方便其在預訓練模型以及微調後的模型間切換，此部分的模組需要能夠在呼叫時選擇要使用的模組
			1. 建立文件的語義索引(Encoding Documents)
				* 目標
					將code_snippets.csv中的每一份程式碼片段，都轉換成一個語義向量(Embedding)

				* 作法
					* 載入預訓練模型
						選擇一個適合的預訓練模型（暫定使用CodeBERT，優化時可嘗試修改），並使用Hugging Face Transformers的函式庫將其載入
					* 遍歷所有文件並進行編碼
						一次讀取一份程式碼片段，將程式碼片段的文字輸入到模型中
						模型會輸出一組數字，通常是一個固定長度（例如768維）的密集向量，這就是這份程式碼的語義表示
						將所有程式碼片段轉換後的向量儲存在一個巨大的NumPy矩陣中
						這個矩陣就是語義索引，大小會是文件總數x向量維度

			2. 處理查詢並進行檢索(Retrieval)
				* 目標
					將使用者查詢轉也換為向量，並在語義索引中尋找最相似的文件向量

				* 作法：針對每一個查詢，重複以下步驟
					1. 編碼查詢
						使用與**建立文件的語義索引**時完全相同的模型，將查詢的自然語言文字也轉換成一個相同維度的向量
					2. 計算相似度
						使用NumPy計算這個查詢向量與語義索引矩陣中所有文件向量之間的餘弦相似度
					3. 排序與輸出
						根據相似度分數對所有文件進行降序排序，並選取分數最高的前10個文件的ID
						對每個查詢，若Top-1 中包含正確的code_id則計1，否則計0；最後平均即Recall@10
						
		* 第二部分：微調(Fine-tuning)預訓練模型
			* 目標
				利用train_queries.csv訓練資料讓通用的預訓練模型變成一個能理解當前任務的專家模型
				目標是讓模型學會將相關的查詢和程式碼，在語義空間中拉得更近
				
			1. 準備訓練資料與模型
				* 說明			
					需要給模型三種類型的樣本，稱為三元組(Triplet)： 
						錨點(Anchor)：一個來自train_queries.csv的查詢
						正樣本(Positive)：與該查詢配對的、正確的程式碼片段 
						負樣本(Negative)：從code_snippets.csv中**隨機挑選的與該查詢不相關的程式碼片段**
				* 作法
					1. 建立三元組
						讀取train_queries.csv，為每一個(查詢, 正確程式碼)的配對
						利用hard negative mining（選相似度較高但實際錯誤的程式碼）挑選一個或多個不相關的程式碼作為負樣本，組成訓練用的三元組
					2. 設定模型與損失函數
						載入預訓練模型、選擇一個適合對比學習的損失函數
						此處使用TripletLoss作為Loss Function
						這個損失函數的目標是讓錨點和正樣本的距離遠小於錨點和負樣本的距離

			2. 進行模型微調
				* 作法
					1. 設定訓練迴圈
						使用PyTorch深度學習框架
					2. 模型訓練
						在每一個訓練步驟中，將一批三元組（錨點、正樣本、負樣本）輸入模型，得到它們各自的嵌入向量
					3. 計算損失
						使用TripletLoss函數計算當前這批預測結果的錯誤程度
					4. 反向傳播與優化
						根據計算出的損失更新模型的內部權重，讓模型朝著將**正樣本拉近，將負樣本推遠**的方向進行微小的調整
					5. 重複訓練
						重複以上步驟，直到模型在驗證集上的表現不再提升為止
					6. 儲存模型
						將微調好新模型儲存起來
						
			3. 使用微調後的模型進行檢索
				* 作法
					呼叫第一部分的模組進行檢索
					須注意此次載入的是微調後的專家模型

# Kaggle submission格式
	需要輸出成 Kaggle要求的CSV格式（query_id, code_id），以及確保submission 500*2格式正確
	
	
# 評估Recall@10
	* 說明
		無論稀疏檢索或密集檢索都需要進行算法的性能評估
		此處採用Recall@10指標
		這個10代表只看前10個檢索結果
		如果正確答案在前10個結果裡 => 這個查詢算命中(+1)
		如果正確答案不在前10個結果裡 => 這個查詢算沒命中(+0)
		最終計算Recall@10 = (命中的查詢數​)/(查詢總數)


# 回答三個問題
1. 在稀疏檢索方法中，比較TF-IDF與BM25的檢索效能。哪一種方法在這份作業中表現得更好？請分析造成差異的可能原因（例如：詞頻處理方式、文件長度正規化) 
	(In Sparse Retrieval methods, compare the retrieval performance of TF-IDF and BM25. Which method performs better in this assignment? Analyze the possible reasons behind the difference (e.g., term frequency handling, document length normalization))

2. 在密集檢索方法中，比較「直接使用預訓練模型」與「利用訓練資料進行微調」的效能差異。哪一種方法表現得更好？請解釋造成差異的可能原因。 
	(In Dense Retrieval methods, compare the performance of using a pre-trained model directly versus fine-tuning with training data. Which approach performs better? Explain the possible reasons for the difference.)

3. 在文字到程式碼檢索的任務中，比較稀疏檢索與密集檢索之間的差異與效能表現。除了這些方法之外，還有哪些其他方法（例如：檢索再排序Retrieve-and-Re-rank）可以進一步提升檢索效能？
	(In the Text-to-Code Retrieval task, compare the differences and performance between Sparse Retrieval and Dense Retrieval. Beyond these approaches, what other methods (e.g., Retrieve-and-Re-rank) could further improve retrieval performance?)





