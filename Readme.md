# 文字到程式碼檢索實作

本專案旨在實作一個文字到程式碼的檢索系統並比較不同的方法，其中包含了稀疏檢索（TF-IDF、BM25）和密集檢索（CodeBERT）兩種方法。

## 專案結構

```
.
├── preprocess.py
├── sparse_retrieval.py
├── dense_retrieval.py
├── fine_tune_model.py
├── prepare_hard_negatives.py
├── hybrid_retrieval.py
├── self_evaluation.py
├── generate_submissions.py
├── answers.md
├── 強化策略.md
├── requirements.txt
└── Readme.md
```

*   `code_snippets.csv`: 包含所有程式碼片段的語料庫  
*   `train_queries.csv`: 用於微調密集檢索模型的訓練資料（此次亦用於檢索器性能的驗證）  
*   `test_queries.csv`: 用於生成Kaggle提交檔案的測試查詢  
*   `preprocess.py`: 包含資料前處理的相關函式  
*   `sparse_retrieval.py`: 實現了 TF-IDF 和 BM25 稀疏檢索模型  
*   `dense_retrieval.py`: 主要用於**評估**密集檢索模型（原始與微調後）的腳本。可以載入不同的模型，並在本地驗證集上測試其性能  
*   `fine_tune_model.py`: 核心的**模型訓練**腳本。實現了使用「困難負樣本挖掘」策略對 `unixcoder-base` 模型進行微調的完整流程  
*   `prepare_hard_negatives.py`: 用於生成困難負樣本的預處理腳本。它會使用 TF-IDF 為訓練集中的每個查詢，從主語料庫中找出最相似的 Top-50 樣本  
*   `hybrid_retrieval.py`: 實現了 RRF (Reciprocal Rank Fusion) 混合檢索策略的腳本。包含**本地驗證**和**生成提交檔案**兩種模式  
*   `self_evaluation.py`: 用於在`train_queries.csv`資料上對**稀疏模型**進行本地驗證  
*   `generate_submissions.py`: 用於產生所有模型（除hybrid外）的最終Kaggle提交檔案  
*   `answers.md`: 針對作業要求的三個問題的詳細回答  
*   `強化策略.md`: 記錄了專案優化過程中不同強化策略的規劃與思考  
*   `requirements.txt`: 專案所需的Python套件  
*   `Readme.md`: 本文件  

## 安裝相依套件

```bash
pip install -r requirements.txt
```

## 執行流程

1.  **（可選）本地驗證稀疏模型**:
    ```bash
    python self_evaluation.py
    ```
    此腳本會使用 `train_queries.csv` 的資料來評估 TF-IDF 和 BM25 的效能。

2.  **訓練並驗證密集模型**:
    ```bash
    python dense_retrieval.py
    ```
    此腳本會載入預訓練的 CodeBERT，在驗證集上進行評估，然後進行微調，並儲存驗證集上表現最好的模型到 `./fine_tuned_codebert`。

3.  **產生最終提交檔案**:
    ```bash
    python generate_submissions.py
    ```
    此腳本會使用所有訓練好的模型，針對 `test_queries.csv` 產生所有四個提交檔案。

## 本地驗證與分析
由於提供的資料中只有用於微調dense model的train_queries，沒有提供驗證資料用於本地的效能驗證
因此此處用train_queries製作了用於本地驗證的驗證資料（用其中所有的code片段做語料庫，並嘗試用query查詢），用以對比Sparse檢索器的效能


### TF-IDF vs. BM25 性能分析
在一個使用`train_queries.csv`作為語料庫和查詢集的本地驗證中，得到了以下結果：
*   **TF-IDF Recall@10: 0.7400**
*   **BM25 Recall@10: 0.6680**

這個結果顯示，在這個特定的數據集上，TF-IDF的表現優於BM25。
這與一般的預期可能有所不同，可能的原因如下：
1.  **文件長度相對一致**
    BM25的文件長度正規化對於文件長度差異較大的語料庫特別有效。
    如果`train_queries.csv`中的程式碼片段長度都差不多，那麼BM25的這個優勢就無法發揮，甚至可能因為不當的懲罰而導致性能下降。
2.  **查詢較短或關鍵字單一**
    BM25的詞頻飽和度機制對於較長的查詢更有幫助。
    如果查詢本身很短，或者只包含少數幾個關鍵字，那麼詞頻的影響可能不是主要因素，TF-IDF的簡單加權方式可能反而更有效。
3.  **預設參數非最佳化**
    BM25有兩個可調參數`k1`和`b`，此處使用的是通用的預設值（k1=1.5, b=0.75）。
    這些參數可能不是這個特定數據集的最佳選擇。
    TF-IDF沒有需要調整的參數，因此不存在這個問題。

### BM25 參數調整
為了嘗試提升BM25的性能，有嘗試對`k1`和`b`參數進行了調整。以下是實驗結果：
| k1  | b    | Recall@10 |
| --- | ---- | --------- |
| 1.2 | 0.6  | 0.6580    |
| 1.2 | 0.75 | 0.6660    |
| 1.2 | 0.9  | 0.6620    |
| 1.5 | 0.6  | 0.6640    |
| 1.5 | 0.75 | 0.6680    |
| 1.5 | 0.9  | 0.6680    |
| 2.0 | 0.6  | 0.6700    |
| 2.0 | 0.75 | 0.6740    |
| 2.0 | 0.9  | 0.6780    |

最佳結果是在`k1=2.0`和`b=0.9`時，Recall@10達到了 **0.6780**。
雖然比預設參數的0.6680 有所提升，但仍然低於TF-IDF的 0.7400。
這進一步證實了，在這個特定的自建驗證集上，TF-IDF 是表現最好的稀疏檢索模型。

### N-gram 實驗
嘗試在斷詞時加入N-gram(bigrams 和 trigrams)來捕捉更多的上下文資訊。以下是自建驗證集上的結果：
*   **Unigrams (1-gram)**:
    *   TF-IDF Recall@10: 0.7380
    *   BM25 Recall@10: 0.6680
*   **Bigrams (1-gram + 2-gram)**:
    *   TF-IDF Recall@10: 0.7300
    *   BM25 Recall@10: 0.6700
*   **Trigrams (1-gram + 2-gram + 3-gram)**:
    *   TF-IDF Recall@10: 0.7320
    *   BM25 Recall@10: 0.6700
從結果來看，加入N-gram並沒有提升稀疏模型的性能，反而略有下降。
這可能表示對於這個數據集，額外的上下文資訊並沒有帶來好處，甚至可能引入了噪音。

### 文字預處理優化：查詢擴展 (Query Expansion)
嘗試了使用詞形還原（Lemmatization）來進行查詢擴展。
當查詢進入時，將**原始查詢詞和其詞形還原後的詞**都納入考量，以期捕捉更多相關的程式碼片段，以下是本地驗證集上的結果：
*   **Unigrams (無查詢擴展)**:
    *   TF-IDF Recall@10: 0.7400
    *   BM25 Recall@10: 0.6680
*   **Unigrams (有查詢擴展)**:
    *   TF-IDF Recall@10: 0.7480
    *   BM25 Recall@10: 0.6720

從結果來看，查詢擴展對TF-IDF和BM25的性能都有輕微的提升。
TF-IDF從0.7400提升到0.7480，BM25從0.6680提升到0.6720。
這表明詞形還原在一定程度上幫助模型捕捉了更多的語義相關性。

### TF-IDF 優化：次線性詞頻縮放 (Sublinear TF Scaling)
將詞頻 (TF) 的計算方式從原本的 `tf` (原始次數) 修改為 `1 + log(tf)` 
這種次線性縮放的策略旨在平滑化詞頻的影響，避免在長文件中頻繁出現的詞彙獲得過高的權重  

改動後發現模型在本地驗證集有了顯著的性能提升：

| 模型 | 原始 TF Recall@10 | 次線性 TF Recall@10 | 提升 |
| --- | --- | --- | --- |
| TF-IDF (基礎) | 0.7400 | **0.7660** | +2.6% |
| TF-IDF (含查詢擴充) | 0.7480 | **0.7860** | +3.8% |  

這證明了「收益遞減」的詞頻計算方式更適合此資料集，它有效地降低了常見詞的噪音，讓模型的檢索結果更為精準。

**Kaggle Score**: 這次優化後的TF-IDF模型提交至Kaggle，獲得了 **0.73600** 的分數，成功超越了0.72000的Strong Baseline。

### 密集檢索模型實驗 (Dense Model Experiments)

#### 嘗試一：使用驗證集分割策略
最初嘗試將 `train_queries.csv` 的10%作為驗證集，並儲存驗證集上`Recall@10`分數最高的模型  
| 模型 | Validation Recall@10 |
| --- | --- |
| 預訓練 CodeBERT | 0.2600 |
| **微調後 CodeBERT (Epoch 1)** | **1.0000** |  

**分析**:
*   **異常分數分析**: 在驗證集上出現 `1.0000` 的完美分數是一個**異常信號**，它並不代表模型的真實性能。其主要原因是**驗證方法存在缺陷**
    驗證集僅有50筆資料，且此處設計的機制會儲存訓練過程中遇到的本地驗證分數最高的權重，這使得很容易出現答案全對但模型不是我們想要的情況
*   **結論**
    回歸到Dense Retrieval中模型的功能，這邊引入語言模型的功能主要是**特徵提取**，因此微調模型參數時不應該用切割出驗證集的方式驗證模型當前表現  
    這邊應該用類似TripletMarginLoss的方式，藉由分析能否將輸入的query的特徵靠近正樣本，遠離負樣本，評估模型當前的表現  
    或者可以另外監控餘弦相似度分佈（正樣本 vs. 負樣本），看區分度是否隨訓練提升

    PS：  
    後來我發現，雖然不能藉由分割驗證集的方式評估訓練中模型的性能並提取最佳參數，但或許可以先取90%的資料用來訓練，然後用100%的code片段作為被查詢的資料庫，用剩餘10%的query進行查詢，評估模型的性能  
    只要不是用評估分數來選權重，只做為性能的評估，應當是可行的  
    **但是由於切出的資料集過小，只用於性能的粗略評估，不用於權重的調整**
    PSPS：  
        後來我又發現，在訓練時似乎把test_queries的語料庫code_snippets拿來用於訓練好的模型評估了，經檢測，train_queries裡面的code片與code_snippets應該是完全不重合的，所以前面對於dense model的評估似乎變得不太可靠


│       
  

#### 嘗試二：回歸模型本質，使用完整資料集訓練

**分析與結論**:
基於「嘗試一」的經驗，認識到在缺乏大規模、高品質驗證集的情況下，追求一個小型、自製驗證集上的最高分是錯誤的。  
語言模型在此任務中的核心功能是**特徵提取**，即為查詢和程式碼產生有意義的語義向量

因此，更可靠的訓練策略是：  
1.  **使用全部訓練資料**
    利用所有500筆`train_queries.csv`的資料對模型進行微調，讓模型看到盡可能多的範例。
2.  **觀察訓練損失 (Training Loss)**
    在沒有可靠驗證集的情況下，應將`TripletMarginLoss`的穩定下降作為模型正在有效學習的主要指標。
    或者可以另外監控餘弦相似度分佈（正樣本 vs. 負樣本），看區分度是否隨訓練提升。
    這代表模型正成功地將「相關的 (查詢, 程式碼) 對」在向量空間中拉近，並將「不相關的」推遠。

### 嘗試三：更換基底Bi-Encoder模型
為了尋找更強大的基底模型，並驗證本地評估機制，對不同的預訓練模型進行了微調，並以嘗試以**本地驗證集**和**Kaggle 公開分數**的表現作為雙重指標進行評估
| 模型 (Model) | 本地分數 (Local Score) | Kaggle 分數 (Kaggle Score) | 排名 (Kaggle/本地) |
| :--- | :--- | :--- | :--- |
| **`microsoft/unixcoder-base`** | **0.88000** | **0.85200** | **1 / 1** |
| `microsoft/graphcodebert-base` | 0.80000 | 0.75200 | 2 / 2 |
| `microsoft/codebert-base` | 0.78000 | 0.65200 | 3 / 3 |
| `Salesforce/codet5p-220m` | 0.70000 | 0.60800 | 4 / 4 |  

#### 實驗結論與分析
1.  **模型選擇**
    實驗結果非常明確，`microsoft/unixcoder-base`無論在本地還是 Kaggle 評分中都表現最佳，是當前任務的首選基底模型。後續的進階實驗應圍繞此模型展開
2.  **本地驗證機制的有效性**
    更重要的是，本次實驗成功驗證了**本地驗證機制的可靠性**  
    四個模型在本地的性能排名與在Kaggle 上的排名**完全一致**，呈現了完美的排名相關性(Rank Correlation)  
    這表明，本地驗證的結果是具有一定參考度的，能用來指導模型選擇和參數調整，從而大幅減少提交到Kaggle的次數、加快迭代速度  


### 嘗試四：困難負樣本挖掘(Hard Negative Mining)

基於嘗試三找到的最佳模型 `microsoft/unixcoder-base`，進一步採用了「困難負樣本挖掘」策略進行微調  
此策略藉由篩選相似但錯誤的負樣本，讓模型學習將相似但錯誤的樣本與正確樣本區分，進而提高檢索性能
首先使用TF-IDF模型為每個訓練查詢從主語料庫(`code_snippets.csv`)中找出Top-50的相似（但錯誤的）程式碼作為困難負樣本  
然後在訓練的每一步，從這些困難樣本中隨機挑選一個與正樣本進行對比學習

**結果**:

| 模型 (Model) | 本地分數 (Local Score) | Kaggle 分數 (Kaggle Score) |
| :--- | :--- | :--- |
| `unixcoder-base` (隨機負樣本) | 0.88000 | 0.85200 |
| **`unixcoder-base` (困難負樣本)** | **0.96000** | **0.87200** |

**結果分析**:
*   Kaggle 分數從 `0.85200` 提升至 **`0.87200`**，證明了讓模型專注於學習與正確答案相似的「困難」樣本，能有效提升其在模糊查詢下的分辨能力
*   本地分數達到了 `0.96000` 的高點，雖然與 Kaggle 分數存在差距，但再次驗證了建立的本地評估機制的趨勢預測能力是可靠的



### 嘗試五：混合檢索 RRF (Reciprocal Rank Fusion)
在分別獲得了最強的稀疏模型（TF-IDF 優化版）和密集模型（Unixcoder + 困難負樣本挖掘）後，嘗試使用更穩健的RRF演算法將兩者結合，以期獲得超越兩者的更強性能
在本地驗證集上融合了TF-IDF (`~0.74`)和密集模型(`0.96`)的Top-100排名結果

**結果**:
| 模型 (Model) | 本地分數 (Local Recall@10) |
| :--- | :--- |
| TF-IDF (優化版) | ~0.7400 |
| Dense (Hard-Neg) | **0.9600** |
| **RRF 混合模型** | 0.8400 |

**結果分析**:
出乎意料地，RRF 混合模型的最終得分 (`0.8400`) 不僅沒有超越密集模型，反而比其單獨表現 (`0.9600`) 下降了許多  
它揭示了混合策略的一個重要前提：**只有當多個模型實力相近或彼此互補時，融合才能帶來正面效益。**

在先前的嘗試中，藉由更換模型以及困難負樣本挖掘，密集模型的性能在本地驗證集上已經達到了`0.9600`  
與之相比，TF-IDF即使經過優化，其分數仍在 `~0.7400` ，將它與密集模型結合在RRF看來更像是噪音而非有益的補充
因此，融合一個相對弱很多的模型反而干擾了強模型的精準判斷，導致了性能下降

### 嘗試六：檢索再排序(Retrieve-and-Re-rank)
一階段（召回）使用經過困難負樣本微調的Bi-Encoder模型，第二階段（精排）則使用 Cross-Encoder 模型

**1. 使用預訓練 Cross-Encoder**
首先使用了一個在通用文本上預訓練的 `cross-encoder/ms-marco-MiniLM-L-6-v2`作為精排器  
*   **結果**：本地驗證分數為 **`0.8800`**，低於召回器單獨的 `0.9600`  
*   **分析**：這或許是因為通用的預訓練Cross-Encoder不理解程式碼檢索這個特定領域的細微之處，它的介入反而干擾了Bi-Encoder的判斷  


**2. 微調 Cross-Encoder**  
為解決領域不匹配問題，接著嘗試利用數據集對Cross-Encoder進行微調  
嘗試擴增了訓練數據（1個正樣本對2個困難負樣本），並訓練了3個 Epoch  
*   **結果**：經過微調後，在本地驗證集上的分數依然是**`0.8800`**，沒有任何提升  
*   **分析**：最終結論是，用來微調的數據量（總計約1500個樣本對）對於一個大型預訓練模型來說仍然太小，不足以讓它在原有強大但通用的基礎上，學會關於程式碼領域、有意義的新知識  

### 實驗七：困難負樣本採樣策略探索（Kaggle 分數更新版）

在確定了 `microsoft/unixcoder-base` 作為最佳基底模型後，本節旨在探索不同的困難負樣本採樣策略，以找到最能提升模型分辨能力的訓練方法。

#### 單一負樣本策略 (N=1)

為每個正樣本嘗試以不同方式抽取1個負樣本組成三元祖。

| 策略 (Strategy) | 本地分數 (Local Score) | Kaggle 分數 (Kaggle Score) |
| :--- | :--- | :--- |
| 從 Top 50 隨機抽樣 | **0.9600** | 0.87200 |
| 僅使用最難樣本 (Top 1) | 0.9400 | 0.90800 |
| **從 Top 5 隨機抽樣** | 0.9400 | **0.92800** |

#### 多重負樣本策略 (N=4)

此策略透過數據增強，為同一個 `(查詢, 正樣本)` 配對多次不同的負樣本，在整個 Epoch 中強化模型對多樣化困難樣本的應對能力。

| 策略 (Strategy) | 本地分數 (Local Score)| Kaggle 分數 (Kaggle Score) | 
| :--- | :--- | :--- |
| 從 Top 50 隨機抽樣 | **0.9800** | 0.89200 | 
| **分層抽樣** | **0.9800** | **0.92400** |
| 只取最難的 Top 4 | 0.8600 | 0.84400 |



### 總結
原本我的計畫是利用分割出訓練的10%query作為本地模型的評估，得到相對好的分數並確認在Kaggle上也能得到不錯的分數後，再將全部的query丟給模型微調  
不過後來我發現在進行嘗試七的時候由於我將利用TF-IDF篩選出的Top 50的困難負樣本生成成了一個JSON檔案，便於微調的時候直接讀取，結果之後忘記把JSON中毒取出來的檔案分割  
這導致這部分的嘗試在進行本地驗證的時候，用於評估的資料是模型看過的，這或許也是此處本地分數排名和Kaggle分數排名出現不一致的原因  
考慮到這部分影響的只是本地驗證的效果，對於模型本身並無影響，且本身最後就是要訓練使用所有問題進行微調的模型，因此此處沒有對此嘗試進行修正  
最終在`fine_tune_model.py`中訓練模型的腳本被我改成了可從Kaggle上分數最高的兩種負採樣策略選一種進行模型微調  


### 補充
後續為了繼續提高Kaggle分數，嘗試在相同架後下更換模型或參數  
此處只記錄使用的模型與參數
| 模型| epochs | num_layers | lr | batch_size | 策略 | Kaggle 分數 (Kaggle Score) |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| 'microsoft/unixcoder-base'| 4 | 5 | 2e-5 | 8 |top5_single | |0.92800| 
| 'microsoft/unixcoder-base'| 4 | 3 | 5e-5 | 8 |top5_single |0.92400| 
| 'microsoft/unixcoder-base'| 4 | 3 | 2e-5 | 4 |top5_single |0.92800| 
| 'sentence-transformers/multi-qa-mpnet-base-dot-v1' | 4 | 3 | 2e-5 | 8 |top5_single | 0.86400 | 
| 'microsoft/unixcoder-base'| 1 | 5 | 2e-5 | 8 |top5_single |**0.94400**| 
| 'microsoft/unixcoder-base'| 1 | 6 | 2e-5 | 8 |top5_single |0.92000| 
| 'microsoft/unixcoder-base'| 1 | 3 | 2e-5 | 8 |top5_single |0.93600| 
| 'microsoft/unixcoder-base'| 1 | 5 | 2e-5 | 8 |stratified_multi|0.93200| 



1. num_layers  
    num_layers代表的是使用模型中最後幾層的輸出合成輸入文字的特徵向量  
    選擇越多層會保留越多的細節特徵  
2. 策略  
    top5_single：從Top5中的困難負樣本隨機選一個
    stratified_multi：從1-10、11-20、21-30、31-40的困難負樣本中各層隨機取一個與正樣本組成一組data（共4組）

####　Iterative Hard Negative Mining（迭代式困難負樣本挖掘）
基於上方參數調整後得到的最佳參數組合：模型='microsoft/unixcoder-base'，num_layers = 1，lr = 2e-5，batch_size = 8，策略 = top5_single  
嘗試對困難負樣本挖掘進行迭代  
| 迭代次數 | Kaggle 分數 (Kaggle Score) |
| :--- | :--- |
| 0（TF-IDF）|0.94400| 
|1| 0.95200| 
|2| 0.95200| 


### 模型
以下為一些嘗試負採樣策略時留下的模型：  
https://drive.google.com/drive/folders/1SdyvFwu7l25AYFYKbu90VvNF9YStlCaB?usp=drive_link


