# 文字到程式碼檢索實作

本專案旨在實作一個文字到程式碼的檢索系統並比較不同的方法，其中包含了稀疏檢索（TF-IDF、BM25）和密集檢索（CodeBERT）兩種方法。

## 專案結構

```
.
├── code_snippets.csv
├── train_queries.csv
├── test_queries.csv
├── preprocess.py
├── sparse_retrieval.py
├── dense_retrieval.py
├── self_evaluation.py
├── generate_submissions.py
├── requirements.txt
└── Readme.md
```

*   `code_snippets.csv`: 包含所有程式碼片段的語料庫。
*   `train_queries.csv`: 用於微調密集檢索模型的訓練資料。
*   `test_queries.csv`: 用於生成Kaggle提交檔案的測試查詢。
*   `preprocess.py`: 包含資料前處理的相關函式。
*   `sparse_retrieval.py`: 實現了TF-IDF和BM25檢索模型。
*   `dense_retrieval.py`: 實現了使用預訓練和微調後CodeBERT的密集檢索模型，並包含訓練與驗證流程。
*   `self_evaluation.py`: 用於在`train_queries.csv`資料上對**稀疏模型**進行本地驗證。
*   `generate_submissions.py`: 用於產生所有模型的最終 Kaggle 提交檔案。
*   `requirements.txt`: 專案所需的Python套件。
*   `Readme.md`: 本文件。

## 安裝相依套件

```bash
pip install -r requirements.txt
```

## 執行流程

1.  **（可選）本地驗證稀疏模型**:
    ```bash
    python self_evaluation.py
    ```
    此腳本會使用 `train_queries.csv` 的資料來評估 TF-IDF 和 BM25 的效能。

2.  **訓練並驗證密集模型**:
    ```bash
    python dense_retrieval.py
    ```
    此腳本會載入預訓練的 CodeBERT，在驗證集上進行評估，然後進行微調，並儲存驗證集上表現最好的模型到 `./fine_tuned_codebert`。

3.  **產生最終提交檔案**:
    ```bash
    python generate_submissions.py
    ```
    此腳本會使用所有訓練好的模型，針對 `test_queries.csv` 產生所有四個提交檔案。

## 本地驗證與分析
由於提供的資料中只有用於微調dense model的train_queries，沒有提供驗證資料用於本地的效能驗證
因此此處用train_queries製作了用於本地驗證的驗證資料（用其中所有的code片段做語料庫，並嘗試用query查詢），用以對比Sparse檢索器的效能


### TF-IDF vs. BM25 性能分析
在一個使用`train_queries.csv`作為語料庫和查詢集的本地驗證中，得到了以下結果：
*   **TF-IDF Recall@10: 0.7400**
*   **BM25 Recall@10: 0.6680**

這個結果顯示，在這個特定的數據集上，TF-IDF的表現優於BM25。
這與一般的預期可能有所不同，可能的原因如下：
1.  **文件長度相對一致**
    BM25的文件長度正規化對於文件長度差異較大的語料庫特別有效。
    如果`train_queries.csv`中的程式碼片段長度都差不多，那麼BM25的這個優勢就無法發揮，甚至可能因為不當的懲罰而導致性能下降。
2.  **查詢較短或關鍵字單一**
    BM25的詞頻飽和度機制對於較長的查詢更有幫助。
    如果查詢本身很短，或者只包含少數幾個關鍵字，那麼詞頻的影響可能不是主要因素，TF-IDF的簡單加權方式可能反而更有效。
3.  **預設參數非最佳化**
    BM25有兩個可調參數`k1`和`b`，此處使用的是通用的預設值（k1=1.5, b=0.75）。
    這些參數可能不是這個特定數據集的最佳選擇。
    TF-IDF沒有需要調整的參數，因此不存在這個問題。

### BM25 參數調整
為了嘗試提升BM25的性能，有嘗試對`k1`和`b`參數進行了調整。以下是實驗結果：
| k1  | b    | Recall@10 |
| --- | ---- | --------- |
| 1.2 | 0.6  | 0.6580    |
| 1.2 | 0.75 | 0.6660    |
| 1.2 | 0.9  | 0.6620    |
| 1.5 | 0.6  | 0.6640    |
| 1.5 | 0.75 | 0.6680    |
| 1.5 | 0.9  | 0.6680    |
| 2.0 | 0.6  | 0.6700    |
| 2.0 | 0.75 | 0.6740    |
| 2.0 | 0.9  | 0.6780    |

最佳結果是在`k1=2.0`和`b=0.9`時，Recall@10達到了 **0.6780**。
雖然比預設參數的0.6680 有所提升，但仍然低於TF-IDF的 0.7400。
這進一步證實了，在這個特定的自建驗證集上，TF-IDF 是表現最好的稀疏檢索模型。

### N-gram 實驗
嘗試在斷詞時加入N-gram(bigrams 和 trigrams)來捕捉更多的上下文資訊。以下是自建驗證集上的結果：
*   **Unigrams (1-gram)**:
    *   TF-IDF Recall@10: 0.7380
    *   BM25 Recall@10: 0.6680
*   **Bigrams (1-gram + 2-gram)**:
    *   TF-IDF Recall@10: 0.7300
    *   BM25 Recall@10: 0.6700
*   **Trigrams (1-gram + 2-gram + 3-gram)**:
    *   TF-IDF Recall@10: 0.7320
    *   BM25 Recall@10: 0.6700
從結果來看，加入N-gram並沒有提升稀疏模型的性能，反而略有下降。
這可能表示對於這個數據集，額外的上下文資訊並沒有帶來好處，甚至可能引入了噪音。

### 文字預處理優化：查詢擴展 (Query Expansion)
嘗試了使用詞形還原（Lemmatization）來進行查詢擴展。
當查詢進入時，將**原始查詢詞和其詞形還原後的詞**都納入考量，以期捕捉更多相關的程式碼片段，以下是本地驗證集上的結果：
*   **Unigrams (無查詢擴展)**:
    *   TF-IDF Recall@10: 0.7400
    *   BM25 Recall@10: 0.6680
*   **Unigrams (有查詢擴展)**:
    *   TF-IDF Recall@10: 0.7480
    *   BM25 Recall@10: 0.6720

從結果來看，查詢擴展對TF-IDF和BM25的性能都有輕微的提升。
TF-IDF從0.7400提升到0.7480，BM25從0.6680提升到0.6720。
這表明詞形還原在一定程度上幫助模型捕捉了更多的語義相關性。

### TF-IDF 優化：次線性詞頻縮放 (Sublinear TF Scaling)
將詞頻 (TF) 的計算方式從原本的 `tf` (原始次數) 修改為 `1 + log(tf)` 
這種次線性縮放的策略旨在平滑化詞頻的影響，避免在長文件中頻繁出現的詞彙獲得過高的權重  

改動後發現模型在本地驗證集有了顯著的性能提升：

| 模型 | 原始 TF Recall@10 | 次線性 TF Recall@10 | 提升 |
| --- | --- | --- | --- |
| TF-IDF (基礎) | 0.7400 | **0.7660** | +2.6% |
| TF-IDF (含查詢擴充) | 0.7480 | **0.7860** | +3.8% |
這證明了「收益遞減」的詞頻計算方式更適合此資料集，它有效地降低了常見詞的噪音，讓模型的檢索結果更為精準。

**Kaggle Score**: 這次優化後的TF-IDF模型提交至Kaggle，獲得了 **0.73600** 的分數，成功超越了0.72000的Strong Baseline。

### 密集檢索模型實驗 (Dense Model Experiments)

#### 嘗試一：使用驗證集分割策略
最初嘗試將 `train_queries.csv` 的10%作為驗證集，並儲存驗證集上`Recall@10`分數最高的模型  
| 模型 | Validation Recall@10 |
| --- | --- |
| 預訓練 CodeBERT | 0.2600 |
| **微調後 CodeBERT (Epoch 1)** | **1.0000** |
**分析**:
*   **異常分數分析**: 在驗證集上出現 `1.0000` 的完美分數是一個**異常信號**，它並不代表模型的真實性能。其主要原因是**驗證方法存在缺陷**
    驗證集僅有50筆資料，且此處設計的機制會儲存訓練過程中遇到的本地驗證分數最高的權重，這使得很容易出現答案全對但模型不是我們想要的情況
*   **結論**
    回歸到Dense Retrieval中模型的功能，這邊引入語言模型的功能主要是**特徵提取**，因此微調模型參數時不應該用切割出驗證集的方式驗證模型當前表現  
    這邊應該用類似TripletMarginLoss的方式，藉由分析能否將輸入的query的特徵靠近正樣本，遠離負樣本，評估模型當前的表現  
    或者可以另外監控餘弦相似度分佈（正樣本 vs. 負樣本），看區分度是否隨訓練提升

    PS：  
    後來我發現，雖然不能藉由分割驗證集的方式評估訓練中模型的性能並提取最佳參數，但或許可以先取90%的資料用來訓練，然後用100%的code片段作為被查詢的資料庫，用剩餘10%的query進行查詢，評估模型的性能  
    只要不是用評估分數來選權重，只做為性能的評估，應當是可行的  
    **但是由於切出的資料集過小，只用於性能的粗略評估，不用於權重的調整**
    PSPS：  
        後來我又發現，在訓練時似乎把test_queries的語料庫code_snippets拿來用於訓練好的模型評估了，經檢測，train_queries裡面的code片與code_snippets應該是完全不重合的，所以前面對於dense model的評估似乎變得不太可靠


│       
  

#### 嘗試二：回歸模型本質，使用完整資料集訓練

**分析與結論**:
基於「嘗試一」的經驗，認識到在缺乏大規模、高品質驗證集的情況下，追求一個小型、自製驗證集上的最高分是錯誤的。  
語言模型在此任務中的核心功能是**特徵提取**，即為查詢和程式碼產生有意義的語義向量

因此，更可靠的訓練策略是：  
1.  **使用全部訓練資料**
    利用所有500筆`train_queries.csv`的資料對模型進行微調，讓模型看到盡可能多的範例。
2.  **觀察訓練損失 (Training Loss)**
    在沒有可靠驗證集的情況下，應將`TripletMarginLoss`的穩定下降作為模型正在有效學習的主要指標。
    或者可以另外監控餘弦相似度分佈（正樣本 vs. 負樣本），看區分度是否隨訓練提升。
    這代表模型正成功地將「相關的 (查詢, 程式碼) 對」在向量空間中拉近，並將「不相關的」推遠。

### 嘗試三：更換基底Bi-Encoder模型
為了尋找更強大的基底模型，對不同的預訓練模型進行評估（不經過微調），並以它們在Kaggle上的公開分數作為指標

| 模型 | Kaggle Score |
| --- | --- |
| `microsoft/codebert-base` | 0.05600 |
| `microsoft/graphcodebert-base` | 0.07600 |
| `Salesforce/codet5p-220m` | *等待提繳* |
| `microsoft/unixcoder-base` | *等待提繳* |

此處嘗試藉由切割驗證集作為本地評分依據：  
| 模型（經過微調） | Kaggle Score |
| --- | --- |
| `microsoft/codebert-base` | 0.7800 |
| `microsoft/graphcodebert-base` |  |
| `Salesforce/codet5p-220m` | |
| `microsoft/unixcoder-base` | |

### 混合檢索(Hybrid Retrieval)
為了結合稀疏模型的關鍵字匹配能力和密集模型的語義理解能力，實作了混合檢索模型  
該模型將TF-IDF(含查詢擴充)和微調後的 CodeBERT 所產生的分數進行正規化，並以 50/50 的權重相加，以此作為最終的排序依據
**結果**:
*   **Kaggle Score**: `0.70800`
**分析**:
*   混合模型的結果低於單獨的 TF-IDF (`0.71600`)。這表示對於此資料集，簡單的 50/50 加權平均不是最佳策略，權重分配 (`alpha`值) 或分數融合方法需要進一步的實驗與調整