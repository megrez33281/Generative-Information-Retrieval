{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clx1JZPqaHhC",
        "outputId": "8d253210-0297-4308-d95c-1d97bc639687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1sQLENaetXcCQadN7tIi2zQFA70T85QHN/Fine_Tune\n"
          ]
        }
      ],
      "source": [
        "!echo \"Mounting Google Drive...\"\n",
        "%cd /\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 包含訓練用資料的位置\n",
        "%cd /content/drive/MyDrive/Fine_Tune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 模型相關設定\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRE_TRAINED_MODEL_NAME = 'microsoft/unixcoder-base'\n",
        "FINE_TUNED_MODEL_PATH = './' + PRE_TRAINED_MODEL_NAME.replace(\"/\", \"-\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8UmRdqmyjZ3z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "\n",
        "num_layers = 4 # 選擇要用最後幾層的Output平均作為特徵\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# 【新設定】每個正樣本要搭配的困難負樣本數量\n",
        "NUM_NEGATIVES_PER_POSITIVE = 4 # 每個正樣本要搭配的困難負樣本數量\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    \"\"\"三元組數據集，用於微調密集檢索模型\"\"\"\n",
        "    def __init__(self, train_data_with_negatives, code_id_to_code_map):\n",
        "        \"\"\"初始化數據集，並在此處完成數據增強（分層抽樣）\"\"\"\n",
        "        self.triplets = []\n",
        "        print(\"\\nCreating training triplets with STRATIFIED negatives...\")\n",
        "        for item in tqdm(train_data_with_negatives):\n",
        "            query = item['query']\n",
        "            positive_code = item['positive_code']\n",
        "\n",
        "            if item['hard_negative_ids']:\n",
        "                stratum_size = 10\n",
        "                for i in range(NUM_NEGATIVES_PER_POSITIVE):\n",
        "                    stratum_start = i * stratum_size\n",
        "                    stratum_end = stratum_start + stratum_size\n",
        "\n",
        "                    # 定義該層的候選池\n",
        "                    stratum_pool = item['hard_negative_ids'][stratum_start:stratum_end]\n",
        "\n",
        "                    # 如果該層有候選者，就從中抽樣\n",
        "                    if stratum_pool:\n",
        "                        neg_id = random.choice(stratum_pool)\n",
        "                        negative_code = code_id_to_code_map[neg_id]\n",
        "                        self.triplets.append([query, positive_code, negative_code])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"返回數據集的大小\"\"\"\n",
        "        return len(self.triplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"獲取一個數據樣本 (一個三元組)\"\"\"\n",
        "        return self.triplets[idx]\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_length=512):\n",
        "    \"\"\"將 batch 的文字一次性 tokenizer，提高效率\"\"\"\n",
        "    anchors, positives, negatives = zip(*batch)\n",
        "\n",
        "    anchor_inputs = tokenizer(list(anchors), return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "    positive_inputs = tokenizer(list(positives), return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "    negative_inputs = tokenizer(list(negatives), return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "    return {\n",
        "        'anchor': {key: val.to(DEVICE) for key, val in anchor_inputs.items()},\n",
        "        'positive': {key: val.to(DEVICE) for key, val in positive_inputs.items()},\n",
        "        'negative': {key: val.to(DEVICE) for key, val in negative_inputs.items()}\n",
        "    }\n",
        "\n",
        "def get_embedding(model, tokenizer, text, max_length=512):\n",
        "    \"\"\"輔助函式，用於獲取單個文本的嵌入向量\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        # 判斷模型是否為 Encoder-Decoder 架構\n",
        "        if hasattr(model, 'get_encoder'):\n",
        "            outputs = model.get_encoder()(**inputs, output_hidden_states=True)\n",
        "        else:\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "        stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "        mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "        embedding = mean_last_layers.mean(dim=1)\n",
        "    return embedding.cpu()\n",
        "\n",
        "# 將anchor、positive、negative的token輸入模型，取多層hidden_state做平均\n",
        "def get_layerwise_embeddings(model, batch_inputs, num_layers=num_layers):\n",
        "    \"\"\"\n",
        "    batch_inputs: batch['anchor'] / batch['positive'] / batch['negative']\n",
        "    num_layers: 取最後幾層做平均（作為文字的特徵）\n",
        "    \"\"\"\n",
        "    # Check if the model has an encoder (i.e., is an encoder-decoder model)\n",
        "    if hasattr(model, 'get_encoder'):\n",
        "        outputs = model.get_encoder()(input_ids=batch_inputs['input_ids'],attention_mask=batch_inputs['attention_mask'], output_hidden_states=True)\n",
        "    else:\n",
        "        outputs = model(**batch_inputs, output_hidden_states=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states  # tuple of all layers\n",
        "\n",
        "    # 取最後 num_layers 層平均\n",
        "    stacked_layers = torch.stack(hidden_states[-num_layers:])  # shape: (num_layers, batch_size, seq_len, hidden_size)\n",
        "    mean_last_layers = torch.mean(stacked_layers, dim=0)      # shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "    # 對 token 平均 pooling，得到每個樣本的句子向量\n",
        "    embeddings = mean_last_layers.mean(dim=1)  # shape: (batch_size, hidden_size)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def evaluate_recall(model, tokenizer, val_df, corpus_df, cached_corpus_embeddings=None):\n",
        "    model.eval()\n",
        "    #  先計算全部的語料庫特徵\n",
        "    if cached_corpus_embeddings is None:\n",
        "        print(\"\\nCreating cached embeddings for the corpus...\")\n",
        "        all_codes = list(corpus_df['code'])\n",
        "        corpus_embeddings = []\n",
        "        batch_size = 32\n",
        "        for i in tqdm(range(0, len(all_codes), batch_size), desc=\"Corpus Embeddings\"):\n",
        "            batch_codes = all_codes[i:i+batch_size]\n",
        "            inputs = tokenizer(batch_codes, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs, output_hidden_states=True)\n",
        "                hidden_states = outputs.hidden_states\n",
        "                stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "                mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "                embeddings = mean_last_layers.mean(dim=1)\n",
        "            corpus_embeddings.append(embeddings.cpu())\n",
        "        corpus_embeddings = torch.cat(corpus_embeddings, dim=0)\n",
        "    else:\n",
        "        corpus_embeddings = cached_corpus_embeddings\n",
        "\n",
        "    recall_at_10 = 0\n",
        "    for _, row in tqdm(val_df.iterrows(), total=val_df.shape[0], desc=\"Evaluating Recall@10\"):\n",
        "        query = row['query']\n",
        "        true_code_string = row['code']\n",
        "        query_embedding = get_embedding(model, tokenizer, query)\n",
        "\n",
        "        # 計算餘弦相似度\n",
        "        scores = torch.nn.functional.cosine_similarity(query_embedding, corpus_embeddings)\n",
        "        top_k_indices = torch.argsort(scores, descending=True)[:10]\n",
        "        top_k_codes = corpus_df.iloc[top_k_indices]['code'].values\n",
        "        if true_code_string in top_k_codes:\n",
        "            recall_at_10 += 1\n",
        "    return recall_at_10 / len(val_df), corpus_embeddings\n",
        "\n",
        "class DenseRetriever:\n",
        "    \"\"\"密集檢索器\"\"\"\n",
        "    def __init__(self, documents, model_name_or_path, batch_size=32):\n",
        "        \"\"\"初始化密集檢索器\"\"\"\n",
        "        self.documents = documents\n",
        "        # 載入預訓練模型和斷詞器\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = AutoModel.from_pretrained(model_name_or_path)\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval() # 預設為評估模式\n",
        "        self.batch_size = batch_size\n",
        "        # 建立文件的嵌入向量（使用 batch 化）\n",
        "        self.doc_embeddings = self._create_doc_embeddings()\n",
        "\n",
        "    def _create_doc_embeddings(self):\n",
        "        \"\"\"建立所有文件的嵌入向量 (batch 化加速)\"\"\"\n",
        "        all_codes = list(self.documents['code'])\n",
        "        embeddings = []\n",
        "        for i in tqdm(range(0, len(all_codes), self.batch_size), desc=\"Creating document embeddings\"):\n",
        "            batch_codes = all_codes[i:i+self.batch_size]\n",
        "            inputs = self.tokenizer(batch_codes, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                # Check if the model has an encoder (i.e., is an encoder-decoder model)\n",
        "                if hasattr(self.model, 'get_encoder'):\n",
        "                    outputs = self.model.get_encoder()(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], output_hidden_states=True)\n",
        "                else:\n",
        "                     outputs = self.model(**inputs, output_hidden_states=True)\n",
        "                hidden_states = outputs.hidden_states\n",
        "                stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "                mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "                batch_embeddings = mean_last_layers.mean(dim=1)\n",
        "            embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "        embeddings = np.vstack(embeddings)\n",
        "        # 對所有文件嵌入向量進行 L2 正規化\n",
        "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        embeddings = embeddings / norms\n",
        "        return embeddings\n",
        "\n",
        "    def retrieve(self, query, k=10):\n",
        "        \"\"\"根據查詢檢索文件\"\"\"\n",
        "        # 對單一 query 編碼\n",
        "        query_embedding = get_embedding(self.model, self.tokenizer, query).cpu().numpy()\n",
        "        # 對查詢嵌入向量進行 L2 正規化\n",
        "        query_norm = np.linalg.norm(query_embedding)\n",
        "        query_embedding = query_embedding / query_norm\n",
        "\n",
        "        # 計算餘弦相似度 (經過正規化後點積等同於餘弦相似度)\n",
        "        scores = np.dot(self.doc_embeddings, query_embedding.T).flatten()\n",
        "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
        "        top_k_scores = scores[top_k_indices]\n",
        "        return top_k_indices, top_k_scores\n",
        "\n",
        "def split_data(train_queries_df):\n",
        "    # 90% 的code-query配對用於訓練，剩餘10%的query用於評估並對答案\n",
        "    # 每個 code 是一個 group\n",
        "    groups = train_queries_df['code']\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "    train_idx, val_idx = next(gss.split(train_queries_df, groups=groups))\n",
        "    train_df = train_queries_df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = train_queries_df.iloc[val_idx].reset_index(drop=True)\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, epochs=3, lr=2e-5, batch_size=8):\n",
        "    \"\"\"微調預訓練模型\"\"\"\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # 這是對訓練資料的準備，會產生每個樣本的anchor/positive/negative張量\n",
        "    dataset = TripletDataset(train_data_with_negatives, code_id_to_code_map)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
        "                            collate_fn=lambda x: collate_fn(x, tokenizer))  # 使用 collate_fn 做 batch tokenizer\n",
        "\n",
        "    # 設定優化器和損失函數\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # 標準Transformer訓練用優化器\n",
        "    loss_fn = torch.nn.TripletMarginLoss(margin=1.0) # 三元組損失，目標是讓anchor（查詢）與positive（正確答案之間的距離小於anchor與negative（錯誤答案）之間的距離，至少相差一個margin\n",
        "\n",
        "    # 訓練模型\n",
        "    for epoch in range(epochs):\n",
        "        model.train() # 切換到訓練模式\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "            optimizer.zero_grad() # 清除上一步梯度\n",
        "\n",
        "            anchor_embeddings = get_layerwise_embeddings(model, batch['anchor'])\n",
        "            positive_embeddings = get_layerwise_embeddings(model, batch['positive'])\n",
        "            negative_embeddings = get_layerwise_embeddings(model, batch['negative'])\n",
        "\n",
        "            # 計算損失\n",
        "            loss = loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "            loss.backward()  # 計算梯度\n",
        "            optimizer.step()  # 更新參數\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. 準備資料 ---\n",
        "    print(\"--- Preparing Data for Fine-tuning ---\")\n",
        "\n",
        "    with open('train_data_with_negatives.json', 'r', encoding='utf-8') as f:\n",
        "        train_data_with_negatives = json.load(f)\n",
        "\n",
        "    code_snippets_df = pd.read_csv('code_snippets.csv')\n",
        "    code_id_to_code_map = pd.Series(code_snippets_df.code.values, index=code_snippets_df.code_id).to_dict()\n",
        "\n",
        "    # --- 2. 初始化模型 ---\n",
        "    print(\"\\n--- Initializing Model ---\")\n",
        "    model_name = 'microsoft/unixcoder-base'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # --- 3. 微調模型 ---\n",
        "    print(\"\\n--- Fine-tuning model with Multi-Negative Strategy ---\")\n",
        "    fine_tuned_model = fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, epochs=3, lr=2e-5, batch_size=8)\n",
        "\n",
        "    # --- 4. 儲存模型 ---\n",
        "    output_dir = FINE_TUNED_MODEL_PATH\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    print(f\"\\nSaving the final model to {output_dir}...\")\n",
        "    fine_tuned_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    print(\"\\n--- Model training complete. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCaH979wGuwo",
        "outputId": "99e832df-c35b-4940-b377-6cedbd3a44af"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing Data for Fine-tuning ---\n",
            "\n",
            "--- Initializing Model ---\n",
            "\n",
            "--- Fine-tuning model with Multi-Negative Strategy ---\n",
            "\n",
            "Creating training triplets with STRATIFIED negatives...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 167450.65it/s]\n",
            "Epoch 1/3: 100%|██████████| 250/250 [08:41<00:00,  2.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Loss: 0.0655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 250/250 [08:48<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Average Loss: 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 250/250 [08:50<00:00,  2.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Average Loss: 0.0053\n",
            "\n",
            "Saving the final model to ./microsoft-unixcoder-base...\n",
            "\n",
            "--- Model training complete. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# dense_retrieval\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 選擇要測試的model\n",
        "try_prtrained_model = False\n",
        "try_fine_tuned_model = True\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. 載入資料並準備驗證集\n",
        "    print(\"--- 1. 載入資料並準備驗證集 ---\")\n",
        "    # 注意，此處用於測試的語料庫也來自於train_queries.csv，訓練時保留了10%的query沒有用於訓練\n",
        "    train_queries_df = pd.read_csv('train_queries.csv')\n",
        "\n",
        "\n",
        "    # 使用與微調腳本完全相同的分割方式\n",
        "    _, val_df = split_data(train_queries_df)\n",
        "    print(f\"已載入 {len(val_df)} 筆樣本用於驗證。\")\n",
        "\n",
        "    # 2. 評估預訓練模型\n",
        "    if try_prtrained_model:\n",
        "        print(\"\\n--- 2. 評估預訓練模型 ---\")\n",
        "        print(f\"模型: {PRE_TRAINED_MODEL_NAME}\")\n",
        "        pretrained_tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "        pretrained_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME).to(DEVICE)\n",
        "\n",
        "        pretrained_recall, corpus_embeddings_pretrained = evaluate_recall(pretrained_model, pretrained_tokenizer, val_df, train_queries_df)\n",
        "        print(f\"\\n預訓練模型 Recall@10: {pretrained_recall:.4f}\")\n",
        "\n",
        "    # 3. 評估微調後的模型\n",
        "    if try_fine_tuned_model:\n",
        "        print(\"\\n--- 3. 評估微調後的模型 ---\")\n",
        "        print(f\"模型: {FINE_TUNED_MODEL_PATH}\")\n",
        "        try:\n",
        "            finetuned_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH)\n",
        "            finetuned_model = AutoModel.from_pretrained(FINE_TUNED_MODEL_PATH).to(DEVICE)\n",
        "\n",
        "            # 微調後的模型需要重新計算語料庫的嵌入向量\n",
        "            finetuned_recall, _ = evaluate_recall(finetuned_model, finetuned_tokenizer, val_df, train_queries_df)\n",
        "            print(f\"\\n微調後模型 Recall@10: {finetuned_recall:.4f}\")\n",
        "\n",
        "        except OSError:\n",
        "            print(f\"錯誤: 在 '{FINE_TUNED_MODEL_PATH}' 找不到微調後的模型。\")\n",
        "            print(\"請先執行 'fine_tune_model.py' 來訓練並儲存模型。\")\n",
        "\n",
        "    print(\"\\n--- 評估完成 ---\")"
      ],
      "metadata": {
        "id": "AqowqPl6hGff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d0f69d-22b4-4f6a-caf4-9cad8ad8332b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 載入資料並準備驗證集 ---\n",
            "已載入 50 筆樣本用於驗證。\n",
            "\n",
            "--- 3. 評估微調後的模型 ---\n",
            "模型: ./microsoft-unixcoder-base\n",
            "\n",
            "Creating cached embeddings for the corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:12<00:00,  1.23it/s]\n",
            "Evaluating Recall@10: 100%|██████████| 50/50 [00:01<00:00, 33.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "微調後模型 Recall@10: 0.8600\n",
            "\n",
            "--- 評估完成 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成dense model的submission\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "def generate_submission(retriever, test_df, output_path, query_expansion=False):\n",
        "    \"\"\"\n",
        "    Generates a submission file for a given retriever.\n",
        "    \"\"\"\n",
        "    print(f\"Generating submission for {output_path}...\")\n",
        "    results = []\n",
        "    # 使用tqdm顯示進度條\n",
        "    for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=output_path):\n",
        "        query_id = row['query_id']\n",
        "        query = row['query']\n",
        "        top_k_indices, _ = retriever.retrieve(query, k=10)\n",
        "\n",
        "        # 直接使用檢索器內部儲存的 documents DataFrame 來獲取 code_id\n",
        "        top_k_code_ids = retriever.documents.iloc[top_k_indices]['code_id'].tolist()\n",
        "\n",
        "        results.append({\n",
        "            'query_id': query_id,\n",
        "            'code_id': ' '.join(map(str, top_k_code_ids))\n",
        "        })\n",
        "\n",
        "    submission_df = pd.DataFrame(results)\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"Submission file saved to {output_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 載入資料 ---\n",
        "    print(\"Loading data...\")\n",
        "    code_snippets_df = pd.read_csv('code_snippets.csv')\n",
        "    test_queries_df = pd.read_csv('test_queries.csv')\n",
        "\n",
        "\n",
        "    # --- 密集模型 ---\n",
        "    # 檢查微調後的模型是否存在\n",
        "    finetuned_model_path = FINE_TUNED_MODEL_PATH\n",
        "\n",
        "    # 預訓練的密集檢索器\n",
        "    print(\"\\nInitializing pre-trained dense model...\")\n",
        "    pretrained_retriever = DenseRetriever(code_snippets_df, model_name_or_path=PRE_TRAINED_MODEL_NAME)\n",
        "    generate_submission(pretrained_retriever, test_queries_df, 'submission_pretrained.csv')\n",
        "\n",
        "    if not os.path.exists(finetuned_model_path):\n",
        "        print(f\"\\nFine-tuned model not found at '{finetuned_model_path}'.\")\n",
        "        print(\"Skipping submission generation for the fine-tuned model.\")\n",
        "    else:\n",
        "        # 微調後的密集檢索器\n",
        "        print(\"\\nInitializing fine-tuned dense model...\")\n",
        "        finetuned_retriever = DenseRetriever(code_snippets_df, model_name_or_path=finetuned_model_path)\n",
        "        generate_submission(finetuned_retriever, test_queries_df, 'submission_finetuned.csv')\n",
        "\n",
        "    print(\"\\nAll submission files have been generated.\")"
      ],
      "metadata": {
        "id": "YOng7GlViAM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74716535-034b-460c-a3b1-926b0556d467"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Initializing pre-trained dense model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating document embeddings: 100%|██████████| 16/16 [00:13<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission for submission_pretrained.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "submission_pretrained.csv: 100%|██████████| 500/500 [00:15<00:00, 32.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved to submission_pretrained.csv\n",
            "\n",
            "Initializing fine-tuned dense model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating document embeddings: 100%|██████████| 16/16 [00:13<00:00,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission for submission_finetuned.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "submission_finetuned.csv: 100%|██████████| 500/500 [00:15<00:00, 33.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved to submission_finetuned.csv\n",
            "\n",
            "All submission files have been generated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}