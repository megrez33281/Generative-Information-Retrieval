{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6437ab17c7274509b42f26dae0b82358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5a32df3d03d4d978af48b7f4747becd",
              "IPY_MODEL_fcefe56a6e584d1b8c7bd6c75a2e699d",
              "IPY_MODEL_f70a0bb85ebd4d42ab72676af8ad1332"
            ],
            "layout": "IPY_MODEL_a9519c9fb2b6411d86ff820013fced6b"
          }
        },
        "b5a32df3d03d4d978af48b7f4747becd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abfd5d3f595f4df4af16168bac99dc55",
            "placeholder": "​",
            "style": "IPY_MODEL_b962389bf8724f4e85cae49d5346f0f7",
            "value": "modules.json: 100%"
          }
        },
        "fcefe56a6e584d1b8c7bd6c75a2e699d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b42b7359aef481d9990e5918ef2ce64",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f43b12b9366f49ee9be7f3e7df6752ee",
            "value": 229
          }
        },
        "f70a0bb85ebd4d42ab72676af8ad1332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc79b0b96df6421abb216fd55d92bf9c",
            "placeholder": "​",
            "style": "IPY_MODEL_56d4c1809b57409191a97977fc34de12",
            "value": " 229/229 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "a9519c9fb2b6411d86ff820013fced6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abfd5d3f595f4df4af16168bac99dc55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b962389bf8724f4e85cae49d5346f0f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b42b7359aef481d9990e5918ef2ce64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43b12b9366f49ee9be7f3e7df6752ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc79b0b96df6421abb216fd55d92bf9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d4c1809b57409191a97977fc34de12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "336e275cb3804879a96434ee6cf64df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a1c5731fc4e45ac8ea12374ddbe8ee1",
              "IPY_MODEL_8cea304ef52f4f0397bf36099ae9b76d",
              "IPY_MODEL_f2c542f3933247e2935dcccac7cba96f"
            ],
            "layout": "IPY_MODEL_b22109b877de4c7ab35f5d41e6a97f57"
          }
        },
        "5a1c5731fc4e45ac8ea12374ddbe8ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3177c1da8fa74c4fb00228f125eb54ea",
            "placeholder": "​",
            "style": "IPY_MODEL_bd528a2986a94f95b597000036c629be",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "8cea304ef52f4f0397bf36099ae9b76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6e64261da294d029046a3925d9ba1d5",
            "max": 212,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0eb633a3bd5d43d3bf58861a3adfa63a",
            "value": 212
          }
        },
        "f2c542f3933247e2935dcccac7cba96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0c586a53121454e9a060e48e974b19a",
            "placeholder": "​",
            "style": "IPY_MODEL_433a838651ff48f89bba72ca2975432f",
            "value": " 212/212 [00:00&lt;00:00, 17.0kB/s]"
          }
        },
        "b22109b877de4c7ab35f5d41e6a97f57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3177c1da8fa74c4fb00228f125eb54ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd528a2986a94f95b597000036c629be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6e64261da294d029046a3925d9ba1d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb633a3bd5d43d3bf58861a3adfa63a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0c586a53121454e9a060e48e974b19a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "433a838651ff48f89bba72ca2975432f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4127e7705c84c969aaa6753eb5d8d54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2995d3a227b4b5daf1835e73e25204b",
              "IPY_MODEL_e7457e37d80c494eb932a227ea4ab753",
              "IPY_MODEL_e077707b0b59414ab218264d43632c51"
            ],
            "layout": "IPY_MODEL_d0c5d905a46a455ca3b6e24abc41f43d"
          }
        },
        "b2995d3a227b4b5daf1835e73e25204b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db5e1f33e0b74f588f079fd352e5343c",
            "placeholder": "​",
            "style": "IPY_MODEL_3bcabc6b8bb84876978397f9b5458157",
            "value": "README.md: "
          }
        },
        "e7457e37d80c494eb932a227ea4ab753": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff459b3c95ed40e3acd91458b7dfa8a0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d166e73e9e9241f1bf764d9152541327",
            "value": 1
          }
        },
        "e077707b0b59414ab218264d43632c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40078e067f3f434b973c3b46bd36b40d",
            "placeholder": "​",
            "style": "IPY_MODEL_19b433c4067d46fb841fd23ac94d5f42",
            "value": " 9.95k/? [00:00&lt;00:00, 991kB/s]"
          }
        },
        "d0c5d905a46a455ca3b6e24abc41f43d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db5e1f33e0b74f588f079fd352e5343c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bcabc6b8bb84876978397f9b5458157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff459b3c95ed40e3acd91458b7dfa8a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d166e73e9e9241f1bf764d9152541327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40078e067f3f434b973c3b46bd36b40d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b433c4067d46fb841fd23ac94d5f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13f547b840a5484d94994251d23f88c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b0d81b804b43539840cb3917eaee46",
              "IPY_MODEL_968e4c2f55864794b006e7d8439eb63e",
              "IPY_MODEL_bd7437f80ed04e7481e82e69c8726051"
            ],
            "layout": "IPY_MODEL_6b767f3fc15b4cbe90d1a134e44fbe94"
          }
        },
        "d9b0d81b804b43539840cb3917eaee46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5236ec9ba224686ba271160765d51cb",
            "placeholder": "​",
            "style": "IPY_MODEL_0f95ea3b17b0465aaa493c26a610b99c",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "968e4c2f55864794b006e7d8439eb63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_348dbe94896b4cf099d7c21e7d93d019",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c83f3219dd7d422cb6a3f7541b418c40",
            "value": 53
          }
        },
        "bd7437f80ed04e7481e82e69c8726051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a2b2718db454818b88f33ff65b19312",
            "placeholder": "​",
            "style": "IPY_MODEL_c6e95b56e192499f98200e84f6e751d8",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.81kB/s]"
          }
        },
        "6b767f3fc15b4cbe90d1a134e44fbe94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5236ec9ba224686ba271160765d51cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f95ea3b17b0465aaa493c26a610b99c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "348dbe94896b4cf099d7c21e7d93d019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c83f3219dd7d422cb6a3f7541b418c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a2b2718db454818b88f33ff65b19312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e95b56e192499f98200e84f6e751d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8ec8d72d79545f086151c8de67376d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dbdd8633e45459eabc73c2c7fef97c1",
              "IPY_MODEL_ca2d4ec6f14c40b2b3b02c376c043a01",
              "IPY_MODEL_7b5d87db87564561ab144b47203982c1"
            ],
            "layout": "IPY_MODEL_20e8a43ce7d34576bfde5c6c5ebdca13"
          }
        },
        "7dbdd8633e45459eabc73c2c7fef97c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a4016b8ed5b4b5a8e1e36f58b23d31b",
            "placeholder": "​",
            "style": "IPY_MODEL_563c72dc542a4f98bca24c8a3b4e8e76",
            "value": "config.json: 100%"
          }
        },
        "ca2d4ec6f14c40b2b3b02c376c043a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6026b0afa56c40d189a2cf7e21bf207b",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50b7f4051b694d06a4526423d89ee1a9",
            "value": 190
          }
        },
        "7b5d87db87564561ab144b47203982c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1caf471403f403baea8de47c4858c11",
            "placeholder": "​",
            "style": "IPY_MODEL_7ebc1f36fc2d4389be654098a85c236f",
            "value": " 190/190 [00:00&lt;00:00, 18.8kB/s]"
          }
        },
        "20e8a43ce7d34576bfde5c6c5ebdca13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4016b8ed5b4b5a8e1e36f58b23d31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563c72dc542a4f98bca24c8a3b4e8e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6026b0afa56c40d189a2cf7e21bf207b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50b7f4051b694d06a4526423d89ee1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1caf471403f403baea8de47c4858c11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ebc1f36fc2d4389be654098a85c236f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**前置設定**"
      ],
      "metadata": {
        "id": "2lRtMqiR683F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clx1JZPqaHhC",
        "outputId": "d58a3331-3ae3-4570-b3ae-c30266949469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Mounted at /content/drive\n",
            "/content/drive/.shortcut-targets-by-id/1sQLENaetXcCQadN7tIi2zQFA70T85QHN/Fine_Tune\n"
          ]
        }
      ],
      "source": [
        "!echo \"Mounting Google Drive...\"\n",
        "%cd /\n",
        "\n",
        "# 此處為了訓練方便，掛載了Google Drive，需要在我的雲端硬碟建立'Fine_Tune'並在裡面放入訓練資料\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# 包含訓練用資料的位置\n",
        "%cd /content/drive/MyDrive/Fine_Tune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 模型相關設定\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PRE_TRAINED_MODEL_NAME = 'microsoft/unixcoder-base'\n",
        "FINE_TUNED_MODEL_PATH = './' + PRE_TRAINED_MODEL_NAME.replace(\"/\", \"-\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8UmRdqmyjZ3z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**文字預處理**"
      ],
      "metadata": {
        "id": "maJQ0p027Ewi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# 此為可選項，可以選擇開啟或不開啟\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    \"\"\"對 token 列表進行詞形還原\"\"\"\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def load_code_snippets(file_path):\n",
        "    \"\"\"從 CSV 檔案載入程式碼片段\"\"\"\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "def tokenize(text, n_gram_range=(1, 1), keep_both=True):\n",
        "    \"\"\"\n",
        "    將程式碼文本進行斷詞，並可選擇性地生成 n-grams\n",
        "    - n_gram_range: (min_n, max_n)，例如 (1,2) 會同時生成unigram與bigram\n",
        "    - keep_both: 是否同時保留\"空格形式\"與\"原樣形式\"\n",
        "    \"\"\"\n",
        "    # 保留運算符作為獨立的 token\n",
        "    tokens = re.findall(r'\\w+|==|!=|<=|>=|[\\+\\-\\*/=<>!&|%\\^~]', text)\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # 如果只要 unigram\n",
        "    if n_gram_range == (1, 1):\n",
        "        return tokens\n",
        "\n",
        "    ngrams = []\n",
        "    for n in range(n_gram_range[0], n_gram_range[1] + 1):\n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            slice_tokens = tokens[i:i+n]\n",
        "            if keep_both:\n",
        "                # 空格形式（以空格進行連接，但可能造成詞語的改變）\n",
        "                ngrams.append(\" \".join(slice_tokens))\n",
        "                # 原樣形式直接連起來）\n",
        "                ngrams.append(\"\".join(slice_tokens))\n",
        "            else:\n",
        "                ngrams.append(\" \".join(slice_tokens))\n",
        "    return ngrams\n",
        "\n",
        "def preprocess(df, n_gram_range=(1, 1)):\n",
        "    \"\"\"對程式碼片段進行預處理\"\"\"\n",
        "    df['tokens'] = df['code'].apply(lambda x: tokenize(x, n_gram_range=n_gram_range))\n",
        "    return df\n",
        "\n",
        "def create_semantic_mapping():\n",
        "    \"\"\"建立從自然語言到程式碼運算符的語意映射\"\"\"\n",
        "\n",
        "    # 考慮到此處的程式碼為python，運算符號本身存在意義，需要另外保留運算符（+-*/等）作為獨立的Token，並為自然語言的查詢建立對應的映射，使其能與運算符匹配\n",
        "    return {\n",
        "        \"add\": \"+\", \"sum\": \"+\", \"plus\": \"+\", \"addition\": \"+\",\n",
        "        \"concatenate\": \"+\", \"join\": \"+\",\n",
        "        \"assign\": \"=\", \"set\": \"=\",\n",
        "        \"subtract\": \"-\", \"minus\": \"-\", \"subtraction\": \"-\",\n",
        "        \"multiply\": \"*\", \"times\": \"*\", \"multiplication\": \"*\",\n",
        "        \"divide\": \"/\", \"division\": \"/\",\n",
        "        \"equals\": \"==\", \"is\": \"==\",\n",
        "        \"less\": \"<\", \"smaller\": \"<\",\n",
        "        \"greater\": \">\", \"larger\": \">\",\n",
        "    }\n",
        "\n",
        "def build_statistics(processed_df):\n",
        "    \"\"\"建立詞彙庫和其他統計數據\"\"\"\n",
        "    tokenized_docs = processed_df['tokens'].tolist()\n",
        "\n",
        "    # 文件頻率，計算每個Token出現在多少篇文件中\n",
        "    doc_freq = Counter()\n",
        "    for doc in tokenized_docs:\n",
        "        # 遍歷每個doc\n",
        "        doc_freq.update(set(doc))\n",
        "\n",
        "    # 詞彙庫\n",
        "    vocab = list(doc_freq.keys())\n",
        "\n",
        "    # token總數\n",
        "    total_tokens = sum(len(doc) for doc in tokenized_docs)\n",
        "\n",
        "    # 文件總數\n",
        "    num_docs = len(tokenized_docs)\n",
        "\n",
        "    # 平均文件長度\n",
        "    avg_doc_len = total_tokens / num_docs if num_docs > 0 else 0\n",
        "\n",
        "    return vocab, doc_freq, tokenized_docs, avg_doc_len\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 載入資料\n",
        "    code_snippets_df = load_code_snippets('code_snippets.csv')\n",
        "\n",
        "    # 預處理資料\n",
        "    processed_df = preprocess(code_snippets_df)\n",
        "\n",
        "    # 建立統計數據\n",
        "    vocab, doc_freq, tokenized_docs, avg_doc_len = build_statistics(processed_df)\n",
        "\n",
        "    # 建立語意映射\n",
        "    semantic_mapping = create_semantic_mapping()\n",
        "\n",
        "    # 印出一些統計數據\n",
        "    print(f\"詞彙庫大小: {len(vocab)}\")\n",
        "    print(f\"文件總數: {len(tokenized_docs)}\")\n",
        "    print(f\"平均文件長度: {avg_doc_len:.2f}\")\n",
        "    print(\"\\n前 10 個最常見的 token:\")\n",
        "    print(doc_freq.most_common(10))\n",
        "    print(\"\\n語意映射:\")\n",
        "    print(semantic_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvQSih4M7Klr",
        "outputId": "fcdc9fcb-1e32-485a-bd13-5f367174c3dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "詞彙庫大小: 4946\n",
            "文件總數: 500\n",
            "平均文件長度: 57.23\n",
            "\n",
            "前 10 個最常見的 token:\n",
            "[('def', 500), ('=', 429), ('return', 362), ('if', 302), ('self', 297), ('in', 191), ('none', 155), ('for', 153), ('not', 152), ('0', 130)]\n",
            "\n",
            "語意映射:\n",
            "{'add': '+', 'sum': '+', 'plus': '+', 'addition': '+', 'concatenate': '+', 'join': '+', 'assign': '=', 'set': '=', 'subtract': '-', 'minus': '-', 'subtraction': '-', 'multiply': '*', 'times': '*', 'multiplication': '*', 'divide': '/', 'division': '/', 'equals': '==', 'is': '==', 'less': '<', 'smaller': '<', 'greater': '>', 'larger': '>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sparse_retrieval**"
      ],
      "metadata": {
        "id": "6e5QD21Q7Q4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class TFIDFRetriever:\n",
        "    \"\"\"TF-IDF檢索器\"\"\"\n",
        "    def __init__(self, documents):\n",
        "        \"\"\"初始化TF-IDF檢索器\"\"\"\n",
        "        self.documents = documents\n",
        "        # 建立詞彙庫、文件頻率等統計數據\n",
        "        self.vocab, self.doc_freq, self.tokenized_docs, self.avg_doc_len = build_statistics(documents)\n",
        "        self.num_docs = len(self.tokenized_docs)\n",
        "\n",
        "        # 把詞彙表（vocab）裡的每個單詞對應到一個整數索引，之後可以用來查詢詞彙對應的索引\n",
        "        self.vocab_map = {word: i for i, word in enumerate(self.vocab)}\n",
        "\n",
        "        # 計算IDF\n",
        "        self.idf = self._calculate_idf()\n",
        "        # 建立TF-IDF向量\n",
        "        self.doc_vectors = self._create_doc_vectors()\n",
        "\n",
        "    def _calculate_idf(self):\n",
        "        \"\"\"用公式計算IDF分數\"\"\"\n",
        "        idf = np.zeros(len(self.vocab))\n",
        "        for i, word in enumerate(self.vocab):\n",
        "            idf[i] = np.log(self.num_docs / (self.doc_freq[word] + 1)) # +1避免分母為0\n",
        "        return idf\n",
        "\n",
        "    def _create_doc_vectors(self):\n",
        "        \"\"\"建立每篇文件的TF-IDF向量\"\"\"\n",
        "\n",
        "        # 建立一個全零矩陣，大小是(文件數量 × 詞彙表大小)\n",
        "        doc_vectors = np.zeros((self.num_docs, len(self.vocab)))\n",
        "        for i, doc in enumerate(self.tokenized_docs):\n",
        "            # doc代表每個文件斷詞後的詞彙列表，利用Counter計算每個詞彙在該文件中出現的次數\n",
        "            tf = Counter(doc)\n",
        "            for word, count in tf.items():\n",
        "                if word in self.vocab_map:\n",
        "                    # 使用次線性詞頻縮放 (sublinear tf scaling): 1 + log(tf)\n",
        "                    doc_vectors[i, self.vocab_map[word]] = 1 + np.log(count)\n",
        "        return doc_vectors * self.idf # 矩陣或向量中對應位置的元素進行乘法運算\n",
        "\n",
        "    def retrieve(self, query, k=10, query_expansion=False):\n",
        "        \"\"\"進行query，返回文件在資料庫中的索引\"\"\"\n",
        "        query_tokens = tokenize(query)\n",
        "        if query_expansion:\n",
        "            # 擴展功能，可選擇是否對token進行詞型還原\n",
        "            lemmatized_tokens = lemmatize_tokens(query_tokens)\n",
        "            query_tokens.extend(lemmatized_tokens)\n",
        "\n",
        "        # 計算query的TF\n",
        "        query_vector = np.zeros(len(self.vocab))\n",
        "        tf = Counter(query_tokens)\n",
        "        for word, count in tf.items():\n",
        "            if word in self.vocab_map:\n",
        "                query_vector[self.vocab_map[word]] = 1 + np.log(count)\n",
        "        # 計算query的TF-IDF\n",
        "        query_vector = query_vector * self.idf\n",
        "\n",
        "        # 計算查詢向量的範數(norm)\n",
        "        query_norm = np.linalg.norm(query_vector)\n",
        "\n",
        "        # 如果查詢向量的範數為0 (代表查詢詞皆不存在於語料庫中)，則所有分數為0\n",
        "        if query_norm == 0:\n",
        "            scores = np.zeros(self.num_docs)\n",
        "        else:\n",
        "            # 計算餘弦相似度（cosine公式）\n",
        "            doc_norms = np.linalg.norm(self.doc_vectors, axis=1)\n",
        "            scores = np.dot(self.doc_vectors, query_vector) / (doc_norms * query_norm)\n",
        "            # 將分母為0可能導致的 nan 值替換為 0，確保數值穩定性\n",
        "            scores = np.nan_to_num(scores)\n",
        "\n",
        "        # 取得前k個結果\n",
        "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
        "        top_k_scores = scores[top_k_indices]\n",
        "\n",
        "        \"\"\"\n",
        "        TF衡量詞在文件內的重要性\n",
        "        IDF衡量詞在整個語料庫中的稀有程度（越稀有越重要）\n",
        "        TF×IDF = 詞在文件中的加權重要性 → 形成TF-IDF向量，高分的詞通常是該文件獨有且重要的詞，在計算相似度時的貢獻也更大\n",
        "        \"\"\"\n",
        "        return top_k_indices, top_k_scores\n",
        "\n",
        "\n",
        "class BM25Retriever:\n",
        "    \"\"\"BM25 檢索器\"\"\"\n",
        "    def __init__(self, documents, k1=1.5, b=0.75):\n",
        "        \"\"\"初始化 BM25 檢索器\"\"\"\n",
        "        self.documents = documents\n",
        "\n",
        "        # k1較大時，高詞頻的詞對分數貢獻增加，飽和程度降低 => 讓高頻詞影響更大\n",
        "        # k1較小時，高詞頻對分數的邊際影響降低，快速飽和 => 避免長文本裡同一個詞過度加分\n",
        "        self.k1 = k1\n",
        "\n",
        "        # 當 b=1 → 完全使用長度正規化，長文件的詞頻被縮小\n",
        "        # 當 b=0 → 不考慮文件長度，所有文件同樣計算\n",
        "        self.b = b\n",
        "\n",
        "        # 建立詞彙庫、文件頻率等統計數據\n",
        "        self.vocab, self.doc_freq, self.tokenized_docs, self.avg_doc_len = build_statistics(documents)\n",
        "        self.num_docs = len(self.tokenized_docs)\n",
        "        self.doc_len = [len(doc) for doc in self.tokenized_docs]\n",
        "        self.vocab_map = {word: i for i, word in enumerate(self.vocab)}\n",
        "\n",
        "        # 計算 IDF\n",
        "        self.idf = self._calculate_idf()\n",
        "\n",
        "    def _calculate_idf(self):\n",
        "        \"\"\"計算BM25的IDF分數\"\"\"\n",
        "        idf = np.zeros(len(self.vocab))\n",
        "        for i, word in enumerate(self.vocab):\n",
        "            idf[i] = np.log(((self.num_docs - self.doc_freq[word] + 0.5) / (self.doc_freq[word] + 0.5)) + 1)\n",
        "        return idf\n",
        "\n",
        "    def retrieve(self, query, k=10, query_expansion=False):\n",
        "        \"\"\"根據查詢檢索文件\"\"\"\n",
        "        query_tokens = tokenize(query)\n",
        "        if query_expansion:\n",
        "            lemmatized_tokens = lemmatize_tokens(query_tokens)\n",
        "            query_tokens.extend(lemmatized_tokens)\n",
        "        scores = np.zeros(self.num_docs)\n",
        "\n",
        "        for i in range(self.num_docs):\n",
        "            tf = Counter(self.tokenized_docs[i])\n",
        "            score = 0\n",
        "            for word in query_tokens:\n",
        "                if word in self.vocab_map:\n",
        "                    tf_word = tf[word]\n",
        "                    idf_word = self.idf[self.vocab_map[word]]\n",
        "                    # 計算BM25分數\n",
        "                    score += idf_word * (tf_word * (self.k1 + 1)) / (tf_word + self.k1 * (1 - self.b + self.b * self.doc_len[i] / self.avg_doc_len))\n",
        "            scores[i] = score\n",
        "\n",
        "        # 取得前k個結果\n",
        "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
        "        top_k_scores = scores[top_k_indices]\n",
        "        return top_k_indices, top_k_scores\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 載入並預處理資料\n",
        "    code_snippets_df = load_code_snippets('code_snippets.csv')\n",
        "    processed_df = preprocess(code_snippets_df)\n",
        "\n",
        "    # 初始化檢索器\n",
        "    tfidf_retriever = TFIDFRetriever(processed_df)\n",
        "    bm25_retriever = BM25Retriever(processed_df)\n",
        "\n",
        "    # 範例查詢\n",
        "    query = \"add two numbers\"\n",
        "\n",
        "    # TF-IDF\n",
        "    tfidf_top_k = tfidf_retriever.retrieve(query)\n",
        "    print(f\"查詢: '{query}'\")\n",
        "    print(f\"TF-IDF 前 10 個檢索到的文件 ID: {tfidf_top_k}\")\n",
        "\n",
        "    # BM25\n",
        "    bm25_top_k = bm25_retriever.retrieve(query)\n",
        "    print(f\"BM25 前 10 個檢索到的文件 ID: {bm25_top_k}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvEl8x6N7e6C",
        "outputId": "bbb2e09f-7a0d-4995-911e-420c6f3ac630"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "查詢: 'add two numbers'\n",
            "TF-IDF 前 10 個檢索到的文件 ID: (array([214, 452, 288, 271, 126, 169, 168, 167, 166, 165]), array([0.23646849, 0.16435265, 0.13304508, 0.11034009, 0.08994983,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]))\n",
            "BM25 前 10 個檢索到的文件 ID: (array([214, 452, 288, 271, 126, 169, 168, 167, 166, 165]), array([6.30910208, 5.31547725, 4.64946264, 3.44801661, 3.10240623,\n",
            "       0.        , 0.        , 0.        , 0.        , 0.        ]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**評估sparse_retrieval本地分數**"
      ],
      "metadata": {
        "id": "1p3MG-Sz7sD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh2G1t8o8LX9",
        "outputId": "dffa0494-c015-4441-d26a-fabe5b2c6aef"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 由於提供的檔案中沒有用於驗證稀疏檢所器的data，此處利用用於微調密集檢索器的資料train_queries.csv建立一個新的語料庫以及對應的問題-答案集，用於評估當前檢索器的效能\n",
        "\n",
        "\n",
        "def evaluate(retriever, df, query_expansion=False):\n",
        "    \"\"\"\n",
        "    在完整的資料集上評估檢索器的效能\n",
        "    資料集同時作為語料庫和查詢集\n",
        "    \"\"\"\n",
        "    recall_at_10 = 0\n",
        "    # 使用tqdm顯示進度條\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Evaluating {retriever.__class__.__name__}\"):\n",
        "        query = row['query']\n",
        "        true_code_id = row['code_id']\n",
        "\n",
        "        top_k_indices, _ = retriever.retrieve(query, k=10, query_expansion=query_expansion)\n",
        "        top_k_code_ids = retriever.documents.iloc[top_k_indices]['code_id'].tolist()\n",
        "\n",
        "        if true_code_id in top_k_code_ids:\n",
        "            recall_at_10 += 1\n",
        "\n",
        "    return recall_at_10 / len(df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 載入train_queries.csv\n",
        "    print(\"Loading train_queries.csv for self-evaluation...\")\n",
        "    queries_df = pd.read_csv('train_queries.csv')\n",
        "\n",
        "    # 建立一個唯一的code_id作為真實答案\n",
        "    queries_df['code_id'] = range(len(queries_df))\n",
        "\n",
        "    # --- 預處理語料庫 ---\n",
        "    # 在此情境下，完整的queries_df就是我們的語料庫\n",
        "    print(\"\\nPreprocessing corpus...\")\n",
        "    processed_corpus = preprocess(queries_df.copy(), n_gram_range=(1, 1))\n",
        "\n",
        "    # --- 實驗一: 基本Unigram(無查詢擴充) ---\n",
        "    print(\"\\n--- Evaluating with Unigrams (No Query Expansion) ---\")\n",
        "    # 初始化檢索器\n",
        "    tfidf_retriever = TFIDFRetriever(processed_corpus)\n",
        "    bm25_retriever = BM25Retriever(processed_corpus)\n",
        "\n",
        "    # 在完整的資料集上進行評估\n",
        "    tfidf_recall = evaluate(tfidf_retriever, queries_df, query_expansion=False)\n",
        "    bm25_recall = evaluate(bm25_retriever, queries_df, query_expansion=False)\n",
        "\n",
        "    print(f\"TF-IDF Recall@10: {tfidf_recall:.4f}\")\n",
        "    print(f\"BM25 Recall@10: {bm25_recall:.4f}\")\n",
        "\n",
        "    # --- 實驗二:Unigram+查詢擴充 ---\n",
        "    print(\"\\n--- Evaluating with Unigrams (With Query Expansion) ---\")\n",
        "    # 檢索器已建立，直接調用評估函式並開啟查詢擴充\n",
        "    tfidf_recall_qe = evaluate(tfidf_retriever, queries_df, query_expansion=True)\n",
        "    bm25_recall_qe = evaluate(bm25_retriever, queries_df, query_expansion=True)\n",
        "\n",
        "    print(f\"TF-IDF with Query Expansion Recall@10: {tfidf_recall_qe:.4f}\")\n",
        "    print(f\"BM25 with Query Expansion Recall@10: {bm25_recall_qe:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iWlnbNj7vp_",
        "outputId": "c0584f0f-c4ed-4ffa-aafb-dfb0facf6adf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train_queries.csv for self-evaluation...\n",
            "\n",
            "Preprocessing corpus...\n",
            "\n",
            "--- Evaluating with Unigrams (No Query Expansion) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TFIDFRetriever: 100%|██████████| 500/500 [00:03<00:00, 136.78it/s]\n",
            "Evaluating BM25Retriever: 100%|██████████| 500/500 [00:10<00:00, 49.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Recall@10: 0.7660\n",
            "BM25 Recall@10: 0.6680\n",
            "\n",
            "--- Evaluating with Unigrams (With Query Expansion) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating TFIDFRetriever: 100%|██████████| 500/500 [00:08<00:00, 55.78it/s]\n",
            "Evaluating BM25Retriever: 100%|██████████| 500/500 [00:16<00:00, 29.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF with Query Expansion Recall@10: 0.7860\n",
            "BM25 with Query Expansion Recall@10: 0.6720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**生成篩選困難負樣本的資料**"
      ],
      "metadata": {
        "id": "LcImUW9F9dVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def prepare_hard_negatives(top_k=50):\n",
        "    \"\"\"\n",
        "    為訓練查詢生成困難負樣本。\n",
        "    對於train_queries.csv（訓練資料）中的每個查詢，使用TF-IDF在code_snippets.csv中（test_queries的語料庫，也是微調模型時的負樣本）\n",
        "    檢索top-k的相似程式碼，並將其作為困難負樣本儲存。\n",
        "    \"\"\"\n",
        "    print(\"--- 開始生成困難負樣本 ---\")\n",
        "\n",
        "    # 1. 載入所有需要的資料\n",
        "    print(\"步驟 1/4: 載入資料...\")\n",
        "    train_queries_df = pd.read_csv('train_queries.csv')\n",
        "    code_snippets_df = load_code_snippets('code_snippets.csv')\n",
        "\n",
        "    # 2. 預處理程式碼片段並初始化TF-IDF檢索器\n",
        "    print(\"步驟 2/4: 初始化 TF-IDF 檢索器...\")\n",
        "    processed_snippets_df = preprocess(code_snippets_df)\n",
        "    # 使用TF-IDF檢索器\n",
        "    tfidf_retriever = TFIDFRetriever(processed_snippets_df)\n",
        "    print(\"檢索器初始化完成。\")\n",
        "\n",
        "    # 3. 為每個訓練查詢尋找困難負樣本\n",
        "    print(\"步驟 3/4: 挖掘困難負樣本...\")\n",
        "    training_data_with_negatives = []\n",
        "    for _, row in tqdm(train_queries_df.iterrows(), total=len(train_queries_df), desc=\"處理查詢\"):\n",
        "        query = row['query']\n",
        "        positive_code = row['code']\n",
        "\n",
        "        # 使用 TF-IDF 檢索 Top-K個候選\n",
        "        # 由於 train_queries.csv 中的 code 不存在於 code_snippets.csv 中，不需要擔心檢索到正樣本\n",
        "        top_indices, _ = tfidf_retriever.retrieve(query, k=top_k, query_expansion=True)\n",
        "\n",
        "        # 將檢索到的索引 (indices) 轉換為實際的 code_id（code_snippets中有code_id）\n",
        "        hard_negative_ids = [int(code_snippets_df.iloc[i]['code_id']) for i in top_indices]\n",
        "\n",
        "        training_data_with_negatives.append({\n",
        "            'query': query,\n",
        "            'positive_code': positive_code,\n",
        "            'hard_negative_ids': hard_negative_ids\n",
        "        })\n",
        "\n",
        "    # 4. 儲存結果到JSON檔案\n",
        "    output_path = 'train_data_with_negatives.json'\n",
        "    print(f\"步驟 4/4: 儲存結果到 {output_path}...\")\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(training_data_with_negatives, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"--- 成功生成並儲存 {len(training_data_with_negatives)} 筆含困難負樣本的訓練資料 ---\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    prepare_hard_negatives()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJGGHP3_9igj",
        "outputId": "27744012-d5ed-4ef8-c6aa-2b3ed53d42fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 開始生成困難負樣本 ---\n",
            "步驟 1/4: 載入資料...\n",
            "步驟 2/4: 初始化 TF-IDF 檢索器...\n",
            "檢索器初始化完成。\n",
            "步驟 3/4: 挖掘困難負樣本...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "處理查詢: 100%|██████████| 500/500 [00:04<00:00, 102.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "步驟 4/4: 儲存結果到 train_data_with_negatives.json...\n",
            "--- 成功生成並儲存 500 筆含困難負樣本的訓練資料 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterative Hard Negative Mining（迭代式困難負樣本挖掘）\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import util\n",
        "\n",
        "# ---------------------- 設定 ----------------------\n",
        "FINE_TUNED_MODEL_PATH_LOCAL = \"microsoft-unixcoder-base_5_Epoch_itr1\"\n",
        "num_layers = 1  # 取最後幾層平均\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "TOP_K = 50\n",
        "BATCH_SIZE = 32\n",
        "OUTPUT_JSON = 'train_data_with_dense_negatives.json'\n",
        "\n",
        "# ---------------------- 主函數 ----------------------\n",
        "def prepare_dense_hard_negatives():\n",
        "    print(\"--- 開始生成困難負樣本（Dense Model） ---\")\n",
        "\n",
        "    # 1. 載入資料\n",
        "    print(\"步驟 1/4: 載入資料...\")\n",
        "    train_queries_df = pd.read_csv('train_queries.csv')\n",
        "    code_snippets_df = pd.read_csv('code_snippets.csv')\n",
        "\n",
        "    # 2. 載入模型與 tokenizer\n",
        "    print(\"步驟 2/4: 初始化模型並編碼程式碼片段...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH_LOCAL)\n",
        "    model = AutoModel.from_pretrained(FINE_TUNED_MODEL_PATH_LOCAL).to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    # 2a. 計算 corpus embeddings\n",
        "    all_codes = list(code_snippets_df['code'])\n",
        "    corpus_embeddings = []\n",
        "    for i in tqdm(range(0, len(all_codes), BATCH_SIZE), desc=\"Corpus Embeddings\"):\n",
        "        batch_codes = all_codes[i:i+BATCH_SIZE]\n",
        "        inputs = tokenizer(batch_codes, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states  # tuple of layers\n",
        "            stacked_layers = torch.stack(hidden_states[-num_layers:])  # shape: [num_layers, batch, seq_len, hidden]\n",
        "            mean_last_layers = torch.mean(stacked_layers, dim=0)      # [batch, seq_len, hidden]\n",
        "            embeddings = mean_last_layers.mean(dim=1)                  # [batch, hidden] mean pooling\n",
        "        corpus_embeddings.append(embeddings.cpu())\n",
        "    corpus_embeddings = torch.cat(corpus_embeddings, dim=0)\n",
        "    print(\"程式碼嵌入完成。\")\n",
        "\n",
        "    # 3. 對每個 query 生成困難負樣本\n",
        "    print(\"步驟 3/4: 挖掘困難負樣本...\")\n",
        "    training_data_with_negatives = []\n",
        "\n",
        "    for _, row in tqdm(train_queries_df.iterrows(), total=len(train_queries_df), desc=\"處理查詢\"):\n",
        "        query = row['query']\n",
        "        positive_code = row['code']\n",
        "\n",
        "        # query embedding\n",
        "        inputs = tokenizer(query, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states\n",
        "            stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "            mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "            query_emb = mean_last_layers.mean(dim=1)  # [1, hidden_size]\n",
        "        query_emb = query_emb.cpu()\n",
        "\n",
        "        # cosine similarity\n",
        "        similarities = util.cos_sim(query_emb, corpus_embeddings)[0]\n",
        "\n",
        "        # top-k\n",
        "        top_indices = torch.topk(similarities, k=TOP_K).indices.tolist()\n",
        "        hard_negative_ids = [int(code_snippets_df.iloc[i]['code_id']) for i in top_indices]\n",
        "\n",
        "        training_data_with_negatives.append({\n",
        "            'query': query,\n",
        "            'positive_code': positive_code,\n",
        "            'hard_negative_ids': hard_negative_ids\n",
        "        })\n",
        "\n",
        "    # 4. 儲存 JSON\n",
        "    print(f\"步驟 4/4: 儲存結果到 {OUTPUT_JSON}...\")\n",
        "    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n",
        "        json.dump(training_data_with_negatives, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    print(f\"--- 成功生成並儲存 {len(training_data_with_negatives)} 筆含 Dense 困難負樣本的訓練資料 ---\")\n",
        "\n",
        "\n",
        "# ---------------------- 執行 ----------------------\n",
        "if __name__ == '__main__':\n",
        "    prepare_dense_hard_negatives()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00fkRCEyrst7",
        "outputId": "cf41a2c2-eab5-4817-f8d4-485c49923e78"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 開始生成困難負樣本（Dense Model） ---\n",
            "步驟 1/4: 載入資料...\n",
            "步驟 2/4: 初始化模型並編碼程式碼片段...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:12<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "程式碼嵌入完成。\n",
            "步驟 3/4: 挖掘困難負樣本...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "處理查詢: 100%|██████████| 500/500 [00:15<00:00, 33.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "步驟 4/4: 儲存結果到 train_data_with_dense_negatives.json...\n",
            "--- 成功生成並儲存 500 筆含 Dense 困難負樣本的訓練資料 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**密集模型微調**"
      ],
      "metadata": {
        "id": "3B-3axh08mrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "# 可選擇'top5_single'或'stratified_multi'，在Kaggle分數最高的前兩個策略\n",
        "STRATEGY = 'top5_single'\n",
        "\n",
        "# 由TF-IDF篩選的負樣本：'train_data_with_negatives.json'\n",
        "# 由微調出的Dense Model篩選的負樣本(TF-IDF篩選的迭代)：'train_data_with_dense_negatives.json'\n",
        "negative_jason = 'train_data_with_dense_negatives.json'\n",
        "\n",
        "\n",
        "num_layers = 1 # 選擇要用最後幾層的Output平均作為特徵\n",
        "Epochs = 5\n",
        "Learning_Rate = 2e-5\n",
        "Batch_Size = 8\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# 【每個正樣本要搭配的困難負樣本數量\n",
        "NUM_NEGATIVES_PER_POSITIVE = 4 # 每個正樣本要搭配的困難負樣本數量\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    \"\"\"三元組數據集，用於微調密集檢索模型\"\"\"\n",
        "    def __init__(self, train_data_with_negatives, code_id_to_code_map, strategy = 'top5_single'):\n",
        "        \"\"\"初始化數據集，並在此處完成數據增強（分層抽樣）\"\"\"\n",
        "        self.triplets = []\n",
        "        self.strategy = strategy\n",
        "        print(\"\\nCreating training triplets with STRATIFIED negatives...\")\n",
        "\n",
        "        # 此處挑選在Kaggle分數最高的前兩個策略\n",
        "        if self.strategy == 'top5_single':\n",
        "            print(\"\\nCreating training triplets with Top-5 Single Negative strategy...\")\n",
        "            for item in tqdm(train_data_with_negatives):\n",
        "                query = item['query']\n",
        "                positive_code = item['positive_code']\n",
        "                hard_negatives_pool = item['hard_negative_ids'][:5] # 只取前 5 個\n",
        "                if hard_negatives_pool:\n",
        "                    neg_id = random.choice(hard_negatives_pool)\n",
        "                    negative_code = code_id_to_code_map[neg_id]\n",
        "                    self.triplets.append([query, positive_code, negative_code])\n",
        "\n",
        "        elif self.strategy == 'stratified_multi':\n",
        "            print(\"\\nCreating training triplets with Stratified Multi-Negative strategy...\")\n",
        "            for item in tqdm(train_data_with_negatives):\n",
        "                query = item['query']\n",
        "                positive_code = item['positive_code']\n",
        "                hard_negatives_pool = item['hard_negative_ids'] # 使用全部 Top 50\n",
        "\n",
        "                if hard_negatives_pool:\n",
        "                    # 實作分層抽樣\n",
        "                    # 層 1: Top 1-10\n",
        "                    # 層 2: Top 11-20\n",
        "                    # 層 3: Top 21-35\n",
        "                    # 層 4: Top 36-50\n",
        "                    strata = [hard_negatives_pool[0:10], hard_negatives_pool[10:20], hard_negatives_pool[20:35], hard_negatives_pool[35:50]]\n",
        "                    for stratum in strata:\n",
        "                        if stratum:\n",
        "                            neg_id = random.choice(stratum)\n",
        "                            negative_code = code_id_to_code_map[neg_id]\n",
        "                            self.triplets.append([query, positive_code, negative_code])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid strategy specified. Choose 'top5_single' or 'stratified_multi'.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"返回數據集的大小\"\"\"\n",
        "        return len(self.triplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"獲取一個數據樣本 (一個三元組)\"\"\"\n",
        "        return self.triplets[idx]\n",
        "\n",
        "def collate_fn(batch, tokenizer, max_length=512):\n",
        "    \"\"\"將 batch 的文字一次性 tokenizer，提高效率\"\"\"\n",
        "    anchors, positives, negatives = zip(*batch)\n",
        "\n",
        "    anchor_inputs = tokenizer(list(anchors), return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "    positive_inputs = tokenizer(list(positives), return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "    negative_inputs = tokenizer(list(negatives), return_tensors='pt', truncation=True, padding='max_length', max_length=max_length)\n",
        "\n",
        "    return {\n",
        "        'anchor': {key: val.to(DEVICE) for key, val in anchor_inputs.items()},\n",
        "        'positive': {key: val.to(DEVICE) for key, val in positive_inputs.items()},\n",
        "        'negative': {key: val.to(DEVICE) for key, val in negative_inputs.items()}\n",
        "    }\n",
        "\n",
        "def get_embedding(model, tokenizer, text, max_length=512):\n",
        "    \"\"\"輔助函式，用於獲取單個文本的嵌入向量\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        # 判斷模型是否為 Encoder-Decoder 架構\n",
        "        if hasattr(model, 'get_encoder'):\n",
        "            outputs = model.get_encoder()(**inputs, output_hidden_states=True)\n",
        "        else:\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "        stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "        mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "        embedding = mean_last_layers.mean(dim=1)\n",
        "    return embedding.cpu()\n",
        "\n",
        "# 將anchor、positive、negative的token輸入模型，取多層hidden_state做平均\n",
        "def get_layerwise_embeddings(model, batch_inputs, num_layers=num_layers):\n",
        "    \"\"\"\n",
        "    batch_inputs: batch['anchor'] / batch['positive'] / batch['negative']\n",
        "    num_layers: 取最後幾層做平均（作為文字的特徵）\n",
        "    \"\"\"\n",
        "    # Check if the model has an encoder (i.e., is an encoder-decoder model)\n",
        "    if hasattr(model, 'get_encoder'):\n",
        "        outputs = model.get_encoder()(input_ids=batch_inputs['input_ids'],attention_mask=batch_inputs['attention_mask'], output_hidden_states=True)\n",
        "    else:\n",
        "        outputs = model(**batch_inputs, output_hidden_states=True)\n",
        "\n",
        "    hidden_states = outputs.hidden_states  # tuple of all layers\n",
        "\n",
        "    # 取最後 num_layers 層平均\n",
        "    stacked_layers = torch.stack(hidden_states[-num_layers:])  # shape: (num_layers, batch_size, seq_len, hidden_size)\n",
        "    mean_last_layers = torch.mean(stacked_layers, dim=0)      # shape: (batch_size, seq_len, hidden_size)\n",
        "\n",
        "    # 對 token 平均 pooling，得到每個樣本的句子向量\n",
        "    embeddings = mean_last_layers.mean(dim=1)  # shape: (batch_size, hidden_size)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def evaluate_recall(model, tokenizer, val_df, corpus_df, cached_corpus_embeddings=None):\n",
        "    model.eval()\n",
        "    #  先計算全部的語料庫特徵\n",
        "    if cached_corpus_embeddings is None:\n",
        "        print(\"\\nCreating cached embeddings for the corpus...\")\n",
        "        all_codes = list(corpus_df['code'])\n",
        "        corpus_embeddings = []\n",
        "        batch_size = 32\n",
        "        for i in tqdm(range(0, len(all_codes), batch_size), desc=\"Corpus Embeddings\"):\n",
        "            batch_codes = all_codes[i:i+batch_size]\n",
        "            inputs = tokenizer(batch_codes, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs, output_hidden_states=True)\n",
        "                hidden_states = outputs.hidden_states\n",
        "                stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "                mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "                embeddings = mean_last_layers.mean(dim=1)\n",
        "            corpus_embeddings.append(embeddings.cpu())\n",
        "        corpus_embeddings = torch.cat(corpus_embeddings, dim=0)\n",
        "    else:\n",
        "        corpus_embeddings = cached_corpus_embeddings\n",
        "\n",
        "    recall_at_10 = 0\n",
        "    for _, row in tqdm(val_df.iterrows(), total=val_df.shape[0], desc=\"Evaluating Recall@10\"):\n",
        "        query = row['query']\n",
        "        true_code_string = row['code']\n",
        "        query_embedding = get_embedding(model, tokenizer, query)\n",
        "\n",
        "        # 計算餘弦相似度\n",
        "        scores = torch.nn.functional.cosine_similarity(query_embedding, corpus_embeddings)\n",
        "        top_k_indices = torch.argsort(scores, descending=True)[:10]\n",
        "        top_k_codes = corpus_df.iloc[top_k_indices]['code'].values\n",
        "        if true_code_string in top_k_codes:\n",
        "            recall_at_10 += 1\n",
        "    return recall_at_10 / len(val_df), corpus_embeddings\n",
        "\n",
        "class DenseRetriever:\n",
        "    \"\"\"密集檢索器\"\"\"\n",
        "    def __init__(self, documents, model_name_or_path, batch_size=32):\n",
        "        \"\"\"初始化密集檢索器\"\"\"\n",
        "        self.documents = documents\n",
        "        # 載入預訓練模型和斷詞器\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "        self.model = AutoModel.from_pretrained(model_name_or_path)\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval() # 預設為評估模式\n",
        "        self.batch_size = batch_size\n",
        "        # 建立文件的嵌入向量（使用 batch 化）\n",
        "        self.doc_embeddings = self._create_doc_embeddings()\n",
        "\n",
        "    def _create_doc_embeddings(self):\n",
        "        \"\"\"建立所有文件的嵌入向量 (batch 化加速)\"\"\"\n",
        "        all_codes = list(self.documents['code'])\n",
        "        embeddings = []\n",
        "        for i in tqdm(range(0, len(all_codes), self.batch_size), desc=\"Creating document embeddings\"):\n",
        "            batch_codes = all_codes[i:i+self.batch_size]\n",
        "            inputs = self.tokenizer(batch_codes, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                # Check if the model has an encoder (i.e., is an encoder-decoder model)\n",
        "                if hasattr(self.model, 'get_encoder'):\n",
        "                    outputs = self.model.get_encoder()(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], output_hidden_states=True)\n",
        "                else:\n",
        "                     outputs = self.model(**inputs, output_hidden_states=True)\n",
        "                hidden_states = outputs.hidden_states\n",
        "                stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "                mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "                batch_embeddings = mean_last_layers.mean(dim=1)\n",
        "            embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "        embeddings = np.vstack(embeddings)\n",
        "        # 對所有文件嵌入向量進行 L2 正規化\n",
        "        norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        embeddings = embeddings / norms\n",
        "        return embeddings\n",
        "\n",
        "    def retrieve(self, query, k=10):\n",
        "        \"\"\"根據查詢檢索文件\"\"\"\n",
        "        # 對單一 query 編碼\n",
        "        query_embedding = get_embedding(self.model, self.tokenizer, query).cpu().numpy()\n",
        "        # 對查詢嵌入向量進行 L2 正規化\n",
        "        query_norm = np.linalg.norm(query_embedding)\n",
        "        query_embedding = query_embedding / query_norm\n",
        "\n",
        "        # 計算餘弦相似度 (經過正規化後點積等同於餘弦相似度)\n",
        "        scores = np.dot(self.doc_embeddings, query_embedding.T).flatten()\n",
        "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
        "        top_k_scores = scores[top_k_indices]\n",
        "        return top_k_indices, top_k_scores\n",
        "\n",
        "def split_data(train_queries_df):\n",
        "    # 90% 的code-query配對用於訓練，剩餘10%的query用於評估並對答案\n",
        "    # 每個 code 是一個 group\n",
        "    groups = train_queries_df['code']\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "    train_idx, val_idx = next(gss.split(train_queries_df, groups=groups))\n",
        "    train_df = train_queries_df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = train_queries_df.iloc[val_idx].reset_index(drop=True)\n",
        "    return train_df, val_df\n",
        "\n",
        "\n",
        "def fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, strategy, epochs=3, lr=2e-5, batch_size=8):\n",
        "    \"\"\"微調預訓練模型\"\"\"\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # 這是對訓練資料的準備，會產生每個樣本的anchor/positive/negative張量\n",
        "    dataset = TripletDataset(train_data_with_negatives, code_id_to_code_map, strategy)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,\n",
        "                            collate_fn=lambda x: collate_fn(x, tokenizer))  # 使用 collate_fn 做 batch tokenizer\n",
        "\n",
        "    # 設定優化器和損失函數\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr) # 標準Transformer訓練用優化器\n",
        "    loss_fn = torch.nn.TripletMarginLoss(margin=1.0) # 三元組損失，目標是讓anchor（查詢）與positive（正確答案之間的距離小於anchor與negative（錯誤答案）之間的距離，至少相差一個margin\n",
        "\n",
        "    # 訓練模型\n",
        "    for epoch in range(epochs):\n",
        "        model.train() # 切換到訓練模式\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "            optimizer.zero_grad() # 清除上一步梯度\n",
        "\n",
        "            anchor_embeddings = get_layerwise_embeddings(model, batch['anchor'])\n",
        "            positive_embeddings = get_layerwise_embeddings(model, batch['positive'])\n",
        "            negative_embeddings = get_layerwise_embeddings(model, batch['negative'])\n",
        "\n",
        "            # 計算損失\n",
        "            loss = loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "            loss.backward()  # 計算梯度\n",
        "            optimizer.step()  # 更新參數\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 1. 準備資料 ---\n",
        "    print(\"--- Preparing Data for Fine-tuning ---\")\n",
        "\n",
        "    with open(negative_jason, 'r', encoding='utf-8') as f:\n",
        "        #  這是事先根據train_queries以及code_snippests製作的每個query對應的code以及用TF-IDF挑選出的前50個負樣本ID\n",
        "        train_data_with_negatives = json.load(f)\n",
        "\n",
        "    code_snippets_df = pd.read_csv('code_snippets.csv')\n",
        "    code_id_to_code_map = pd.Series(code_snippets_df.code.values, index=code_snippets_df.code_id).to_dict()\n",
        "\n",
        "    # --- 2. 初始化模型 ---\n",
        "    print(\"\\n--- Initializing Model ---\")\n",
        "    model_name = PRE_TRAINED_MODEL_NAME\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    # --- 3. 微調模型 ---\n",
        "    print(\"\\n--- Fine-tuning model with Multi-Negative Strategy ---\")\n",
        "    fine_tuned_model = fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, STRATEGY, epochs=Epochs, lr=Learning_Rate, batch_size=Batch_Size)\n",
        "\n",
        "    # --- 4. 儲存模型 ---\n",
        "    output_dir = FINE_TUNED_MODEL_PATH\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    print(f\"\\nSaving the final model to {output_dir}...\")\n",
        "    fine_tuned_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    print(\"\\n--- Model training complete. ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCaH979wGuwo",
        "outputId": "40abbf05-428a-49a4-f496-c4320e76b720"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Preparing Data for Fine-tuning ---\n",
            "\n",
            "--- Initializing Model ---\n",
            "\n",
            "--- Fine-tuning model with Multi-Negative Strategy ---\n",
            "\n",
            "Creating training triplets with STRATIFIED negatives...\n",
            "\n",
            "Creating training triplets with Top-5 Single Negative strategy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 358120.22it/s]\n",
            "Epoch 1/5: 100%|██████████| 63/63 [02:04<00:00,  1.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Loss: 0.2977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 63/63 [02:07<00:00,  2.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Average Loss: 0.0309\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 63/63 [02:08<00:00,  2.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Average Loss: 0.0004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 63/63 [02:09<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Average Loss: 0.0013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 63/63 [02:09<00:00,  2.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Average Loss: 0.0066\n",
            "\n",
            "Saving the final model to ./microsoft-unixcoder-base...\n",
            "\n",
            "--- Model training complete. ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**預訓練密集模型與微調密集模型本地驗證**"
      ],
      "metadata": {
        "id": "nnSPdviG8w7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# dense_retrieval\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "\n",
        "FINE_TUNED_MODEL_PATH_LOCAL = FINE_TUNED_MODEL_PATH\n",
        "\n",
        "\n",
        "# 選擇要測試的model\n",
        "try_prtrained_model = False\n",
        "try_fine_tuned_model = True\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. 載入資料並準備驗證集\n",
        "    print(\"--- 1. 載入資料並準備驗證集 ---\")\n",
        "    # 注意，此處用於測試的語料庫也來自於train_queries.csv，訓練時保留了10%的query沒有用於訓練\n",
        "    train_queries_df = pd.read_csv('train_queries.csv')\n",
        "\n",
        "\n",
        "    # 使用與微調腳本完全相同的分割方式\n",
        "    _, val_df = split_data(train_queries_df)\n",
        "    print(f\"已載入 {len(val_df)} 筆樣本用於驗證。\")\n",
        "    # 2. 評估預訓練模型\n",
        "    if try_prtrained_model:\n",
        "        print(\"\\n--- 2. 評估預訓練模型 ---\")\n",
        "        print(f\"模型: {FINE_TUNED_MODEL_PATH_LOCAL}\")\n",
        "        pretrained_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH_LOCAL)\n",
        "        pretrained_model = AutoModel.from_pretrained(FINE_TUNED_MODEL_PATH_LOCAL).to(DEVICE)\n",
        "\n",
        "        pretrained_recall, corpus_embeddings_pretrained = evaluate_recall(pretrained_model, pretrained_tokenizer, val_df, train_queries_df)\n",
        "        print(f\"\\n預訓練模型 Recall@10: {pretrained_recall:.4f}\")\n",
        "\n",
        "    # 3. 評估微調後的模型\n",
        "    if try_fine_tuned_model:\n",
        "        print(\"\\n--- 3. 評估微調後的模型 ---\")\n",
        "        print(f\"模型: {FINE_TUNED_MODEL_PATH_LOCAL}\")\n",
        "        try:\n",
        "            finetuned_tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH_LOCAL)\n",
        "            finetuned_model = AutoModel.from_pretrained(FINE_TUNED_MODEL_PATH_LOCAL).to(DEVICE)\n",
        "\n",
        "            # 微調後的模型需要重新計算語料庫的嵌入向量\n",
        "            finetuned_recall, _ = evaluate_recall(finetuned_model, finetuned_tokenizer, val_df, train_queries_df)\n",
        "            print(f\"\\n微調後模型 Recall@10: {finetuned_recall:.4f}\")\n",
        "\n",
        "        except OSError:\n",
        "            print(f\"錯誤: 在 '{FINE_TUNED_MODEL_PATH_LOCAL}' 找不到微調後的模型。\")\n",
        "            print(\"請先執行 'fine_tune_model.py' 來訓練並儲存模型。\")\n",
        "\n",
        "    print(\"\\n--- 評估完成 ---\")"
      ],
      "metadata": {
        "id": "AqowqPl6hGff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f85f82d-4cea-48e6-be78-f122f4f50a75"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 載入資料並準備驗證集 ---\n",
            "已載入 50 筆樣本用於驗證。\n",
            "\n",
            "--- 3. 評估微調後的模型 ---\n",
            "模型: ./microsoft-unixcoder-base\n",
            "\n",
            "Creating cached embeddings for the corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:13<00:00,  1.14it/s]\n",
            "Evaluating Recall@10: 100%|██████████| 50/50 [00:01<00:00, 31.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "微調後模型 Recall@10: 0.9800\n",
            "\n",
            "--- 評估完成 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**生成四種模型的Submission**"
      ],
      "metadata": {
        "id": "kEAZ0iwq9BDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 生成dense model的submission\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "Sparse = False\n",
        "Dense = True\n",
        "\n",
        "def generate_submission(retriever, test_df, output_path, query_expansion=False):\n",
        "    \"\"\"\n",
        "    Generates a submission file for a given retriever.\n",
        "    \"\"\"\n",
        "    print(f\"Generating submission for {output_path}...\")\n",
        "    results = []\n",
        "    # 使用tqdm顯示進度條\n",
        "    for _, row in tqdm(test_df.iterrows(), total=test_df.shape[0], desc=output_path):\n",
        "        query_id = row['query_id']\n",
        "        query = row['query']\n",
        "        top_k_indices, _ = retriever.retrieve(query, k=10)\n",
        "\n",
        "        # 直接使用檢索器內部儲存的 documents DataFrame 來獲取 code_id\n",
        "        top_k_code_ids = retriever.documents.iloc[top_k_indices]['code_id'].tolist()\n",
        "\n",
        "        results.append({\n",
        "            'query_id': query_id,\n",
        "            'code_id': ' '.join(map(str, top_k_code_ids))\n",
        "        })\n",
        "\n",
        "    submission_df = pd.DataFrame(results)\n",
        "    submission_df.to_csv(output_path, index=False)\n",
        "    print(f\"Submission file saved to {output_path}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 載入資料 ---\n",
        "    print(\"Loading data...\")\n",
        "    code_snippets_df = pd.read_csv('code_snippets.csv')\n",
        "    test_queries_df = pd.read_csv('test_queries.csv')\n",
        "\n",
        "\n",
        "    # --- 稀疏模型 ---\n",
        "    if Sparse:\n",
        "      print(\"\\nInitializing sparse models with best parameters...\")\n",
        "      processed_snippets_df = preprocess(code_snippets_df.copy())\n",
        "\n",
        "      # TF-IDF Retriever with Query Expansion\n",
        "      tfidf_retriever = TFIDFRetriever(processed_snippets_df)\n",
        "      generate_submission(tfidf_retriever, test_queries_df, 'submission_tfidf.csv', query_expansion=True)\n",
        "\n",
        "      # BM25 Retriever with optimized parameters and Query Expansion\n",
        "      bm25_retriever = BM25Retriever(processed_snippets_df, k1=2.0, b=0.9)\n",
        "      generate_submission(bm25_retriever, test_queries_df, 'submission_bm25.csv', query_expansion=True)\n",
        "\n",
        "\n",
        "    # --- 密集模型 ---\n",
        "    if Dense:\n",
        "      # 檢查微調後的模型是否存在\n",
        "      finetuned_model_path = FINE_TUNED_MODEL_PATH\n",
        "      # 預訓練的密集檢索器\n",
        "      print(\"\\nInitializing pre-trained dense model...\")\n",
        "      pretrained_retriever = DenseRetriever(code_snippets_df, model_name_or_path=PRE_TRAINED_MODEL_NAME)\n",
        "      generate_submission(pretrained_retriever, test_queries_df, 'submission_pretrained.csv')\n",
        "\n",
        "      if not os.path.exists(finetuned_model_path):\n",
        "          print(f\"\\nFine-tuned model not found at '{finetuned_model_path}'.\")\n",
        "          print(\"Skipping submission generation for the fine-tuned model.\")\n",
        "      else:\n",
        "          # 微調後的密集檢索器\n",
        "          print(\"\\nInitializing fine-tuned dense model...\")\n",
        "          finetuned_retriever = DenseRetriever(code_snippets_df, model_name_or_path=finetuned_model_path)\n",
        "          generate_submission(finetuned_retriever, test_queries_df, 'submission_finetuned.csv')\n",
        "\n",
        "    print(\"\\nAll submission files have been generated.\")"
      ],
      "metadata": {
        "id": "YOng7GlViAM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469881aa-372b-4191-cdc7-df1f8d0a5ccd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "\n",
            "Initializing pre-trained dense model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating document embeddings: 100%|██████████| 16/16 [00:14<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission for submission_pretrained.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "submission_pretrained.csv: 100%|██████████| 500/500 [00:15<00:00, 31.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved to submission_pretrained.csv\n",
            "\n",
            "Initializing fine-tuned dense model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating document embeddings: 100%|██████████| 16/16 [00:14<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating submission for submission_finetuned.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "submission_finetuned.csv: 100%|██████████| 500/500 [00:15<00:00, 31.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved to submission_finetuned.csv\n",
            "\n",
            "All submission files have been generated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**其他**"
      ],
      "metadata": {
        "id": "k0VVkAh393rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**混合模型**"
      ],
      "metadata": {
        "id": "1uCguCHH97xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(ranked_lists, k=60):\n",
        "    \"\"\"\n",
        "    使用 RRF 演算法融合多個排名列表。\n",
        "    :param ranked_lists: 一個包含多個排名列表的列表。每個排名列表是 code_id 或 code_content 的列表。\n",
        "    :param k: RRF 演算法中的常數，通常設為 60。\n",
        "    :return: 融合併重新排序後的項目列表。\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    RRF的核心思想是完全忽略掉原始分數，只關心每個檢索器給出的排名\n",
        "    對於每一個候選的程式碼（code_id），它的最終RRF分數是它在每個檢索結果列表中的倒數排名分數的總和\n",
        "    總而言之RRF會獎勵那些在多個不同檢索系統中都穩定地排在前面的項目，它完全繞開了不同系統之間分數無法直接比較的問題\n",
        "    \"\"\"\n",
        "    rrf_scores = defaultdict(float)\n",
        "\n",
        "    for ranked_list in ranked_lists:\n",
        "        for rank, item in enumerate(ranked_list):\n",
        "            rrf_scores[item] += 1 / (k + rank + 1)\n",
        "\n",
        "    sorted_items = sorted(rrf_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    fused_list = [item[0] for item in sorted_items]\n",
        "\n",
        "    return fused_list\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- 模式設定 ---\n",
        "    # True: 執行本地驗證 (使用 train_queries.csv)\n",
        "    # False: 產生 Kaggle 提交檔案 (使用 test_queries.csv)\n",
        "    RUN_VALIDATION = True\n",
        "\n",
        "    # --- 1. 本地驗證模式 ---\n",
        "    if RUN_VALIDATION:\n",
        "        # --- 測試模式設定 ---\n",
        "        # 'tfidf': 只測試 TF-IDF 的表現\n",
        "        # 'dense': 只測試 Dense Retriever 的表現\n",
        "        # 'rrf': 測試 RRF 混合模型的表現\n",
        "        TEST_MODE = 'rrf' # 可以切換這個值來進行測試\n",
        "\n",
        "        print(f\"--- Running in Local Validation Mode (Test Mode: {TEST_MODE}) ---\")\n",
        "\n",
        "        # 載入訓練資料並分割\n",
        "        print(\"Loading and splitting train_queries.csv for validation...\")\n",
        "        train_queries_df = pd.read_csv('train_queries.csv')\n",
        "        _, val_df = split_data(train_queries_df)\n",
        "\n",
        "        # 驗證時，整個 train_queries_df 就是我們的語料庫\n",
        "        corpus_df = train_queries_df\n",
        "        print(f\"Using {len(val_df)} queries for validation against a corpus of {len(corpus_df)} code snippets.\")\n",
        "\n",
        "        # 初始化檢索器 (使用 train_queries 作為語料庫)\n",
        "        print(\"\\nInitializing retrievers for validation...\")\n",
        "        processed_corpus_df = preprocess(corpus_df.copy())\n",
        "\n",
        "        if TEST_MODE == 'tfidf' or TEST_MODE == 'rrf':\n",
        "            tfidf_retriever = TFIDFRetriever(processed_corpus_df)\n",
        "\n",
        "        if TEST_MODE == 'dense' or TEST_MODE == 'rrf':\n",
        "            finetuned_model_path = FINE_TUNED_MODEL_PATH\n",
        "            print(f\"Loading dense model from: {finetuned_model_path}\")\n",
        "            dense_retriever = DenseRetriever(corpus_df, model_name_or_path=finetuned_model_path)\n",
        "\n",
        "        top_n_candidates = 100\n",
        "        recall_at_10_count = 0\n",
        "\n",
        "        print(f\"\\nEvaluating {TEST_MODE} retrieval (top_n={top_n_candidates})...\")\n",
        "        for _, row in tqdm(val_df.iterrows(), total=val_df.shape[0]):\n",
        "            query = row['query']\n",
        "            true_code_content = row['code']\n",
        "\n",
        "            if TEST_MODE == 'tfidf':\n",
        "                tfidf_indices, _ = tfidf_retriever.retrieve(query, k=10, query_expansion=True)\n",
        "                top_10_codes = corpus_df.iloc[tfidf_indices]['code'].tolist()\n",
        "\n",
        "            elif TEST_MODE == 'dense':\n",
        "                dense_indices, _ = dense_retriever.retrieve(query, k=10)\n",
        "                top_10_codes = corpus_df.iloc[dense_indices]['code'].tolist()\n",
        "\n",
        "            elif TEST_MODE == 'rrf':\n",
        "                tfidf_indices, _ = tfidf_retriever.retrieve(query, k=top_n_candidates, query_expansion=True)\n",
        "                tfidf_ranked_codes = corpus_df.iloc[tfidf_indices]['code'].tolist()\n",
        "\n",
        "                dense_indices, _ = dense_retriever.retrieve(query, k=top_n_candidates)\n",
        "                dense_ranked_codes = corpus_df.iloc[dense_indices]['code'].tolist()\n",
        "\n",
        "                fused_ranked_list = reciprocal_rank_fusion([tfidf_ranked_codes, dense_ranked_codes])\n",
        "                top_10_codes = fused_ranked_list[:10]\n",
        "\n",
        "            if true_code_content in top_10_codes:\n",
        "                recall_at_10_count += 1\n",
        "\n",
        "        final_recall = recall_at_10_count / len(val_df)\n",
        "        print(f\"\\n--- Validation Complete ---\")\n",
        "        print(f\"Model: {TEST_MODE}, Local Recall@10: {final_recall:.4f}\")\n",
        "\n",
        "    # --- 2. Kaggle 預測模式 ---\n",
        "    else:\n",
        "        print(\"--- Running in Prediction Mode ---\")\n",
        "\n",
        "        # 載入資料\n",
        "        print(\"Loading data for prediction...\")\n",
        "        code_snippets_df = load_code_snippets('code_snippets.csv')\n",
        "        test_queries_df = pd.read_csv('test_queries.csv')\n",
        "\n",
        "        # 初始化檢索器 (使用 code_snippets 作為語料庫)\n",
        "        print(\"\\nInitializing retrievers for prediction...\")\n",
        "        processed_snippets_df = preprocess(code_snippets_df.copy())\n",
        "        tfidf_retriever = TFIDFRetriever(processed_snippets_df)\n",
        "\n",
        "        finetuned_model_path = FINE_TUNED_MODEL_PATH\n",
        "        print(f\"Loading dense model from: {finetuned_model_path}\")\n",
        "        dense_retriever = DenseRetriever(code_snippets_df, model_name_or_path=finetuned_model_path)\n",
        "\n",
        "        #  執行RRF混合檢索\n",
        "        top_n_candidates = 100\n",
        "        final_results = []\n",
        "\n",
        "        print(f\"\\nGenerating hybrid retrieval submission with RRF (top_n={top_n_candidates})...\")\n",
        "        for _, row in tqdm(test_queries_df.iterrows(), total=test_queries_df.shape[0]):\n",
        "            query_id = row['query_id']\n",
        "            query = row['query']\n",
        "\n",
        "            tfidf_indices, _ = tfidf_retriever.retrieve(query, k=top_n_candidates, query_expansion=True)\n",
        "            tfidf_ranked_ids = code_snippets_df.iloc[tfidf_indices]['code_id'].tolist()\n",
        "\n",
        "            dense_indices, _ = dense_retriever.retrieve(query, k=top_n_candidates)\n",
        "            dense_ranked_ids = code_snippets_df.iloc[dense_indices]['code_id'].tolist()\n",
        "\n",
        "            fused_ranked_list = reciprocal_rank_fusion([tfidf_ranked_ids, dense_ranked_ids])\n",
        "            top_10_code_ids = fused_ranked_list[:10]\n",
        "\n",
        "            final_results.append({\n",
        "                'query_id': query_id,\n",
        "                'code_id': ' '.join(map(str, top_10_code_ids))\n",
        "            })\n",
        "\n",
        "        # 儲存提交檔案\n",
        "        submission_df = pd.DataFrame(final_results)\n",
        "        output_path = 'submission_hybrid_rrf.csv'\n",
        "        submission_df.to_csv(output_path, index=False)\n",
        "        print(f\"\\nHybrid RRF submission file saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht9F-9Ey9_Nc",
        "outputId": "363b7e47-7ce7-42e4-947f-e304dae947d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running in Local Validation Mode (Test Mode: rrf) ---\n",
            "Loading and splitting train_queries.csv for validation...\n",
            "Using 50 queries for validation against a corpus of 500 code snippets.\n",
            "\n",
            "Initializing retrievers for validation...\n",
            "Loading dense model from: ./microsoft-unixcoder-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating document embeddings: 100%|██████████| 16/16 [00:13<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating rrf retrieval (top_n=100)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:05<00:00,  8.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Validation Complete ---\n",
            "Model: rrf, Local Recall@10: 0.8200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e95f092"
      },
      "source": [
        "# Task\n",
        "Explore different strategies to improve the Recall@10 of the dense retrieval model, including adjusting fine-tuning parameters, trying different negative sampling strategies, evaluating different model architectures, and potentially combining with a sparse retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff14cc79"
      },
      "source": [
        "## 調整微調參數\n",
        "\n",
        "### Subtask:\n",
        "嘗試不同的學習率、Batch Size 和訓練 Epochs，觀察對 Recall@10 的影響。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a1253f5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to modify the `fine_tune_model` function call in cell `lCaH979wGuwo` to experiment with different hyperparameters. I will try a few combinations and run the fine-tuning and evaluation cells for each combination. I will start with the original parameters as a baseline, then try increasing the learning rate and reducing the batch size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05d8366c",
        "outputId": "0c0930f7-3053-4ee3-e62a-df46cb7b853d"
      },
      "source": [
        "# fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, STRATEGY, epochs=3, lr=2e-5, batch_size=8)\n",
        "\n",
        "# Experiment 1: Baseline (Original Parameters) - Already executed in the notebook\n",
        "\n",
        "# Experiment 2: Increase Learning Rate\n",
        "# Recall@10：0.92400\n",
        "print(\"\\n--- Fine-tuning model with increased Learning Rate ---\")\n",
        "fine_tuned_model_lr_increased = fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, STRATEGY, epochs=3, lr=5e-5, batch_size=8)\n",
        "fine_tuned_model_lr_increased.save_pretrained(FINE_TUNED_MODEL_PATH + \"_lr_increased\")\n",
        "tokenizer.save_pretrained(FINE_TUNED_MODEL_PATH + \"_lr_increased\")\n",
        "\n",
        "# Experiment 3: Reduce Batch Size\n",
        "# Recall@10：0.92800\n",
        "print(\"\\n--- Fine-tuning model with reduced Batch Size ---\")\n",
        "fine_tuned_model_batch_reduced = fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, STRATEGY, epochs=3, lr=2e-5, batch_size=4)\n",
        "fine_tuned_model_batch_reduced.save_pretrained(FINE_TUNED_MODEL_PATH + \"_batch_reduced\")\n",
        "tokenizer.save_pretrained(FINE_TUNED_MODEL_PATH + \"_batch_reduced\")\n",
        "\n",
        "# Experiment 4: Increase Epochs (with baseline LR and BS)\n",
        "# Recall@10：0.92800\n",
        "print(\"\\n--- Fine-tuning model with increased Epochs ---\")\n",
        "fine_tuned_model_epochs_increased = fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, STRATEGY, epochs=5, lr=2e-5, batch_size=8)\n",
        "fine_tuned_model_epochs_increased.save_pretrained(FINE_TUNED_MODEL_PATH + \"_epochs_increased\")\n",
        "tokenizer.save_pretrained(FINE_TUNED_MODEL_PATH + \"_epochs_increased\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fine-tuning model with increased Learning Rate ---\n",
            "\n",
            "Creating training triplets with STRATIFIED negatives...\n",
            "\n",
            "Creating training triplets with Top-5 Single Negative strategy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 261522.88it/s]\n",
            "Epoch 1/3: 100%|██████████| 63/63 [02:16<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Loss: 0.0557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 63/63 [02:17<00:00,  2.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Average Loss: 0.0406\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 63/63 [02:16<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Average Loss: 0.0071\n",
            "\n",
            "--- Fine-tuning model with reduced Batch Size ---\n",
            "\n",
            "Creating training triplets with STRATIFIED negatives...\n",
            "\n",
            "Creating training triplets with Top-5 Single Negative strategy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 256375.55it/s]\n",
            "Epoch 1/3: 100%|██████████| 125/125 [02:27<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Loss: 0.0134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 125/125 [02:26<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Average Loss: 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 125/125 [02:26<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Average Loss: 0.0010\n",
            "\n",
            "--- Fine-tuning model with increased Epochs ---\n",
            "\n",
            "Creating training triplets with STRATIFIED negatives...\n",
            "\n",
            "Creating training triplets with Top-5 Single Negative strategy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 372495.91it/s]\n",
            "Epoch 1/5: 100%|██████████| 63/63 [02:16<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Average Loss: 0.0056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 63/63 [02:16<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Average Loss: 0.0005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 63/63 [02:16<00:00,  2.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Average Loss: 0.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 63/63 [02:16<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Average Loss: 0.0003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 63/63 [02:16<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Average Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./microsoft-unixcoder-base_epochs_increased/tokenizer_config.json',\n",
              " './microsoft-unixcoder-base_epochs_increased/special_tokens_map.json',\n",
              " './microsoft-unixcoder-base_epochs_increased/vocab.json',\n",
              " './microsoft-unixcoder-base_epochs_increased/merges.txt',\n",
              " './microsoft-unixcoder-base_epochs_increased/added_tokens.json',\n",
              " './microsoft-unixcoder-base_epochs_increased/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5af52930",
        "outputId": "b36b0c91-2fdf-43eb-80a8-3c1f53480680"
      },
      "source": [
        "# dense_retrieval evaluation with different hyperparameters\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Dictionary to store recall scores for different experiments\n",
        "recall_scores = {}\n",
        "\n",
        "# Base model path\n",
        "base_model_path = FINE_TUNED_MODEL_PATH\n",
        "\n",
        "# List of experiment names and their corresponding model paths\n",
        "experiments = {\n",
        "    \"Baseline (Epochs=3, LR=2e-5, BS=8)\": base_model_path,\n",
        "    \"Increased Learning Rate (Epochs=3, LR=5e-5, BS=8)\": base_model_path + \"_lr_increased\",\n",
        "    \"Reduced Batch Size (Epochs=3, LR=2e-5, BS=4)\": base_model_path + \"_batch_reduced\",\n",
        "    \"Increased Epochs (Epochs=5, LR=2e-5, BS=8)\": base_model_path + \"_epochs_increased\",\n",
        "}\n",
        "\n",
        "# Model and device settings\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_layers = 4 # Make sure num_layers is defined\n",
        "\n",
        "\n",
        "def split_data(train_queries_df):\n",
        "    # 90% 的code-query配對用於訓練，剩餘10%的query用於評估並對答案\n",
        "    # 每個 code 是一個 group\n",
        "    groups = train_queries_df['code']\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "    train_idx, val_idx = next(gss.split(train_queries_df, groups=groups))\n",
        "    train_df = train_queries_df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = train_queries_df.iloc[val_idx].reset_index(drop=True)\n",
        "    return train_df, val_df\n",
        "\n",
        "def get_embedding(model, tokenizer, text, max_length=512):\n",
        "    \"\"\"輔助函式，用於獲取單個文本的嵌入向量\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=max_length).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        # 判斷模型是否為 Encoder-Decoder 架構\n",
        "        if hasattr(model, 'get_encoder'):\n",
        "            outputs = model.get_encoder()(**inputs, output_hidden_states=True)\n",
        "        else:\n",
        "            outputs = model(**inputs, output_hidden_states=True)\n",
        "        hidden_states = outputs.hidden_states\n",
        "        stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "        mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "        embedding = mean_last_layers.mean(dim=1)\n",
        "    return embedding.cpu()\n",
        "\n",
        "def evaluate_recall(model, tokenizer, val_df, corpus_df, cached_corpus_embeddings=None):\n",
        "    model.eval()\n",
        "    #  先計算全部的語料庫特徵\n",
        "    if cached_corpus_embeddings is None:\n",
        "        print(\"\\nCreating cached embeddings for the corpus...\")\n",
        "        all_codes = list(corpus_df['code'])\n",
        "        corpus_embeddings = []\n",
        "        batch_size = 32\n",
        "        for i in tqdm(range(0, len(all_codes), batch_size), desc=\"Corpus Embeddings\"):\n",
        "            batch_codes = all_codes[i:i+batch_size]\n",
        "            inputs = tokenizer(batch_codes, return_tensors='pt', truncation=True, padding='max_length', max_length=512).to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs, output_hidden_states=True)\n",
        "                hidden_states = outputs.hidden_states\n",
        "                stacked_layers = torch.stack(hidden_states[-num_layers:])\n",
        "                mean_last_layers = torch.mean(stacked_layers, dim=0)\n",
        "                embeddings = mean_last_layers.mean(dim=1)\n",
        "            corpus_embeddings.append(embeddings.cpu())\n",
        "        corpus_embeddings = torch.cat(corpus_embeddings, dim=0)\n",
        "    else:\n",
        "        corpus_embeddings = cached_corpus_embeddings\n",
        "\n",
        "    recall_at_10 = 0\n",
        "    for _, row in tqdm(val_df.iterrows(), total=val_df.shape[0], desc=\"Evaluating Recall@10\"):\n",
        "        query = row['query']\n",
        "        true_code_string = row['code']\n",
        "        query_embedding = get_embedding(model, tokenizer, query)\n",
        "\n",
        "        # 計算餘弦相似度\n",
        "        scores = torch.nn.functional.cosine_similarity(query_embedding, corpus_embeddings)\n",
        "        top_k_indices = torch.argsort(scores, descending=True)[:10]\n",
        "        top_k_codes = corpus_df.iloc[top_k_indices]['code'].values\n",
        "        if true_code_string in top_k_codes:\n",
        "            recall_at_10 += 1\n",
        "    return recall_at_10 / len(val_df), corpus_embeddings\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 1. 載入資料並準備驗證集\n",
        "    print(\"--- 1. Loading Data and Preparing Validation Set ---\")\n",
        "    # 注意，此處用於測試的語料庫也來自於train_queries.csv，訓練時保留了10%的query沒有用於訓練\n",
        "    train_queries_df = pd.read_csv('train_queries.csv')\n",
        "\n",
        "\n",
        "    # 使用與微調腳本完全相同的分割方式\n",
        "    _, val_df = split_data(train_queries_df)\n",
        "    print(f\"Loaded {len(val_df)} samples for validation.\")\n",
        "\n",
        "    # 2. 評估不同的微調模型\n",
        "    print(\"\\n--- 2. Evaluating Fine-tuned Models with Different Hyperparameters ---\")\n",
        "\n",
        "    # Pre-calculate corpus embeddings for efficiency if needed (optional)\n",
        "    # This is only beneficial if the corpus doesn't change between model evaluations\n",
        "    cached_corpus_embeddings = None\n",
        "    # print(\"\\nCalculating corpus embeddings for efficiency...\")\n",
        "    # temp_tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    # temp_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME).to(DEVICE)\n",
        "    # _, cached_corpus_embeddings = evaluate_recall(temp_model, temp_tokenizer, val_df, train_queries_df, cached_corpus_embeddings=None)\n",
        "    # del temp_tokenizer, temp_model\n",
        "    # torch.cuda.empty_cache() # Clear GPU memory\n",
        "\n",
        "\n",
        "    for exp_name, model_path in experiments.items():\n",
        "        print(f\"\\n--- Evaluating: {exp_name} ---\")\n",
        "        print(f\"Model Path: {model_path}\")\n",
        "        try:\n",
        "            finetuned_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            finetuned_model = AutoModel.from_pretrained(model_path).to(DEVICE)\n",
        "\n",
        "            # Evaluate the model and get the recall score\n",
        "            finetuned_recall, _ = evaluate_recall(finetuned_model, finetuned_tokenizer, val_df, train_queries_df, cached_corpus_embeddings=cached_corpus_embeddings)\n",
        "            recall_scores[exp_name] = finetuned_recall\n",
        "            print(f\"{exp_name} Recall@10: {finetuned_recall:.4f}\")\n",
        "\n",
        "            # Clean up model and tokenizer to free GPU memory\n",
        "            del finetuned_model, finetuned_tokenizer\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        except OSError:\n",
        "            print(f\"Error: Fine-tuned model not found at '{model_path}'.\")\n",
        "            print(f\"Skipping evaluation for {exp_name}.\")\n",
        "\n",
        "\n",
        "    # 3. Print summary of results\n",
        "    print(\"\\n--- Hyperparameter Tuning Results Summary ---\")\n",
        "    for exp_name, recall in recall_scores.items():\n",
        "        print(f\"{exp_name}: Recall@10 = {recall:.4f}\")\n",
        "\n",
        "    print(\"\\n--- Evaluation Complete ---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Loading Data and Preparing Validation Set ---\n",
            "Loaded 50 samples for validation.\n",
            "\n",
            "--- 2. Evaluating Fine-tuned Models with Different Hyperparameters ---\n",
            "\n",
            "--- Evaluating: Baseline (Epochs=3, LR=2e-5, BS=8) ---\n",
            "Model Path: ./microsoft-unixcoder-base\n",
            "\n",
            "Creating cached embeddings for the corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:14<00:00,  1.10it/s]\n",
            "Evaluating Recall@10: 100%|██████████| 50/50 [00:01<00:00, 31.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline (Epochs=3, LR=2e-5, BS=8) Recall@10: 0.9400\n",
            "\n",
            "--- Evaluating: Increased Learning Rate (Epochs=3, LR=5e-5, BS=8) ---\n",
            "Model Path: ./microsoft-unixcoder-base_lr_increased\n",
            "\n",
            "Creating cached embeddings for the corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:14<00:00,  1.10it/s]\n",
            "Evaluating Recall@10: 100%|██████████| 50/50 [00:01<00:00, 31.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Increased Learning Rate (Epochs=3, LR=5e-5, BS=8) Recall@10: 0.9800\n",
            "\n",
            "--- Evaluating: Reduced Batch Size (Epochs=3, LR=2e-5, BS=4) ---\n",
            "Model Path: ./microsoft-unixcoder-base_batch_reduced\n",
            "\n",
            "Creating cached embeddings for the corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:14<00:00,  1.13it/s]\n",
            "Evaluating Recall@10: 100%|██████████| 50/50 [00:01<00:00, 32.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced Batch Size (Epochs=3, LR=2e-5, BS=4) Recall@10: 0.9200\n",
            "\n",
            "--- Evaluating: Increased Epochs (Epochs=5, LR=2e-5, BS=8) ---\n",
            "Model Path: ./microsoft-unixcoder-base_epochs_increased\n",
            "\n",
            "Creating cached embeddings for the corpus...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Corpus Embeddings: 100%|██████████| 16/16 [00:13<00:00,  1.15it/s]\n",
            "Evaluating Recall@10: 100%|██████████| 50/50 [00:01<00:00, 32.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Increased Epochs (Epochs=5, LR=2e-5, BS=8) Recall@10: 0.9400\n",
            "\n",
            "--- Hyperparameter Tuning Results Summary ---\n",
            "Baseline (Epochs=3, LR=2e-5, BS=8): Recall@10 = 0.9400\n",
            "Increased Learning Rate (Epochs=3, LR=5e-5, BS=8): Recall@10 = 0.9800\n",
            "Reduced Batch Size (Epochs=3, LR=2e-5, BS=4): Recall@10 = 0.9200\n",
            "Increased Epochs (Epochs=5, LR=2e-5, BS=8): Recall@10 = 0.9400\n",
            "\n",
            "--- Evaluation Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23e7cd0c",
        "outputId": "b1126fcb-4c93-4ca8-fdf4-50fcc8acb5ef"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "68d1896a",
        "outputId": "84388be9-4c0b-4347-8c42-c05654fd8347"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "# 選擇一個 Sentence-BERT 模型\n",
        "# 'sentence-transformers/all-MiniLM-L6-v2' 是一個常用且效率不錯的模型\n",
        "# 'sentence-transformers/multi-qa-mpnet-base-dot-v1' 在問答和檢索任務上表現較好\n",
        "SENTENCE_TRANSFORMER_MODEL = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
        "\n",
        "with open('train_data_with_negatives.json', 'r', encoding='utf-8') as f:\n",
        "  #  這是事先根據train_queries以及code_snippests製作的每個query對應的code以及用TF-IDF挑選出的前50個負樣本ID\n",
        "  train_data_with_negatives = json.load(f)\n",
        "\n",
        "code_snippets_df = pd.read_csv('code_snippets.csv')\n",
        "code_id_to_code_map = pd.Series(code_snippets_df.code.values, index=code_snippets_df.code_id).to_dict()\n",
        "\n",
        "# --- 2. 初始化模型 ---\n",
        "print(\"\\n--- Initializing Model ---\")\n",
        "model_name = SENTENCE_TRANSFORMER_MODEL\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# --- 3. 微調模型 ---\n",
        "print(\"\\n--- Fine-tuning model with increased Learning Rate ---\")\n",
        "# 降低 batch_size 嘗試避免 OOM 錯誤\n",
        "fine_tuned_model_lr_increased = fine_tune_model(model, tokenizer, train_data_with_negatives, code_id_to_code_map, STRATEGY, epochs=3, lr=5e-5, batch_size=4)\n",
        "fine_tuned_model_lr_increased.save_pretrained(\"multi-qa-mpnet-base-dot-v1\")\n",
        "tokenizer.save_pretrained(\"multi-qa-mpnet-base-dot-v1\")\n",
        "\n",
        "print(\"\\n--- Model training complete. ---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initializing Model ---\n",
            "\n",
            "--- Fine-tuning model with increased Learning Rate ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 14293 has 14.73 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 187.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2407738156.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Fine-tuning model with increased Learning Rate ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 降低 batch_size 嘗試避免 OOM 錯誤\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mfine_tuned_model_lr_increased\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_with_negatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_id_to_code_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTRATEGY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mfine_tuned_model_lr_increased\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi-qa-mpnet-base-dot-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi-qa-mpnet-base-dot-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2016014666.py\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model, tokenizer, train_data_with_negatives, code_id_to_code_map, strategy, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_with_negatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_id_to_code_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;34m\"\"\"微調預訓練模型\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# 這是對訓練資料的準備，會產生每個樣本的anchor/positive/negative張量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4460\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4461\u001b[0m                 )\n\u001b[0;32m-> 4462\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 14293 has 14.73 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 187.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551,
          "referenced_widgets": [
            "6437ab17c7274509b42f26dae0b82358",
            "b5a32df3d03d4d978af48b7f4747becd",
            "fcefe56a6e584d1b8c7bd6c75a2e699d",
            "f70a0bb85ebd4d42ab72676af8ad1332",
            "a9519c9fb2b6411d86ff820013fced6b",
            "abfd5d3f595f4df4af16168bac99dc55",
            "b962389bf8724f4e85cae49d5346f0f7",
            "5b42b7359aef481d9990e5918ef2ce64",
            "f43b12b9366f49ee9be7f3e7df6752ee",
            "fc79b0b96df6421abb216fd55d92bf9c",
            "56d4c1809b57409191a97977fc34de12",
            "336e275cb3804879a96434ee6cf64df2",
            "5a1c5731fc4e45ac8ea12374ddbe8ee1",
            "8cea304ef52f4f0397bf36099ae9b76d",
            "f2c542f3933247e2935dcccac7cba96f",
            "b22109b877de4c7ab35f5d41e6a97f57",
            "3177c1da8fa74c4fb00228f125eb54ea",
            "bd528a2986a94f95b597000036c629be",
            "d6e64261da294d029046a3925d9ba1d5",
            "0eb633a3bd5d43d3bf58861a3adfa63a",
            "d0c586a53121454e9a060e48e974b19a",
            "433a838651ff48f89bba72ca2975432f",
            "e4127e7705c84c969aaa6753eb5d8d54",
            "b2995d3a227b4b5daf1835e73e25204b",
            "e7457e37d80c494eb932a227ea4ab753",
            "e077707b0b59414ab218264d43632c51",
            "d0c5d905a46a455ca3b6e24abc41f43d",
            "db5e1f33e0b74f588f079fd352e5343c",
            "3bcabc6b8bb84876978397f9b5458157",
            "ff459b3c95ed40e3acd91458b7dfa8a0",
            "d166e73e9e9241f1bf764d9152541327",
            "40078e067f3f434b973c3b46bd36b40d",
            "19b433c4067d46fb841fd23ac94d5f42",
            "13f547b840a5484d94994251d23f88c9",
            "d9b0d81b804b43539840cb3917eaee46",
            "968e4c2f55864794b006e7d8439eb63e",
            "bd7437f80ed04e7481e82e69c8726051",
            "6b767f3fc15b4cbe90d1a134e44fbe94",
            "d5236ec9ba224686ba271160765d51cb",
            "0f95ea3b17b0465aaa493c26a610b99c",
            "348dbe94896b4cf099d7c21e7d93d019",
            "c83f3219dd7d422cb6a3f7541b418c40",
            "7a2b2718db454818b88f33ff65b19312",
            "c6e95b56e192499f98200e84f6e751d8",
            "e8ec8d72d79545f086151c8de67376d6",
            "7dbdd8633e45459eabc73c2c7fef97c1",
            "ca2d4ec6f14c40b2b3b02c376c043a01",
            "7b5d87db87564561ab144b47203982c1",
            "20e8a43ce7d34576bfde5c6c5ebdca13",
            "6a4016b8ed5b4b5a8e1e36f58b23d31b",
            "563c72dc542a4f98bca24c8a3b4e8e76",
            "6026b0afa56c40d189a2cf7e21bf207b",
            "50b7f4051b694d06a4526423d89ee1a9",
            "d1caf471403f403baea8de47c4858c11",
            "7ebc1f36fc2d4389be654098a85c236f"
          ]
        },
        "id": "2e59dd3f",
        "outputId": "1f1d0287-859d-47a6-dc66-a9d8164ef284"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 選擇要測試的 Sentence-BERT 模型\n",
        "SENTENCE_TRANSFORMER_MODEL = 'sentence-transformers/multi-qa-mpnet-base-dot-v1'\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 載入模型\n",
        "print(f\"Loading Sentence-BERT model: {SENTENCE_TRANSFORMER_MODEL}\")\n",
        "model = SentenceTransformer(SENTENCE_TRANSFORMER_MODEL).to(DEVICE)\n",
        "model.eval() # 設定為評估模式\n",
        "\n",
        "# 載入資料並準備驗證集\n",
        "print(\"--- Loading Data and Preparing Validation Set ---\")\n",
        "train_queries_df = pd.read_csv('train_queries.csv')\n",
        "\n",
        "# 使用與微調腳本相同的分割方式\n",
        "def split_data(train_queries_df):\n",
        "    groups = train_queries_df['code']\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)\n",
        "    train_idx, val_idx = next(gss.split(train_queries_df, groups=groups))\n",
        "    train_df = train_queries_df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_df = train_queries_df.iloc[val_idx].reset_index(drop=True)\n",
        "    return train_df, val_df\n",
        "\n",
        "_, val_df = split_data(train_queries_df)\n",
        "print(f\"Loaded {len(val_df)} samples for validation.\")\n",
        "\n",
        "# 整個 train_queries_df 作為語料庫\n",
        "corpus_df = train_queries_df\n",
        "\n",
        "# 生成語料庫嵌入向量\n",
        "print(\"\\nCreating corpus embeddings...\")\n",
        "corpus_embeddings = model.encode(corpus_df['code'].tolist(), show_progress_bar=True, convert_to_tensor=True, device=DEVICE)\n",
        "\n",
        "# 評估 Recall@10\n",
        "recall_at_10 = 0\n",
        "print(\"\\nEvaluating Recall@10...\")\n",
        "for _, row in tqdm(val_df.iterrows(), total=val_df.shape[0], desc=\"Evaluating Recall@10\"):\n",
        "    query = row['query']\n",
        "    true_code_string = row['code']\n",
        "\n",
        "    # 生成查詢嵌入向量\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True, device=DEVICE)\n",
        "\n",
        "    # 計算餘弦相似度\n",
        "    scores = torch.nn.functional.cosine_similarity(query_embedding.unsqueeze(0), corpus_embeddings)\n",
        "    top_k_indices = torch.argsort(scores, descending=True)[:10]\n",
        "    top_k_codes = corpus_df.iloc[top_k_indices]['code'].values\n",
        "\n",
        "    if true_code_string in top_k_codes:\n",
        "        recall_at_10 += 1\n",
        "\n",
        "final_recall = recall_at_10 / len(val_df)\n",
        "print(f\"\\nPre-trained Sentence-BERT Model ({SENTENCE_TRANSFORMER_MODEL}) Recall@10: {final_recall:.4f}\")\n",
        "\n",
        "print(\"\\n--- Evaluation Complete ---\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Sentence-BERT model: sentence-transformers/multi-qa-mpnet-base-dot-v1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6437ab17c7274509b42f26dae0b82358"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "336e275cb3804879a96434ee6cf64df2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4127e7705c84c969aaa6753eb5d8d54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13f547b840a5484d94994251d23f88c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8ec8d72d79545f086151c8de67376d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 14293 has 14.73 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 187.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1436124566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 載入模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading Sentence-BERT model: {SENTENCE_TRANSFORMER_MODEL}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTENCE_TRANSFORMER_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 設定為評估模式\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hpu_graph_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 14293 has 14.73 GiB memory in use. Of the allocated memory 14.42 GiB is allocated by PyTorch, and 187.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95fb99d6"
      },
      "source": [
        "Sentence-BERT 模型通常不需要像其他 Transformer 模型那樣進行端到端的微調，因為它們已經在大量資料上進行過對比學習，能夠直接生成語義上有意義的嵌入向量。\n",
        "\n",
        "不過，如果需要針對您的特定資料集進一步優化效能，可以使用 `SentenceTransformers` 函式庫提供的微調方法，例如 `TripletLoss` 或 `MultipleNegativesRankingLoss`，搭配您的訓練資料（`train_queries.csv` 和 `train_data_with_negatives.json` 中的資訊）。\n",
        "\n",
        "您希望嘗試對 Sentence-BERT 模型進行微調嗎？或者先看看直接使用預訓練模型的 Recall@10 分數是否符合您的預期？"
      ]
    }
  ]
}