code_id,code
1,"def _initialize_bagit(self):  
        
        self.self_check()
        bagit = os.path.join(self.folder, ""bagit.txt"")
        
        
        with open(bagit, ""w"", encoding=ENCODING, newline=) as bag_it_file:
            
            bag_it_file.write(u""BagIt-Version: 0.97\n"")
            bag_it_file.write(u""Tag-File-Character-Encoding: %s\n"" % ENCODING)"
2,"def _add_common_constrain(token_lst: List[Dict], d: Dict) -> List[Dict]:
        

        result = []
        for a_token in token_lst:
            if not tf_transfer(d[""is_required""]):
                a_token[""OP""] = ""?""
            result.append(a_token)
        return result"
3,"def closest_cell_center(self,x,y):
        
        return self.matrixidx2sheet(*self.sheet2matrixidx(x,y))"
4,"def init_color_symbols(self):
        
        
        if self.is_prod:
            return
        keys = [
            , , , , ,
            , , , , ,
            , , , , ,
            , , , , ,
        ]

        _symbols = [ for _ in keys]
        
        txt = self.cfg.get(, )
        if txt:
            sym = txt.split()[0].split()
            _symbols = sym + _symbols[len(sym):]

        for i, k in enumerate(keys):
            if k in [, ]:
                self.symbols[k] = R.format(_symbols[i])
            else:
                self.symbols[k] = G.format(_symbols[i])"
5,"def sources(source_id=None, **kwargs):
    
    if source_id or  in kwargs:
        return source(source_id, **kwargs)
    return Fred().sources(**kwargs)"
6,"def retrieveVals(self):
        
        ntpinfo = NTPinfo()
        stats = ntpinfo.getHostOffset(self._remoteHost)
        if stats:
            graph_name =  % self._remoteHost
            if self.hasGraph(graph_name):
                self.setGraphVal(graph_name, , stats.get())
            graph_name =  % self._remoteHost
            if self.hasGraph(graph_name):
                self.setGraphVal(graph_name, , stats.get())
                self.setGraphVal(graph_name, , stats.get())"
7,"def connect_route53(aws_access_key_id=None, aws_secret_access_key=None, **kwargs):
    
    from boto.route53 import Route53Connection
    return Route53Connection(aws_access_key_id, aws_secret_access_key, **kwargs)"
8,"def _browse(cls, env, ids, from_record=None, iterated=None):
        
        records = cls()
        records._env_local = env
        records._ids = _normalize_ids(ids)
        if iterated:
            records._values = iterated._values
            records._values_to_write = iterated._values_to_write
        else:
            records._from_record = from_record
            records._values = {}
            records._values_to_write = {}
            for field in cls._columns:
                records._values[field] = {}
                records._values_to_write[field] = {}
            records._init_values()
        return records"
9,"def run(url, output, config):
    
    parsed_url = urlparse.urlparse(url)
    db_module_path = SUPPORTED_DATABASE_MODULES.get(parsed_url.scheme)
    if not db_module_path:
        raise ValueError(""Unsupported database scheme: "" % (parsed_url.scheme,))
    db_module = importlib.import_module(db_module_path)
    session.reset()
    for line in db_module.sanitize(url=parsed_url, config=config):
        output.write(line + ""\n"")"
10,"def kill_process(modeladmin, request, queryset):
    
    for process in queryset:
        process.stop(signum=signal.SIGKILL)"
11,"def get_lock(key, value=None, expiry_time=60):
    
    from uliweb.utils.common import get_uuid

    redis = get_redis()
    value = value or get_uuid()
    return redis.set(key, value, ex=expiry_time, nx=True)"
12,"def regional_maximum(image, mask = None, structure=None, ties_are_ok=False):
    
    global eight_connect
    if not ties_are_ok:
        
        
        
        result = regional_maximum(image, mask, structure, True)
        if not np.any(result):
            return result
        distance = scind.distance_transform_edt(result)
        
        
        
        
        
        labels, label_count = scind.label(result, eight_connect)
        np.random.seed(0)
        ro_distance = rank_order(distance)[0].astype(float)
        count = np.product(ro_distance.shape)
        ro_distance.flat += (np.random.permutation(count).astype(float) / 
                             float(count))
        positions = scind.maximum_position(ro_distance, labels,
                                           np.arange(label_count)+1)
        positions = np.array(positions, np.uint32)
        result = np.zeros(image.shape, bool)
        if positions.ndim == 1:
            result[positions[0],positions[1]] = True
        else:
            result[positions[:,0],positions[:,1]] = True
        return result
    result = np.ones(image.shape,bool)
    if structure is None:
        structure = scind.generate_binary_structure(image.ndim, image.ndim)
    
    
    
    
    big_mask = np.zeros(np.array(image.shape) + np.array(structure.shape), bool)
    structure_half_shape = np.array(structure.shape)//2
    big_mask[structure_half_shape[0]:structure_half_shape[0]+image.shape[0],
             structure_half_shape[1]:structure_half_shape[1]+image.shape[1]]=\
        mask if mask is not None else True
    for i in range(structure.shape[0]):
        off_i = i-structure_half_shape[0]
        for j in range(structure.shape[1]):
            if i == structure_half_shape[0] and j == structure_half_shape[1]:
                continue
            off_j = j-structure_half_shape[1]
            if structure[i,j]:
                result = np.logical_and(result, big_mask[i:i+image.shape[0],
                                                         j:j+image.shape[1]])
                
                
                
                
                src_i_min = max(0,-off_i)
                src_i_max = min(image.shape[0], image.shape[0]-off_i)
                off_i_min = max(0,off_i)
                off_i_max = min(image.shape[0], image.shape[0]+off_i)
                src_j_min = max(0,-off_j)
                src_j_max = min(image.shape[1], image.shape[1]-off_j)
                off_j_min = max(0,off_j)
                off_j_max = min(image.shape[1], image.shape[1]+off_j)
                min_mask = (image[src_i_min:src_i_max,
                                  src_j_min:src_j_max] <
                            image[off_i_min:off_i_max,
                                  off_j_min:off_j_max])
                result[src_i_min:src_i_max,
                       src_j_min:src_j_max][min_mask] = False
    return result"
13,"def job_skipped(self, job, queue):
        
        job.queued.delete()
        self.log(self.job_skipped_message(job, queue), level=)
        if hasattr(job, ):
            job.on_skipped(queue)"
14,"def p_items(self, p):
        
        p[0] = p[1] + (p[2],)
        p.set_lineno(0, p.lineno(1))"
15,"def get_style(self, style):
        
        style_key = tuple(style.items())
        s = self.STYLE_FACTORY.get(style_key, None)
        if s is None:
            s = xlwt.XFStyle()
            for key, values in style.items():
                if key == ""background"":
                    p = xlwt.Pattern()
                    for attr, value in values:
                        p.__setattr__(attr, value)
                    s.pattern = p
                elif key == ""format"":
                    s.num_format_str = values
                elif key == ""alignment"":
                    a = xlwt.Alignment()
                    for attr, value in values:
                        a.__setattr__(attr, value)
                    s.alignment = a
                elif key == ""border"":
                    b = xlwt.Formatting.Borders()
                    for attr, value in values:
                        b.__setattr__(attr, value)
                    s.borders = b
                elif key == ""font"":
                    f = self.get_font(values)
                    s.font = f
            self.STYLE_FACTORY[style_key] = s
        return s"
16,"def get_private_rooms(self, **kwargs):
        
        return GetPrivateRooms(settings=self.settings, **kwargs).call(**kwargs)"
17,"def time_snowflake(datetime_obj, high=False):
    
    unix_seconds = (datetime_obj - type(datetime_obj)(1970, 1, 1)).total_seconds()
    discord_millis = int(unix_seconds * 1000 - DISCORD_EPOCH)

    return (discord_millis << 22) + (2**22-1 if high else 0)"
18,"def ancestors(self, node, relations=None, reflexive=False):
        
        if reflexive:
            ancs = self.ancestors(node, relations, reflexive=False)
            ancs.append(node)
            return ancs

        g = None
        if relations is None:
            g = self.get_graph()
        else:
            g = self.get_filtered_graph(relations)
        if node in g:
            return list(nx.ancestors(g, node))
        else:
            return []"
19,"def p_review_date_1(self, p):
        
        try:
            if six.PY2:
                value = p[2].decode(encoding=)
            else:
                value = p[2]
            self.builder.add_review_date(self.document, value)
        except CardinalityError:
            self.more_than_one_error(, p.lineno(1))
        except OrderError:
            self.order_error(, , p.lineno(1))"
20,"def decode_ast(registry, ast_json):
    
    if ast_json.get(""@type""):
        subclass = registry.get_cls(ast_json[""@type""], tuple(ast_json[""@fields""]))
        return subclass(
            ast_json[""children""],
            ast_json[""field_references""],
            ast_json[""label_references""],
            position=ast_json[""@position""],
        )
    else:
        return ast_json"
21,"def make_abstract_dist(req):
    
    
    if req.editable:
        return IsSDist(req)
    elif req.link and req.link.is_wheel:
        return IsWheel(req)
    else:
        return IsSDist(req)"
22,"def load(self):
		
		glTexImage3D(GL_TEXTURE_3D, 0, GL_LUMINANCE16_ALPHA16, 
			self.width, self.width, self.width, 0, GL_LUMINANCE_ALPHA, 
			GL_UNSIGNED_SHORT, ctypes.byref(self.data))"
23,"def geostatistical_prior_builder(pst, struct_dict,sigma_range=4,
                                 par_knowledge_dict=None,verbose=False):
    

    if isinstance(pst,str):
        pst = pyemu.Pst(pst)
    assert isinstance(pst,pyemu.Pst),""pst arg must be a Pst instance, not {0}"".\
        format(type(pst))
    if verbose: print(""building diagonal cov"")
    full_cov = pyemu.Cov.from_parameter_data(pst,sigma_range=sigma_range)

    full_cov_dict = {n:float(v) for n,v in zip(full_cov.col_names,full_cov.x)}
    
    par = pst.parameter_data
    for gs,items in struct_dict.items():
        if verbose: print(""processing "",gs)
        if isinstance(gs,str):
            gss = pyemu.geostats.read_struct_file(gs)
            if isinstance(gss,list):
                warnings.warn(""using first geostat structure in file {0}"".\
                              format(gs),PyemuWarning)
                gs = gss[0]
            else:
                gs = gss
        if not isinstance(items,list):
            items = [items]
        for item in items:
            if isinstance(item,str):
                assert os.path.exists(item),""file {0} not found"".\
                    format(item)
                if item.lower().endswith("".tpl""):
                    df = pyemu.pp_utils.pp_tpl_to_dataframe(item)
                elif item.lower.endswith("".csv""):
                    df = pd.read_csv(item)
            else:
                df = item
            for req in [,,]:
                if req not in df.columns:
                    raise Exception(""{0} is not in the columns"".format(req))
            missing = df.loc[df.parnme.apply(
                    lambda x : x not in par.parnme),""parnme""]
            if len(missing) > 0:
                warnings.warn(""the following parameters are not "" + \
                              ""in the control file: {0}"".\
                              format(.join(missing)),PyemuWarning)
                df = df.loc[df.parnme.apply(lambda x: x not in missing)]
            if ""zone"" not in df.columns:
                df.loc[:,""zone""] = 1
            zones = df.zone.unique()
            aset = set(pst.adj_par_names)
            for zone in zones:
                df_zone = df.loc[df.zone==zone,:].copy()
                df_zone = df_zone.loc[df_zone.parnme.apply(lambda x: x in aset), :]
                if df_zone.shape[0] == 0:
                    warnings.warn(""all parameters in zone {0} tied and/or fixed, skipping..."".format(zone),
                                  PyemuWarning)
                    continue
                
                df_zone.sort_index(inplace=True)
                if verbose: print(""build cov matrix"")
                cov = gs.covariance_matrix(df_zone.x,df_zone.y,df_zone.parnme)
                if verbose: print(""done"")
                
                if verbose: print(""getting diag var cov"",df_zone.shape[0])
                
                tpl_var = max([full_cov_dict[pn] for pn in df_zone.parnme])
                
                
                
                
                if verbose: print(""scaling full cov by diag var cov"")
                cov *= tpl_var
                if verbose: print(""test for inversion"")
                try:
                    ci = cov.inv
                except:
                    df_zone.to_csv(""prior_builder_crash.csv"")
                    raise Exception(""error inverting cov {0}"".
                                    format(cov.row_names[:3]))

                    if verbose: print()
                full_cov.replace(cov)
                
                
                
                

    if par_knowledge_dict is not None:
        full_cov = condition_on_par_knowledge(full_cov,
                    par_knowledge_dict=par_knowledge_dict)
    return full_cov"
24,"def watching(w, watch, max_count=None, clear=True):
    

    if w and not watch:
        watch = 2
    if watch and clear:
        click.clear()
    yield 0

    if max_count is not None and max_count < 1:
        return

    counter = 1
    while watch and counter <= (max_count or counter):
        time.sleep(watch)
        counter += 1
        if clear:
            click.clear()
        yield 0"
25,"def checkForDeadlocks(self):
        
        totalRunningJobs = len(self.batchSystem.getRunningBatchJobIDs())
        totalServicesIssued = self.serviceJobsIssued + self.preemptableServiceJobsIssued
        
        if totalServicesIssued >= totalRunningJobs and totalRunningJobs > 0:
            serviceJobs = [x for x in list(self.jobBatchSystemIDToIssuedJob.keys()) if isinstance(self.jobBatchSystemIDToIssuedJob[x], ServiceJobNode)]
            runningServiceJobs = set([x for x in serviceJobs if self.serviceManager.isRunning(self.jobBatchSystemIDToIssuedJob[x])])
            assert len(runningServiceJobs) <= totalRunningJobs

            
            if len(runningServiceJobs) == totalRunningJobs:
                
                if self.potentialDeadlockedJobs != runningServiceJobs:
                    self.potentialDeadlockedJobs = runningServiceJobs
                    self.potentialDeadlockTime = time.time()
                elif time.time() - self.potentialDeadlockTime >= self.config.deadlockWait:
                    raise DeadlockException(""The system is service deadlocked - all %d running jobs are active services"" % totalRunningJobs)
            else:
                
                self.potentialDeadlockedJobs = set()
                self.potentialDeadlockTime = 0
        else:
            
            self.potentialDeadlockedJobs = set()
            self.potentialDeadlockTime = 0"
26,"def set_log_level(level):
    
    from .._connect import main as _glconnect
    unity = _glconnect.get_unity()
    return unity.set_log_level(level)"
27,"def get_source_sum(file_name=,
                   source=,
                   source_hash=None,
                   source_hash_name=None,
                   saltenv=):
    ***
    def _invalid_source_hash_format():
        
        raise CommandExecutionError(
            
            
            
            
            
            .format(
                source_hash,
                .join(salt.utils.files.VALID_PROTOS),
                .join(
                    [.format(HASHES_REVMAP[x], x)
                     for x in sorted(HASHES_REVMAP)]
                ),
            )
        )

    hash_fn = None
    if os.path.isabs(source_hash):
        hash_fn = source_hash
    else:
        try:
            proto = _urlparse(source_hash).scheme
            if proto in salt.utils.files.VALID_PROTOS:
                hash_fn = __salt__[](source_hash, saltenv)
                if not hash_fn:
                    raise CommandExecutionError(
                        .format(source_hash)
                    )
            else:
                if proto != :
                    
                    
        return ret"
28,"def setup_filter(self, root_path, path_list):
        
        self.root_path = osp.normpath(to_text_string(root_path))
        self.path_list = [osp.normpath(to_text_string(p)) for p in path_list]
        self.invalidateFilter()"
29,"def iter_definitions(definitions, operations):
    
    
    for error_schema_class in [ErrorSchema, ErrorContextSchema, SubErrorSchema]:
        yield error_schema_class()

    
    for operation, obj, rule, func in operations:
        yield get_request_schema(func)
        yield get_response_schema(func)"
30,"def lose(spin):
    

    try:
        spin.close()
    except Exception as excpt:
        err = excpt.args[0]
        spin.drive(CLOSE_ERR, err)
    finally:
        spin.destroy()
        spin.drive(LOST)"
31,"def make_file_system_tree(root_folder, _parent=None):
    
    root_node = Node(os.path.basename(root_folder), _parent)
    root_node.path = root_folder
    for item in os.listdir(root_folder):
        item_path = os.path.join(root_folder, item)
        if os.path.isfile(item_path):
            file_node = Node(os.path.basename(item), root_node)
            file_node.path = item_path
        elif os.path.isdir(item_path):
            
            make_file_system_tree(item_path, _parent=root_node)
    return root_node"
32,"def print_fatal_results(results, level=0):
    
    print_level(logger.critical, _RED + ""[X] Fatal Error: %s"", level, results.error)"
33,"def _from_keras_log_format(data, **kwargs):
    
    data_val = pd.DataFrame(data[[]])

    data_val[] = data[]
    data_val[] = data[]
    data_val[] = 

    data_training = pd.DataFrame(data[[, , ]])
    data_training[] = 

    result = pd.concat([data_training, data_val], sort=False)
    plot(result, **kwargs)"
34,"def frontiersZipInput(zip_path, output_prefix, download=None):
    
    log.debug()
    
    pathname, pathext = os.path.splitext(zip_path)
    path, name = os.path.split(pathname)
    if not pathext == :  
        log.error()
        print()
        sys.exit(1)
    
    file_root = name.split()[0]
    zipname1 = ""{0}-r{1}.zip"".format(file_root, )
    zipname2 = ""{0}-r{1}.zip"".format(file_root, )
    
    output = os.path.join(output_prefix, file_root)
    if os.path.isdir(output):
        shutil.rmtree(output)  
    output_meta = os.path.join(output, )
    images_output = os.path.join(output, , )
    with zipfile.ZipFile(os.path.join(path, zipname1), ) as xml_zip:
        zip_dir = .format(file_root)
        xml = .join([zip_dir, .format(file_root)])
        try:
            xml_zip.extract(xml)
        except KeyError:
            log.critical(.format(xml))
            sys.exit(.format(xml))
        else:
            if not os.path.isdir(output_meta):
                os.makedirs(output_meta)
            shutil.copy(xml, os.path.join(output_meta))
            os.remove(xml)
            os.rmdir(zip_dir)
    with zipfile.ZipFile(os.path.join(path, zipname2), ) as image_zip:
        zip_dir = .format(file_root)
        for i in image_zip.namelist():
            if  in i:
                image_zip.extract(i)
        if not os.path.isdir(images_output):
            os.makedirs(images_output)
        unzipped_images = os.path.join(zip_dir, , )
        for i in os.listdir(unzipped_images):
            shutil.copy(os.path.join(unzipped_images, i), images_output)
        shutil.rmtree(zip_dir)
    return file_root"
35,"def all_agents(stmts):
    
    agents = []
    for stmt in stmts:
        for agent in stmt.agent_list():
            
                agents.append(agent)
    return agents"
36,"def checkInstalledPip(package, speak=True, speakSimilar=True):
    
    packages = sorted([i.key for i in pip.get_installed_distributions()])
    installed = package in packages
    similar = None

    if not installed:
        similar = [pkg for pkg in packages if package in pkg]

    if speak:
        speakInstalledPackages(package, ""pip"", installed, similar, speakSimilar)

    return (installed, similar)"
37,"def request_validation_mail(self):
        
        message = Msg(EMsg.ClientRequestValidationMail, extended=True)

        resp = self.send_job_and_wait(message, timeout=10)

        if resp is None:
            return EResult.Timeout
        else:
            return EResult(resp.eresult)"
38,"def open(self, url, mode=, reload=False, filename=None):
        
        if  not in url:
            path_that_might_be_relative = url
            path = os.path.join(self.directory, path_that_might_be_relative)
            return open(path, mode)
        if filename is None:
            filename = urlparse(url).path.split()[-1]
        path = self.path_to(filename)
        if reload and os.path.exists(path):
            os.remove(path)
        if not os.path.exists(path):
            download(url, path, self.verbose)
        return open(path, mode)"
39,"def create_edge(self, from_doc, to_doc, edge_data={}):
        

        return Edge.create(
            collection=self,
            from_doc=from_doc,
            to_doc=to_doc,
            edge_data=edge_data
        )"
40,"def str_to_int_array(string, base=16):
    

    int_strings = string.split()
    return [int(int_str, base) for int_str in int_strings]"
41,"def cast_to_a1_notation(method):
    
    @wraps(method)
    def wrapper(self, *args, **kwargs):
        try:
            if len(args):
                int(args[0])

            
            range_start = rowcol_to_a1(*args[:2])
            range_end = rowcol_to_a1(*args[-2:])
            range_name = .join((range_start, range_end))

            args = (range_name,) + args[4:]
        except ValueError:
            pass

        return method(self, *args, **kwargs)

    return wrapper"
42,"def log_to(logger):
    
    logger_id = id(logger)

    def decorator(function):
        func = add_label(function, , logger_id=logger_id)
        return func
    return decorator"
43,"def flexifunction_directory_encode(self, target_system, target_component, directory_type, start_index, count, directory_data):
                
                return MAVLink_flexifunction_directory_message(target_system, target_component, directory_type, start_index, count, directory_data)"
44,"def preprocessing_excel(path):
    
    if not os.path.exists(path):
        raise ValueError(""Error: %s file not found"" % path)

    
    df = pd.read_excel(path, sheetname=0, header=0)

    
    

    

    
    df.iloc[:, 0] = pd.Series(df.iloc[:, 0]).fillna(method=)

    
    

    df = df[df.ix[:, 1].notnull()]
    df = df.reset_index(drop=True)

    
    df.ix[:, 2].fillna(0, inplace=True)

    
    if (df.ix[:, 1].isnull().sum()) != 0:
        raise ValueError(""Error: Empty cells in the gene column"")

    
    

    return df"
45,"def get_form_kwargs(self, key):
        
        kwargs = super(MultipleModelFormMixin, self).get_form_kwargs(key)
        if hasattr(self, .format(key)):
            kwargs.update({: getattr(self, .format(key))})
        return kwargs"
46,"def to_spmatrix(self):
        r
        mat = sparse.coo_matrix(1)
        for z, x in zip(self._z, self._x):
            if not z and not x:  
                mat = sparse.bmat([[mat, None], [None, mat]], format=)
            elif z and not x:  
                mat = sparse.bmat([[mat, None], [None, -mat]], format=)
            elif not z and x:  
                mat = sparse.bmat([[None, mat], [mat, None]], format=)
            else:  
                mat = mat * 1j
                mat = sparse.bmat([[None, -mat], [mat, None]], format=)

        return mat.tocsr()"
47,"def set_style(self, style):
        
        if style == ""none"":
            self.style = None
        elif prompt_toolkit is None:
            raise CoconutException(""syntax highlighting is not supported on this Python version"")
        elif style == ""list"":
            print(""Coconut Styles: none, "" + "", "".join(pygments.styles.get_all_styles()))
            sys.exit(0)
        elif style in pygments.styles.get_all_styles():
            self.style = style
        else:
            raise CoconutException(""unrecognized pygments style"", style, extra=""use  to show all valid styles"")"
48,"def decodekey(self, key):
        
        if key[:1] in (b, b, b):
            if key[1:2] == b""\x00"" and len(key) == 2 + self.wordsize:
                return struct.unpack("">sB"" + self.fmt, key)
            else:
                return key[:1], key[1:].decode(, )
        if key[:1] == b:
            return struct.unpack("">s"" + self.fmt, key)
        if len(key) == 1 + self.wordsize:
            return struct.unpack(self.keyfmt[:3], key)
        if len(key) == 1 + self.wordsize + 1:
            return struct.unpack(self.keyfmt[:4], key)
        if len(key) == 1 + 2 * self.wordsize + 1:
            return struct.unpack(self.keyfmt[:5], key)
        if len(key) > 1 + self.wordsize + 1:
            f = struct.unpack_from(self.keyfmt[:4], key)
            return f + (key[2 + self.wordsize:], )
        raise Exception(""unknown key format"")"
49,"def _flattenComponent(glyphSet, component):
    

    glyph = glyphSet[component.baseGlyph]
    if not glyph.components:
        transformation = Transform(*component.transformation)
        return [(component.baseGlyph, transformation)]

    all_flattened_components = []
    for nested in glyph.components:
        flattened_components = _flattenComponent(glyphSet, nested)
        for i, (_, tr) in enumerate(flattened_components):
            tr = tr.transform(component.transformation)
            flattened_components[i] = (flattened_components[i][0], tr)
        all_flattened_components.extend(flattened_components)
    return all_flattened_components"
50,"async def packet_receiver(queue):
    
    LOG.info(""Entering packet_receiver"")
    while True:
        packet = await queue.get()
        if packet is None:
            break

        LOG.info(""Framenumber %s"", packet.framenumber)
    LOG.info(""Exiting packet_receiver"")"
51,"def check_crs(self, dataset):
        
        grid_mapping = util.get_crs_variable(dataset)
        if grid_mapping is None:
            return Result(
                BaseCheck.MEDIUM,
                False,
                ,
                []
            )
        crs_variable = dataset.variables[grid_mapping]
        test_ctx = TestCtx(BaseCheck.MEDIUM, .format(crs_variable.name))
        test_ctx.assert_true(crs_variable is not None, )

        epsg_code = getattr(crs_variable, , )
        semi_major_axis = getattr(crs_variable, , None)
        inverse_flattening = getattr(crs_variable, , None)

        test_ctx.assert_true(epsg_code != ,
                             .format(epsg_code))
        test_ctx.assert_true(semi_major_axis is not None,
                             .format(epsg_code))
        test_ctx.assert_true(inverse_flattening is not None,
                             .format(epsg_code))
        return test_ctx.to_result()"
52,"def login(self, username=None, password=None):
        
        if username is not None:
            self._cache[CONST.ID] = username
        if password is not None:
            self._cache[CONST.PASSWORD] = password

        if (self._cache[CONST.ID] is None or
                not isinstance(self._cache[CONST.ID], str)):
            raise AbodeAuthenticationException(ERROR.USERNAME)

        if (self._cache[CONST.PASSWORD] is None or
                not isinstance(self._cache[CONST.PASSWORD], str)):
            raise AbodeAuthenticationException(ERROR.PASSWORD)

        self._save_cache()

        self._token = None

        login_data = {
            CONST.ID: self._cache[CONST.ID],
            CONST.PASSWORD: self._cache[CONST.PASSWORD],
            CONST.UUID: self._cache[CONST.UUID]
        }

        response = self._session.post(CONST.LOGIN_URL, json=login_data)
        response_object = json.loads(response.text)

        oauth_token = self._session.get(CONST.OAUTH_TOKEN_URL)
        oauth_token_object = json.loads(oauth_token.text)

        if response.status_code != 200:
            raise AbodeAuthenticationException((response.status_code,
                                                response_object[]))

        _LOGGER.debug(""Login Response: %s"", response.text)

        self._token = response_object[]
        self._panel = response_object[]
        self._user = response_object[]
        self._oauth_token = oauth_token_object[]

        _LOGGER.info(""Login successful"")

        return True"
53,"def _BYTES_TO_BITS():
    
    the_table = 256*[None]
    bits_per_byte = list(range(7, -1, -1))
    for n in range(256):
        l = n
        bits = 8*[None]
        for i in bits_per_byte:
            bits[i] = [n & 1]
            n >>= 1
        the_table[l] = .join(bits)
    return the_table"
54,"def vel_in_A_to_vel_in_B(vel_A, ang_vel_A, pose_A_in_B):
    
    pos_A_in_B = pose_A_in_B[:3, 3]
    rot_A_in_B = pose_A_in_B[:3, :3]
    skew_symm = _skew_symmetric_translation(pos_A_in_B)
    vel_B = rot_A_in_B.dot(vel_A) + skew_symm.dot(rot_A_in_B.dot(ang_vel_A))
    ang_vel_B = rot_A_in_B.dot(ang_vel_A)
    return vel_B, ang_vel_B"
55,"def create_api_v4_neighbor(self):
        
        return ApiV4Neighbor(
            self.networkapi_url,
            self.user,
            self.password,
            self.user_ldap)"
56,"def percent(self, value: float) -> :
        
        raise_not_number(value)
        self.maximum = .format(value)
        return self"
57,"def ru_strftime(date, format=""%d.%m.%Y"", inflected_day=False, preposition=False):
    
    try:
        res = dt.ru_strftime(format,
                             date,
                             inflected=True,
                             inflected_day=inflected_day,
                             preposition=preposition)
    except Exception as err:
        
        try:
            default_date = date.strftime(format)
        except Exception:
            default_date = str(date)
        res = default_value % {: err, : default_date}
    return res"
58,"def in6_getLocalUniquePrefix():
    
    
    
    
    

    
    
    
    
    

    tod = time.time() 
    i = int(tod)
    j = int((tod - i)*(2**32))
    tod = struct.pack(""!II"", i,j)
    
    rawmac = get_if_raw_hwaddr(conf.iface6)
    mac = b"":"".join(map(lambda x: b""%.02x"" % ord(x), list(rawmac)))
    
    eui64 = inet_pton(socket.AF_INET6,  + in6_mactoifaceid(mac))[8:] 
    import sha
    globalid = sha.new(tod+eui64).digest()[:5]
    return inet_ntop(socket.AF_INET6, b + globalid + b*10)"
59,"def sync(self):
        
        for key_hash, obj in six.iteritems(self._cache):
            
            
            if not obj.has_expired():
                file_path = self._path_for_hash(key_hash)
                with open(str(file_path), self._write_mode) as f:
                    obj.dump(f)"
60,"def _check_for_encoding(b):
    
    eol = b.find(b)
    if eol < 0:
        return _check_line_for_encoding(b)[0]
    enc, again = _check_line_for_encoding(b[:eol])
    if enc or not again:
        return enc
    eol2 = b.find(b, eol + 1)
    if eol2 < 0:
        return _check_line_for_encoding(b[eol + 1:])[0]
    return _check_line_for_encoding(b[eol + 1:eol2])[0]"
61,"def get_targets(ipyclient):
    
    
    hosts = []
    for eid in ipyclient.ids:
        engine = ipyclient[eid]
        if not engine.outstanding:
            hosts.append(engine.apply(socket.gethostname))

    
    hosts = [i.get() for i in hosts]
    hostset = set(hosts)
    hostzip = zip(hosts, ipyclient.ids)
    hostdict = {host: [i[1] for i in hostzip if i[0] == host] for host in hostset}
    targets = list(itertools.chain(*[hostdict[i][:2] for i in hostdict]))

    
    return targets"
62,"def jira_parens_role(name, rawtext, text, lineno, inliner,
                     options=None, content=None):
    
    return jira_bracket_role(name, rawtext, text, lineno, inliner,
                             options=None, content=None,
                             open_symbol=, close_symbol=)"
63,"def get_relationships_for_destination(self, destination_id=None):
        
        if destination_id is None:
            raise NullArgument()
        url_path = ( +
                    self._catalog_idstr +  +
                    str(destination_id))
        return objects.RelationshipList(self._get_request(url_path))"
64,"def consume(self, istream, ostream, batch=False):
        
        datapoints = []  

        if batch:
            sleep = max(0.01, self.option.sleep)
            fd = istream.fileno()
            while True:
                try:
                    if select.select([fd], [], [], sleep):
                        try:
                            line = istream.readline()
                            if line == :
                                break
                            datapoints.append(self.consume_line(line))
                        except ValueError:
                            continue

                        if self.option.sort_by_column:
                            datapoints = sorted(datapoints, key=itemgetter(self.option.sort_by_column - 1))

                        if len(datapoints) > 1:
                            datapoints = datapoints[-self.maximum_points:]
                            self.update([dp[0] for dp in datapoints], [dp[1] for dp in datapoints])
                            self.render(ostream)

                        time.sleep(sleep)

                except KeyboardInterrupt:
                    break

        else:
            for line in istream:
                try:
                    datapoints.append(self.consume_line(line))
                except ValueError:
                    pass

            if self.option.sort_by_column:
                datapoints = sorted(datapoints, key=itemgetter(self.option.sort_by_column - 1))

            self.update([dp[0] for dp in datapoints], [dp[1] for dp in datapoints])
            self.render(ostream)"
65,"def pop(self, settings):
        
        if settings:
            for name in self.__slots__:
                self._set(name, settings.pop(name, UNSET))"
66,"def getWorker(self, name):
		
		if not name in self.worker_list:
			self.logger.error(""Worker {0} is not registered!"".format(name))
			raise Exception(""Worker {0} is not registered!"".format(name))

		return self.worker_list[name]"
67,"def as_artist(self, origin=(0, 0), **kwargs):
        
        from matplotlib.patches import Polygon
        xy = np.vstack([self.vertices.x - origin[0],
                        self.vertices.y - origin[1]]).transpose()

        mpl_params = self.mpl_properties_default()
        mpl_params.update(kwargs)

        return Polygon(xy=xy, **mpl_params)"
68,"def synchronize(self):
        
        gdocs_trans_csv = os.path.join(self.temp_path, GDOCS_TRANS_CSV)
        gdocs_meta_csv = os.path.join(self.temp_path, GDOCS_META_CSV)
        local_trans_csv = os.path.join(self.temp_path, LOCAL_TRANS_CSV)
        local_meta_csv = os.path.join(self.temp_path, LOCAL_META_CSV)

        try:
            entry = self._download_csv_from_gdocs(gdocs_trans_csv,
                                                  gdocs_meta_csv)
        except PODocsError as e:
            if  in str(e) \
                    or  in str(e):
                self.upload()
            else:
                raise PODocsError(e)
        else:
            self._merge_local_and_gdoc(entry, local_trans_csv, local_meta_csv,
                                       gdocs_trans_csv, gdocs_meta_csv)

            try:
                csv_to_po(local_trans_csv, local_meta_csv,
                          self.locale_root, self.po_files_path, self.header)
            except IOError as e:
                raise PODocsError(e)

        self._clear_temp()"
69,"def remove(mod, persist=False, comment=True):
    t remove line from /boot/loader.conf but only
        comment it

    CLI Example:

    .. code-block:: bash

        salt  kmod.remove vmm
    cmd.run_allkldunload {0}retcodeError removing module {0}: {1}stderr'])"
70,"def make_payload(base, method, params):
    
    payload = {
        : ,
        : .format(**locals()),
        : utils.serialize_dict(params),
        : 1,
    }
    return payload"
71,"def recruit(self, n=1):
        
        logger.info(""Recruiting {} CLI participants"".format(n))
        urls = []
        template = ""{}/ad?recruiter={}&assignmentId={}&hitId={}&workerId={}&mode={}""
        for i in range(n):
            ad_url = template.format(
                get_base_url(),
                self.nickname,
                generate_random_id(),
                generate_random_id(),
                generate_random_id(),
                self._get_mode(),
            )
            logger.info(""{} {}"".format(NEW_RECRUIT_LOG_PREFIX, ad_url))
            urls.append(ad_url)

        return urls"
72,"def libvlc_media_player_set_agl(p_mi, drawable):
    
    f = _Cfunctions.get(, None) or \
        _Cfunction(, ((1,), (1,),), None,
                    None, MediaPlayer, ctypes.c_uint32)
    return f(p_mi, drawable)"
73,"def _init_solc_binary(version):
        

        if not version:
            return os.environ.get(""SOLC"") or ""solc""

        
        main_version = solc.main.get_solc_version_string()
        main_version_number = re.match(r""\d+.\d+.\d+"", main_version)
        if main_version is None:
            raise CriticalError(
                ""Could not extract solc version from string {}"".format(main_version)
            )
        if version == main_version_number:
            log.info(""Given version matches installed version"")
            solc_binary = os.environ.get(""SOLC"") or ""solc""
        else:
            solc_binary = util.solc_exists(version)
            if solc_binary:
                log.info(""Given version is already installed"")
            else:
                try:
                    solc.install_solc(""v"" + version)
                    solc_binary = util.solc_exists(version)
                    if not solc_binary:
                        raise SolcError()
                except SolcError:
                    raise CriticalError(
                        ""There was an error when trying to install the specified solc version""
                    )

            log.info(""Setting the compiler to %s"", solc_binary)

        return solc_binary"
74,"def update_hmet_card_file(hmet_card_file_path, new_hmet_data_path):
    
    hmet_card_file_path_temp = ""{0}_tmp"".format(hmet_card_file_path)
    try:
        remove(hmet_card_file_path_temp)
    except OSError:
        pass

    copy(hmet_card_file_path, hmet_card_file_path_temp)

    with io_open(hmet_card_file_path_temp, , newline=) as out_hmet_list_file:
        with open(hmet_card_file_path) as old_hmet_list_file:
            for date_path in old_hmet_list_file:
                out_hmet_list_file.write(u""{0}\n"".format(path.join(new_hmet_data_path,
                                                         path.basename(date_path))))
    try:
        remove(hmet_card_file_path)
    except OSError:
        pass

    rename(hmet_card_file_path_temp, hmet_card_file_path)"
75,"def earth_gyro(RAW_IMU,ATTITUDE):
    
    r = rotation(ATTITUDE)
    accel = Vector3(degrees(RAW_IMU.xgyro), degrees(RAW_IMU.ygyro), degrees(RAW_IMU.zgyro)) * 0.001
    return r * accel"
76,"def ls(obj=None):
    
    if obj is None:
        
        import builtins
        all = builtins.__dict__.copy()
        all.update(globals())
        objlst = sorted(conf.layers, key=lambda x:x.__name__)
        for o in objlst:
            print(""%-10s : %s"" %(o.__name__,o.name))
    else:
        if isinstance(obj, type) and issubclass(obj, Packet):
            for f in obj.fields_desc:
                print(""%-10s : %-20s = (%s)"" % (f.name, f.__class__.__name__,  repr(f.default)))
        elif isinstance(obj, Packet):
            for f in obj.fields_desc:
                print(""%-10s : %-20s = %-15s (%s)"" % (f.name, f.__class__.__name__, repr(getattr(obj,f.name)), repr(f.default)))
            if not isinstance(obj.payload, NoPayload):
                print(""--"")
                ls(obj.payload)
                

        else:
            print(""Not a packet class. Type  to list packet classes."")"
77,"def powerset(iterable):
    
    ""list(powerset([1,2,3])) --> [(), (1,), (2,), (3,), (1,2), (1,3), (2,3), (1,2,3)]""
    s = list(iterable)
    return chain.from_iterable(combinations(s, r) for r in range(len(s) + 1))"
78,"def exportTreeItem(self, sheet, cols, item):
        
        
        for c, col in enumerate(cols):
            data = unwrapVariant(item.data(Qt.EditRole, col))
            if data:
                sheet.write(self._currrow, c, nativestring(data))
            else:
                sheet.write(self._currrow, c, nativestring(item.text(col)))
        
        self._currrow += 1
        
        
        for c in range(item.childCount()):
            self.exportTreeItem(sheet, cols, item.child(c))"
79,"def insert(self, index, object):
        
        self._check(object.id)
        list.insert(self, index, object)
        
        _dict = self._dict
        for i, j in iteritems(_dict):
            if j >= index:
                _dict[i] = j + 1
        _dict[object.id] = index"
80,"def bbox(self):
        
        xmin = min(self.start.real, self.end.real)
        xmax = max(self.start.real, self.end.real)
        ymin = min(self.start.imag, self.end.imag)
        ymax = max(self.start.imag, self.end.imag)
        return xmin, xmax, ymin, ymax"
81,"def normalized_scalar_param_list(param, length, param_conv=None,
                                 keep_none=True, return_nonconv=False):
    
    length, length_in = int(length), length
    if length < 0:
        raise ValueError(
                         .format(length_in))

    param = np.array(param, dtype=object, copy=True, ndmin=1)
    nonconv_list = list(np.broadcast_to(param, (length,)))

    if len(nonconv_list) != length:
        raise ValueError(
                         .format(len(nonconv_list), length))

    if param_conv is None:
        out_list = list(nonconv_list)
    else:
        out_list = []
        for p in nonconv_list:
            if p is None and keep_none:
                out_list.append(p)
            else:
                out_list.append(param_conv(p))

    if return_nonconv:
        return out_list, nonconv_list
    else:
        return out_list"
82,"def get(cls, channel, start, end, bits=None, **kwargs):
        
        new = cls.DictClass.get([channel], start, end, **kwargs)[channel]
        if bits:
            new.bits = bits
        return new"
83,"def simplify_spline(self, path_indexes=None, smooth=.0002):
        
        return simplify.simplify_spline(self,
                                        path_indexes=path_indexes,
                                        smooth=smooth)"
84,"def get_attributes(self, attributes, default=):
        
        if isinstance(attributes, str):
            attributes = [attributes]

        attrs = [getattr(self, attr, default) for attr in attributes]

        if len(attrs) == 1:
            return attrs[0]

        return tuple(attrs)"
85,"def update(self, *data, **kwargs):
        
        updated_item_count = 0
        try:
            items = self.all()

            for item in items:
                item.update(*data, **kwargs)
                updated_item_count += 1
        except Exception:
            
            raise

        return updated_item_count"
86,"def name(self, decl_string):
        
        if not self.has_pattern(decl_string):
            return decl_string
        args_begin = decl_string.find(self.__begin)
        return decl_string[0: args_begin].strip()"
87,"def DeleteInstance(self, InstanceName, **extra):
        
        

        exc = None
        method_name = 

        if self._operation_recorders:
            self.operation_recorder_reset()
            self.operation_recorder_stage_pywbem_args(
                method=method_name,
                InstanceName=InstanceName,
                **extra)

        try:

            stats = self.statistics.start_timer(method_name)
            namespace = self._iparam_namespace_from_objectname(
                InstanceName, )
            instancename = self._iparam_instancename(InstanceName)

            self._imethodcall(
                method_name,
                namespace,
                InstanceName=instancename,
                has_return_value=False,
                **extra)
            return

        except (CIMXMLParseError, XMLParseError) as exce:
            exce.request_data = self.last_raw_request
            exce.response_data = self.last_raw_reply
            exc = exce
            raise
        except Exception as exce:
            exc = exce
            raise
        finally:
            self._last_operation_time = stats.stop_timer(
                self.last_request_len, self.last_reply_len,
                self.last_server_response_time, exc)
            if self._operation_recorders:
                self.operation_recorder_stage_result(None, exc)"
88,"def map2cube(data_map, layout):
    r

    if np.all(np.array(data_map.shape) % np.array(layout)):
        raise ValueError(
                         )

    d_shape = np.array(data_map.shape) // np.array(layout)

    return np.array([data_map[(slice(i * d_shape[0], (i + 1) * d_shape[0]),
                    slice(j * d_shape[1], (j + 1) * d_shape[1]))] for i in
                    range(layout[0]) for j in range(layout[1])])"
89,"def add_rule_to_model(model, rule, annotations=None):
    
    try:
        model.add_component(rule)
        
        if annotations:
            model.annotations += annotations
    
    except ComponentDuplicateNameError:
        msg = ""Rule %s already in model! Skipping."" % rule.name
        logger.debug(msg)"
90,"def updatewhere(clas,pool_or_cursor,where_keys,**update_keys):
    ""this doesn,=%s and update %s set %s where %s'%(clas.TABLE,setclause,whereclause)
    vals = tuple(update_keys.values()+where_keys.values())
    commit_or_execute(pool_or_cursor,q,vals)"
91,"def _set_attribute(self, attribute, name, value):
        
        try:
            if attribute is None:
                attribute = self._attribute_file_open( name )
            else:
                attribute.seek(0)

            if isinstance(value, str):
                value = value.encode()
            attribute.write(value)
            attribute.flush()
        except Exception as ex:
            self._raise_friendly_access_error(ex, name)
        return attribute"
92,"def link_set(self, rel, href, allow_duplicates=False, **atts):
        
        if (self.ln is None):
            
            self.ln = []
            link = None
        else:
            link = self.link(rel)
        if (link is not None and not allow_duplicates):
            
            link[] = href
        else:
            
            link = {: rel, : href}
            self.ln.append(link)
        for k in atts:
            link[k] = atts[k]"
93,"def _clear_maximum_terms(self, match_key):
        
        try:  
            del self._query_terms[match_key][]
        except KeyError:
            pass
        try:  
            del self._query_terms[match_key][]
        except KeyError:
            pass
        try:
            if self._query_terms[match_key] == {}:
                del self._query_terms[match_key]
        except KeyError:
            pass"
94,"def is_valid_ipv6_prefix(ipv6_prefix):
    

    
    if not isinstance(ipv6_prefix, str):
        return False

    tokens = ipv6_prefix.split()
    if len(tokens) != 2:
        return False

    
    return is_valid_ipv6(tokens[0]) and is_valid_ip_prefix(tokens[1], 128)"
95,"def _sigma_pi_midE(self, Tp):
        
        m_p = self._m_p
        Qp = (Tp - self._Tth) / m_p
        multip = -6e-3 + 0.237 * Qp - 0.023 * Qp ** 2
        return self._sigma_inel(Tp) * multip"
96,"def _post_md5_skip_on_check(self, key, md5_match):
        
        
        with self._md5_meta_lock:
            src, rfile = self._md5_map.pop(key)
        uid = blobxfer.operations.upload.Uploader.create_unique_id(src, rfile)
        if md5_match:
            with self._upload_lock:
                self._upload_set.remove(uid)
                self._upload_total -= 1
            if self._general_options.dry_run:
                logger.info(.format(
                    src.absolute_path, rfile.path))
        else:
            if self._general_options.dry_run:
                with self._upload_lock:
                    self._upload_set.remove(uid)
                    self._upload_total -= 1
                logger.info(.format(
                    src.absolute_path, rfile.path))
            else:
                self._add_to_upload_queue(src, rfile, uid)"
97,"def _monitor(self):
        
        err_msg = (""invalid internal state:""
                   "" _stop_nowait can not be set if _stop is not set"")
        assert self._stop.isSet() or not self._stop_nowait.isSet(), err_msg

        q = self.queue
        has_task_done = hasattr(q, )
        while not self._stop.isSet():
            try:
                record = self.dequeue(True)
                if record is self._sentinel_item:
                    break
                self.handle(record)
                if has_task_done:
                    q.task_done()
            except queue.Empty:
                pass

        
        
        while not self._stop_nowait.isSet():
            try:
                record = self.dequeue(False)
                if record is self._sentinel_item:
                    break
                self.handle(record)
                if has_task_done:
                    q.task_done()
            except queue.Empty:
                break"
98,"def exists(self, file_ref):
        
        if file_ref not in FILE_REFS:
            raise ValueError(""Unknown file_ref: . Available: ."" % (file_ref, list(sorted(FILE_REFS._fields))))
        return os.path.isfile(self._path(file_ref))"
99,"def make_summary(self, find_ocv=False, find_ir=False,
                     find_end_voltage=False,
                     use_cellpy_stat_file=None, all_tests=True,
                     dataset_number=0, ensure_step_table=True,
                     convert_date=False):
        

        
        if self.tester == ""arbin"":
            convert_date = True

        if ensure_step_table is None:
            ensure_step_table = self.ensure_step_table
        
        
        
        
        
        if use_cellpy_stat_file is None:
            use_cellpy_stat_file = prms.Reader.use_cellpy_stat_file
            self.logger.debug(""using use_cellpy_stat_file from prms"")
            self.logger.debug(f""use_cellpy_stat_file: {use_cellpy_stat_file}"")

        if all_tests is True:
            for j in range(len(self.datasets)):
                txt = ""creating summary for file ""
                test = self.datasets[j]
                if not self._is_not_empty_dataset(test):
                    self.logger.info(""empty test %i"" % j)
                    return
                if isinstance(test.loaded_from, (list, tuple)):
                    for f in test.loaded_from:
                        txt += f
                        txt += ""\n""
                else:
                    txt += str(test.loaded_from)

                if not test.mass_given:
                    txt += "" mass for test %i is not given"" % j
                    txt += "" setting it to %f mg"" % test.mass
                self.logger.debug(txt)

                self._make_summary(j,
                                   find_ocv=find_ocv,
                                   find_ir=find_ir,
                                   find_end_voltage=find_end_voltage,
                                   use_cellpy_stat_file=use_cellpy_stat_file,
                                   ensure_step_table=ensure_step_table,
                                   convert_date=convert_date,
                                   )
        else:
            self.logger.debug(""creating summary for only one test"")
            dataset_number = self._validate_dataset_number(dataset_number)
            if dataset_number is None:
                self._report_empty_dataset()
                return
            self._make_summary(dataset_number,
                               find_ocv=find_ocv,
                               find_ir=find_ir,
                               find_end_voltage=find_end_voltage,
                               use_cellpy_stat_file=use_cellpy_stat_file,
                               ensure_step_table=ensure_step_table,
                               convert_date=convert_date,
                               )
        return self"
100,"def plot_observed_recurrence(
        catalogue, completeness, dmag, end_year=None, filename=None,
        figure_size=(8, 6), filetype=, dpi=300, ax=None):
    
    
    if isinstance(completeness, float):
        
        completeness = np.array([[np.min(catalogue.data[]),
                                  completeness]])
    if not end_year:
        end_year = catalogue.update_end_year()
    catalogue.data[""dtime""] = catalogue.get_decimal_time()
    cent_mag, t_per, n_obs = get_completeness_counts(catalogue,
                                                     completeness,
                                                     dmag)
    obs_rates = n_obs / t_per
    cum_obs_rates = np.array([np.sum(obs_rates[i:])
                              for i in range(len(obs_rates))])

    if ax is None:
        fig, ax = plt.subplots(figsize=figure_size)
    else:
        fig = ax.get_figure()

    ax.semilogy(cent_mag, obs_rates, , label=""Incremental"")
    ax.semilogy(cent_mag, cum_obs_rates, , label=""Cumulative"")
    ax.set_xlim([cent_mag[0] - 0.1, cent_mag[-1] + 0.1])
    ax.set_xlabel()
    ax.set_ylabel()
    ax.legend()
    _save_image(fig, filename, filetype, dpi)"
101,"def predict_size_distribution_component_models(self, model_names, input_columns, output_columns, metadata_cols,
                                                   data_mode=""forecast"", location=6):
        
        groups = self.size_distribution_models.keys()
        predictions = pd.DataFrame(self.data[data_mode][""combo""][metadata_cols])
        for group in groups:
            group_idxs = self.data[data_mode][""combo""][self.group_col] == group
            group_count = np.count_nonzero(group_idxs)
            print(self.size_distribution_models[group])
            if group_count > 0:
                log_mean = self.size_distribution_models[group][""lognorm""][""mean""]
                log_sd = self.size_distribution_models[group][""lognorm""][""sd""]
                for m, model_name in enumerate(model_names):
                    raw_preds = np.zeros((group_count, len(output_columns)))
                    for c in range(len(output_columns)):
                        raw_preds[:, c] = self.size_distribution_models[group][
                            ""pc_{0:d}"".format(c)][model_name].predict(self.data[data_mode][""combo""].loc[group_idxs,
                                                                                                        input_columns])
                    log_norm_preds = self.size_distribution_models[group][""lognorm""][""pca""].inverse_transform(raw_preds)
                    log_norm_preds[:, 0] *= -1
                    multi_predictions = np.exp(log_norm_preds * log_sd + log_mean)
                    if multi_predictions.shape[1] == 2:
                        multi_predictions_temp = np.zeros((multi_predictions.shape[0], 3))
                        multi_predictions_temp[:, 0] = multi_predictions[:, 0]
                        multi_predictions_temp[:, 1] = location
                        multi_predictions_temp[:, 2] = multi_predictions[:, 1]
                        multi_predictions = multi_predictions_temp
                    for p, pred_col in enumerate([""shape"", ""location"", ""scale""]):
                        predictions.loc[group_idxs, model_name.replace("" "", ""-"") + ""_"" + pred_col] = \
                            multi_predictions[:, p]
        return predictions"
102,"def killProcess(self, pid):
        
        SYNCHRONIZE = 0x00100000
        PROCESS_TERMINATE = 0x0001
        hProcess = self._kernel32.OpenProcess(SYNCHRONIZE|PROCESS_TERMINATE, True, pid)
        result = self._kernel32.TerminateProcess(hProcess, 0)
        self._kernel32.CloseHandle(hProcess)"
103,"def cli(env, columns):
    

    mgr = SoftLayer.UserManager(env.client)
    users = mgr.list_users()

    table = formatting.Table(columns.columns)
    for user in users:
        table.add_row([value or formatting.blank()
                       for value in columns.row(user)])

    env.fout(table)"
104,"def _add_consequences(self, variant_obj, raw_variant_line):
        
        consequences = []
        for consequence in SO_TERMS:
            if consequence in raw_variant_line:
                consequences.append(consequence)
        
        variant_obj.consequences = consequences"
105,"def view_build():
    
    build = g.build
    page_size = min(request.args.get(, 10, type=int), 50)
    offset = request.args.get(, 0, type=int)

    ops = operations.BuildOps(build.id)
    has_next_page, candidate_list, stats_counts = ops.get_candidates(
        page_size, offset)

    
    release_dict = {}
    created_dict = {}
    run_stats_dict = {}
    for candidate in candidate_list:
        release_list = release_dict.setdefault(candidate.name, [])
        release_list.append(candidate)
        max_created = created_dict.get(candidate.name, candidate.created)
        created_dict[candidate.name] = max(candidate.created, max_created)
        run_stats_dict[candidate.id] = dict(
            runs_total=0,
            runs_complete=0,
            runs_successful=0,
            runs_failed=0,
            runs_baseline=0,
            runs_pending=0)

    
    for release_list in release_dict.itervalues():
        release_list.sort(key=lambda x: x.number, reverse=True)

    
    release_age_list = [
        (value, key) for key, value in created_dict.iteritems()]
    release_age_list.sort(reverse=True)
    release_name_list = [key for _, key in release_age_list]

    
    for candidate_id, status, count in stats_counts:
        stats_dict = run_stats_dict[candidate_id]
        for key in ops.get_stats_keys(status):
            stats_dict[key] += count

    return render_template(
        ,
        build=build,
        release_name_list=release_name_list,
        release_dict=release_dict,
        run_stats_dict=run_stats_dict,
        has_next_page=has_next_page,
        current_offset=offset,
        next_offset=offset + page_size,
        last_offset=max(0, offset - page_size),
        page_size=page_size)"
106,"def _sim_texture(r1, r2):
    
    return sum([min(a, b) for a, b in zip(r1[""hist_t""], r2[""hist_t""])])"
107,"def hooks(self, project):
        

        return self.get_queryset().filter(
            Q(project=None) |
            Q(project=project)
        ).distinct()"
108,"def cumulative_statistics(self):
        
        if self._cumulative_statistics is None:
            self._cumulative_statistics = WorkflowCumulativeStatisticsList(
                self._version,
                workspace_sid=self._solution[],
                workflow_sid=self._solution[],
            )
        return self._cumulative_statistics"
109,"def _determinebase_address(self):
        

        traced_address = set()
        self.functions = set()
        self.call_map = networkx.DiGraph()
        self.cfg = networkx.DiGraph()
        initial_state = self.project.factory.blank_state(mode=""fastpath"")
        initial_options = initial_state.options - { o.TRACK_CONSTRAINTS } - o.refs
        initial_options |= { o.SUPER_FASTPATH }
        
        initial_state.options = initial_options
        
        
        
        
        
        function_exits = defaultdict(set)

        dump_file_prefix = self.project.filename

        if self._pickle_intermediate_results and \
                os.path.exists(dump_file_prefix + ""_indirect_jumps.angr""):
            l.debug(""Loading existing intermediate results."")
            self._indirect_jumps = pickle.load(open(dump_file_prefix + ""_indirect_jumps.angr"", ""rb""))
            self.cfg = pickle.load(open(dump_file_prefix + ""_coercecfg.angr"", ""rb""))
            self._unassured_functions = pickle.load(open(dump_file_prefix + ""_unassured_functions.angr"", ""rb""))
        else:
            
            
            self._scan_function_prologues(traced_address, function_exits, initial_state)

            if self._pickle_intermediate_results:
                l.debug(""Dumping intermediate results."")
                pickle.dump(self._indirect_jumps, open(dump_file_prefix + ""_indirect_jumps.angr"", ""wb""), -1)
                pickle.dump(self.cfg, open(dump_file_prefix + ""_coercecfg.angr"", ""wb""), -1)
                pickle.dump(self._unassured_functions, open(dump_file_prefix + ""_unassured_functions.angr"", ""wb""), -1)

        if len(self._indirect_jumps):
            
            
            function_starts = self._process_indirect_jumps()

            self.base_address = self._solve_forbase_address(function_starts, self._unassured_functions)

            l.info(""Base address should be 0x%x"", self.base_address)

        else:
            l.debug(""No indirect jumps are found. We switch to the slowpath mode."")
            
            while True:
                next_addr = self._get_next_code_addr(initial_state)
                percentage = self._seg_list.occupied_size * 100.0 / (self._valid_memory_region_size)
                l.info(""Analyzing %xh, progress %0.04f%%"", next_addr, percentage)
                if next_addr is None:
                    break

                self.call_map.add_node(next_addr)

                self._scan_code(traced_address, function_exits, initial_state, next_addr)

        
        
        for src, s in function_exits.items():
            if src in self.call_map:
                for target in s:
                    if target in self.call_map:
                        self.call_map.add_edge(src, target)

        nodes = sorted(self.call_map.nodes())
        for i in range(len(nodes) - 1):
            if nodes[i] >= nodes[i + 1] - 4:
                for dst in self.call_map.successors(nodes[i + 1]):
                    self.call_map.add_edge(nodes[i], dst)
                for src in self.call_map.predecessors(nodes[i + 1]):
                    self.call_map.add_edge(src, nodes[i])
                self.call_map.remove_node(nodes[i + 1])

        l.debug(""Construction finished."")"
110,"def get_default_object_parsers(parser_finder: ParserFinder, conversion_finder: ConversionFinder) -> List[AnyParser]:
    
    return [SingleFileParserFunction(parser_function=read_object_from_pickle,
                                     streaming_mode=False,
                                     supported_exts={},
                                     supported_types={AnyObject}),
            MultifileObjectParser(parser_finder, conversion_finder)
            ]"
111,"def p_relate_using_statement_1(self, p):
        
        p[0] = RelateUsingNode(from_variable_name=p[2],
                               to_variable_name=p[4],
                               rel_id=p[6],
                               phrase=None,
                               using_variable_name=p[8])"
112,"def requires_auth(func):
    
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        if self.token is None or self.token_expired:
            self.login()
        elif self.token_needs_refresh:
            self.refresh_token()
        return func(self, *args, **kwargs)
    return wrapper"
113,"def getlist(self, key: ) -> Sequence[object]:
        r
        if not (isinstance(key, type(self)) and
                key.type is EntityType.property):
            return []
        claims_map = self.attributes.get() or {}
        assert isinstance(claims_map, collections.abc.Mapping)
        claims = claims_map.get(key.id, [])
        claims.sort(key=lambda claim: claim[],  
                    reverse=True)
        logger = logging.getLogger(__name__ + )
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(,
                         __import__().pformat(claims))
        decode = self.client.decode_datavalue
        return [decode(snak[], snak[])
                for snak in (claim[] for claim in claims)]"
114,"def get(self, sid):
        
        return TaskQueueContext(self._version, workspace_sid=self._solution[], sid=sid, )"
115,"def get_reference_to_class(cls, class_or_class_name):
        
        if isinstance(class_or_class_name, type):
            return class_or_class_name

        elif isinstance(class_or_class_name, string_types):
            if "":"" in class_or_class_name:
                mod_name, class_name = class_or_class_name.split("":"")

                if not mod_name in sys.modules:
                    __import__(mod_name)

                mod = sys.modules[mod_name]
                return mod.__dict__[class_name]

            else:
                return cls.load_class_from_locals(class_or_class_name)

        else:
            msg = ""Unexpected Type "" % type(class_or_class_name)
            raise InternalCashewException(msg)"
116,"def wiki_2x2_base():
  
  hparams = mtf_transformer.mtf_transformer_base_lm()
  hparams.shared_embedding_and_softmax_weights = False
  
  hparams.attention_dropout = 0.0
  hparams.relu_dropout = 0.0
  hparams.layer_prepostprocess_dropout = 0.0
  hparams.max_length = 1024
  
  hparams.batch_size = 32
  
  
  
  hparams.learning_rate_schedule = ""rsqrt_decay""
  hparams.mesh_shape = ""all:8""
  hparams.layout = ""batch:all;experts:all""

  
  moe.set_default_moe_hparams(hparams)
  hparams.moe_num_experts = 16
  hparams.moe_hidden_size = 8192

  hparams.decoder_layers = [""att"", ""drd""] * 6
  hparams.d_model = 1024
  hparams.d_ff = 2048
  hparams.d_kv = 128
  hparams.num_heads = 4

  return hparams"
117,"def column_definition(table_name, col_name):
    
    col_type = orca.get_table(table_name).column_type(col_name)

    if col_type != :
        return jsonify(type=col_type)

    filename, lineno, source = \
        orca.get_raw_column(table_name, col_name).func_source_data()

    html = highlight(source, PythonLexer(), HtmlFormatter())

    return jsonify(
        type=, filename=filename, lineno=lineno, text=source,
        html=html)"
118,"def is_img(obj):
    
    try:
        get_data   = getattr(obj, )
        get_affine = getattr(obj, )

        return isinstance(get_data,   collections.Callable) and \
               isinstance(get_affine, collections.Callable)
    except AttributeError:
        return False"
119,"def execstr_funckw(func):
    
    import utool as ut
    funckw = ut.get_func_kwargs(func)
    return ut.execstr_dict(funckw, explicit=True)"
120,"def syllabify(word):
    
    compound = not word.isalpha()
    syllabify = _syllabify_complex if compound else _syllabify_simplex
    syllabifications = list(syllabify(word))

    
    if len(syllabifications) > 1:
        syllabifications = rank(syllabifications)

    for word, rules in syllabifications:
        yield _post_process(word, rules)"
121,"def Collect(self, knowledge_base):
    
    environment_variable = knowledge_base.GetEnvironmentVariable()
    allusersappdata = getattr(environment_variable, , None)

    if not allusersappdata:
      environment_variable = knowledge_base.GetEnvironmentVariable(
          )
      allusersdata = getattr(environment_variable, , None)

      if allusersdata:
        allusersappdata = .join([allusersdata, ])

    if allusersappdata:
      environment_variable = artifacts.EnvironmentVariableArtifact(
          case_sensitive=False, name=, value=allusersappdata)

      try:
        logger.debug(.format(
            , allusersappdata))
        knowledge_base.AddEnvironmentVariable(environment_variable)
      except KeyError:
        
        pass"
122,"def metrics(self, *metrics):
        
        for m in metrics:
            self._cauldron.use(self._shelf.find(m, Metric))
        self.dirty = True
        return self"
123,"def domain_get(auth=None, **kwargs):
    **
    cloud = get_operator_cloud(auth)
    kwargs = _clean_kwargs(**kwargs)
    return cloud.get_domain(**kwargs)"
124,"def update_account_password_policy(allow_users_to_change_password=None,
                                   hard_expiry=None, max_password_age=None,
                                   minimum_password_length=None,
                                   password_reuse_prevention=None,
                                   require_lowercase_characters=None,
                                   require_numbers=None, require_symbols=None,
                                   require_uppercase_characters=None,
                                   region=None, key=None, keyid=None,
                                   profile=None):
    
    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)
    try:
        conn.update_account_password_policy(allow_users_to_change_password,
                                            hard_expiry, max_password_age,
                                            minimum_password_length,
                                            password_reuse_prevention,
                                            require_lowercase_characters,
                                            require_numbers, require_symbols,
                                            require_uppercase_characters)
        log.info()
        return True
    except boto.exception.BotoServerError as e:
        log.debug(e)
        msg = 
        log.error(msg)
        return False"
125,"def select_current_cell_in_visible_portion(self):
        
        cursor = self.textCursor()
        cursor.movePosition(QTextCursor.StartOfBlock)
        cur_pos = prev_pos = cursor.position()

        beg_pos = self.cursorForPosition(QPoint(0, 0)).position()
        bottom_right = QPoint(self.viewport().width() - 1,
                              self.viewport().height() - 1)
        end_pos = self.cursorForPosition(bottom_right).position()

        
        
        while self.is_cell_separator(cursor):
            cursor.movePosition(QTextCursor.NextBlock)
            prev_pos = cur_pos
            cur_pos = cursor.position()
            if cur_pos == prev_pos:
                return cursor, False, False
        prev_pos = cur_pos
        
        while not self.is_cell_separator(cursor)\
          and cursor.position() >= beg_pos:
            cursor.movePosition(QTextCursor.PreviousBlock)
            prev_pos = cur_pos
            cur_pos = cursor.position()
            if cur_pos == prev_pos:
                if self.is_cell_separator(cursor):
                    return cursor, False, False
                else:
                    break
        cell_at_screen_start = cursor.position() <= beg_pos
        cursor.setPosition(prev_pos)
        cell_at_file_start = cursor.atStart()
        
        if not cell_at_file_start:
            cursor.movePosition(QTextCursor.PreviousBlock)
            cursor.movePosition(QTextCursor.NextBlock,
                                QTextCursor.KeepAnchor)
        
        
        
        while not self.is_cell_separator(cursor)\
          and cursor.position() <= end_pos:
            cursor.movePosition(QTextCursor.NextBlock,
                                QTextCursor.KeepAnchor)
            cur_pos = cursor.position()
            if cur_pos == prev_pos:
                cursor.movePosition(QTextCursor.EndOfBlock,
                                    QTextCursor.KeepAnchor)
                break
            prev_pos = cur_pos
        cell_at_file_end = cursor.atEnd()
        cell_at_screen_end = cursor.position() >= end_pos
        return cursor,\
               cell_at_file_start and cell_at_file_end,\
               cell_at_screen_start and cell_at_screen_end"
126,"def get_video(self, node):
        
        video = Video()
        video.embed_code = self.get_embed_code(node)
        video.embed_type = self.get_embed_type(node)
        video.width = self.get_width(node)
        video.height = self.get_height(node)
        video.src = self.get_src(node)
        video.provider = self.get_provider(video.src)
        return video"
127,"def collect_fields(
        self,
        runtime_type: GraphQLObjectType,
        selection_set: SelectionSetNode,
        fields: Dict[str, List[FieldNode]],
        visited_fragment_names: Set[str],
    ) -> Dict[str, List[FieldNode]]:
        
        for selection in selection_set.selections:
            if isinstance(selection, FieldNode):
                if not self.should_include_node(selection):
                    continue
                name = get_field_entry_key(selection)
                fields.setdefault(name, []).append(selection)
            elif isinstance(selection, InlineFragmentNode):
                if not self.should_include_node(
                    selection
                ) or not self.does_fragment_condition_match(selection, runtime_type):
                    continue
                self.collect_fields(
                    runtime_type,
                    selection.selection_set,
                    fields,
                    visited_fragment_names,
                )
            elif isinstance(selection, FragmentSpreadNode):
                frag_name = selection.name.value
                if frag_name in visited_fragment_names or not self.should_include_node(
                    selection
                ):
                    continue
                visited_fragment_names.add(frag_name)
                fragment = self.fragments.get(frag_name)
                if not fragment or not self.does_fragment_condition_match(
                    fragment, runtime_type
                ):
                    continue
                self.collect_fields(
                    runtime_type, fragment.selection_set, fields, visited_fragment_names
                )
        return fields"
128,"def pop_one(self, priority=None):
        
        with self.lock:
            if not priority:
                priority = self.highest_entry()
            if self.closed:
                return [THREAD_STOP]
            elif not self.queue:
                return None
            else:
                v =self.pop(priority=priority)
                if v is THREAD_STOP:  
                    self.closed.go()
                return v"
129,"def editpermissions_group_view(self, request, group_id, forum_id=None):
        
        group = get_object_or_404(Group, pk=group_id)
        forum = get_object_or_404(Forum, pk=forum_id) if forum_id else None

        
        context = self.get_forum_perms_base_context(request, forum)
        context[] = forum
        context[] = .format(_(), group)
        context[] = self._get_permissions_form(
            request, GroupForumPermission, {: forum, : group},
        )

        return render(request, self.editpermissions_group_view_template_name, context)"
130,"def ConfigSectionMap(Config, section):
    
    cfg = {}
    options = Config.options(section)
    for option in options:
        try:
            cfg[option] = Config.get(section, option)
            if cfg[option] == -1:
                logging.debug(""skip: {0}"".format(option))
        except:
            logging.debug(""exception on {0}!"".format(option))
            cfg[option] = None
    return cfg"
131,"def logout(self, request):
        ""Logs out user and redirects them to Nexus home""
        from django.contrib.auth import logout

        logout(request)

        return HttpResponseRedirect(reverse(, current_app=self.name))"
132,"def send(self, sender: PytgbotApiBot):
        
        return sender.send_chat_action(
            
            action=self.action, chat_id=self.receiver
        )"
133,"def PyobjColumns(obj):
    
    return [ColumnAttr(k, type(getattr(obj, k))) for k in getPublicAttrs(obj)]"
134,"def bind(cls, target):
		
		
		if cls.__bound__ is not None:
			return cls
		
		cls.__bound__ = cls.get_collection(target)
		
		return cls"
135,"def _print_ll(self):
        
        print ""
        for i in range(self.width):
            print ""  %4d   ""%i,
        print
        for L in [, , , ]:
            print ""
            for i in range(self.width):
                print  ""%8.3f ""%self.ll[i][L],
            print"
136,"def ffmpeg_has_loudnorm():
    
    cmd_runner = CommandRunner([get_ffmpeg_exe(), ])
    cmd_runner.run_command()
    output = cmd_runner.get_output()
    if  in output:
        return True
    else:
        logger.warning(
            ""Your ffmpeg version does not support the  filter. ""
            ""Please make sure you are running ffmpeg v3.1 or above.""
        )
        return False"
137,"def get_segment_summary_times(scienceFile, segmentName):
    
    
    segmentName = segmentName.split()
    if not len(segmentName) in [2,3]:
        raise ValueError(""Invalid channel name %s."" %(segmentName))
    ifo = segmentName[0]
    channel = segmentName[1]
    version = 
    if len(segmentName) == 3:
        version = int(segmentName[2])

    
    xmldoc = utils.load_filename(scienceFile.cache_entry.path,
                             gz=scienceFile.cache_entry.path.endswith(""gz""),
                             contenthandler=ContentHandler)

    
    segmentDefTable = table.get_table(xmldoc, ""segment_definer"")
    for entry in segmentDefTable:
        if (entry.ifos == ifo) and (entry.name == channel):
            if len(segmentName) == 2 or (entry.version==version):
                segDefID = entry.segment_def_id
                break
    else:
        raise ValueError(""Cannot find channel %s in segment_definer table.""\
                         %(segmentName))

    
    segmentSummTable = table.get_table(xmldoc, ""segment_summary"")
    summSegList = segments.segmentlist([])
    for entry in segmentSummTable:
        if entry.segment_def_id == segDefID:
            segment = segments.segment(entry.start_time, entry.end_time)
            summSegList.append(segment)
    summSegList.coalesce()

    return summSegList"
138,"def get_new_apikey(lcc_server):
    

    USERHOME = os.path.expanduser()
    APIKEYFILE = os.path.join(USERHOME,
                              ,
                              ,
                               % lcc_server.replace(
                                  ,
                                  
                              ).replace(
                                  ,
                                  
                              ))

    
    url =  % lcc_server

    
    resp = urlopen(url)

    if resp.code == 200:

        respdict = json.loads(resp.read())

    else:

        LOGERROR( %
                 lcc_server)
        LOGERROR( % resp.status_code)
        return None

    
    
    
    
    apikey = respdict[][]
    expires = respdict[][]

    

    if not os.path.exists(os.path.dirname(APIKEYFILE)):
        os.makedirs(os.path.dirname(APIKEYFILE))

    with open(APIKEYFILE,) as outfd:
        outfd.write( % (apikey, expires))

    
    os.chmod(APIKEYFILE, 0o100600)

    LOGINFO( % (lcc_server,
                                                                   expires))
    LOGINFO( % APIKEYFILE)

    return apikey, expires"
139,"def post_list(self, request, **kwargs):
        
        
        
        
        request.method =   
        dispatch_request = convert_post_to_VERB(request, )
        return self.dispatch(, dispatch_request, **kwargs)"
140,"def leave_group(self, group_jid):
        
        log.info(""[+] Leaving group {}"".format(group_jid))
        return self._send_xmpp_element(group_adminship.LeaveGroupRequest(group_jid))"
141,"def get_help_width():
  
  if not sys.stdout.isatty() or termios is None or fcntl is None:
    return _DEFAULT_HELP_WIDTH
  try:
    data = fcntl.ioctl(sys.stdout, termios.TIOCGWINSZ, )
    columns = struct.unpack(, data)[1]
    
    
    if columns >= _MIN_HELP_WIDTH:
      return columns
    
    return int(os.getenv(, _DEFAULT_HELP_WIDTH))

  except (TypeError, IOError, struct.error):
    return _DEFAULT_HELP_WIDTH"
142,"def max_number(self, rows: List[Row], column: NumberColumn) -> Number:
        
        cell_values = [row.values[column.name] for row in rows]
        if not cell_values:
            return 0.0  
        if not all([isinstance(value, Number) for value in cell_values]):
            raise ExecutionError(f""Invalid values for number selection function: {cell_values}"")
        return max(cell_values)"
143,"def telnet_login(
        self,
        pri_prompt_terminator=""
        alt_prompt_terminator="">"",
        username_pattern=r""Login Name:"",
        pwd_pattern=r""assword"",
        delay_factor=1,
        max_loops=60,
    ):
        
        super(HPProcurveTelnet, self).telnet_login(
            pri_prompt_terminator=pri_prompt_terminator,
            alt_prompt_terminator=alt_prompt_terminator,
            username_pattern=username_pattern,
            pwd_pattern=pwd_pattern,
            delay_factor=delay_factor,
            max_loops=max_loops,
        )"
144,"def list(self, list_folders=False, list_files=False):
        

        a_files = os.listdir(self.folder.path)
        for a_file in a_files:
            path = self.folder.child(a_file)
            if os.path.isdir(path):
                if list_folders:
                    yield Folder(path)
            elif list_files:
                if not self.pattern or fnmatch.fnmatch(a_file, self.pattern):
                    yield File(path)"
145,"def get_components(self):
        
        components = []
        for app_id in self.components:
            components.append(self.components[app_id])
        return components"
146,"def check_commands(self, cmd_list):
        
        
        
        if isinstance(cmd_list, basestring):
            cmd_list = [cmd_list]

        for cmd in cmd_list:
            if self.command_run(""which {}"".format(cmd)) == 0:
                return cmd"
147,"def main():
    
    from docopt import docopt
    args = docopt(__doc__, version=kp.version)

    kp.logger.set_level(""km3pipe"", args[])

    pipe = kp.Pipeline()
    pipe.attach(
        kp.io.ch.CHPump,
        host=args[],
        port=int(args[]),
        tags=args[],
        timeout=int(args[]),
        max_queue=int(args[])
    )
    pipe.attach(LigierSender, target_ip=args[], port=int(args[]))
    pipe.drain()"
148,"def findWCSExtn(filename):
    
    rootname,extroot = fileutil.parseFilename(filename)
    extnum = None
    if extroot is None:
        fimg = fits.open(rootname, memmap=False)
        for i,extn in enumerate(fimg):
            if  in extn.header:
                refwcs = wcsutil.HSTWCS(.format(rootname,i))
                if refwcs.wcs.has_cd():
                    extnum = .format(i)
                    break
        fimg.close()
    else:
        try:
            refwcs = wcsutil.HSTWCS(filename)
            if refwcs.wcs.has_cd():
                extnum = extroot
        except:
            extnum = None

    return extnum"
149,"def find_chunks(tagged, language=""en""):
    
    chunked = [x for x in tagged]
    tags = """".join(""%s%s"" % (tag, SEPARATOR) for token, tag in tagged)
    
    for tag, rule in CHUNKS[int(language in (""ca"", ""es"", ""pt"", ""fr"", ""it"", ""pt"", ""ro""))]:
        for m in rule.finditer(tags):
            
            
            i = m.start()
            j = tags[:i].count(SEPARATOR)
            n = m.group(0).count(SEPARATOR)
            for k in range(j, j+n):
                if len(chunked[k]) == 3:
                    continue
                if len(chunked[k]) < 3:
                    
                    if k == j and chunked[k][1] in (""CC"", ""CJ"", "",""):
                        j += 1
                    
                    elif k == j:
                        chunked[k].append(""B-"" + tag)
                    
                    else:
                        chunked[k].append(""I-"" + tag)
    
    for chink in filter(lambda x: len(x) < 3, chunked):
        chink.append(""O"")
    
    for i, (word, tag, chunk) in enumerate(chunked):
        if tag.startswith(""RB"") and chunk == ""B-NP"":
            
            
            
            if i < len(chunked)-1 and not chunked[i+1][1].startswith(""JJ""):
                chunked[i+0][2] = ""B-ADVP""
                chunked[i+1][2] = ""B-NP""
            if i < len(chunked)-1 and chunked[i+1][1] in (""CC"", ""CJ"", "",""):
                chunked[i+1][2] = ""O""
            if i < len(chunked)-2 and chunked[i+1][2] == ""O"":
                chunked[i+2][2] = ""B-NP""
    return chunked"
150,"def full_path(self):
        

        if self.parent:
            return os.path.join(self.parent.full_path, self.name)
        return self.name"
151,"def serialize(expr):
    

    result = None

    if isinstance(expr, string_types):
        result = expr

    elif expr is not None:
        result = .format(expr)

    return result"
152,"def fit(self, Z):
        
        X = Z[:, ] if isinstance(Z, DictRDD) else Z
        check_rdd(X, (np.ndarray, sp.spmatrix))
        if self.init == :
            self._mllib_model = MLlibKMeans.train(
                X.unblock(),
                self.n_clusters,
                maxIterations=self.max_iter,
                initializationMode=""k-means||"")
            self.cluster_centers_ = self._mllib_model.centers
        else:
            models = X.map(lambda X: super(SparkKMeans, self).fit(X))
            models = models.map(lambda model: model.cluster_centers_).collect()
            return super(SparkKMeans, self).fit(np.concatenate(models))"
153,"def _get_parent_id(self):
        
        parent_id = self._options.get()
        if parent_id:
            return parent_id

        from furious.context import get_current_async

        try:
            async = get_current_async()
        except errors.NotInContextError:
            async = None

        if async:
            parent_id = "":"".join([async.parent_id.split("":"")[0], async.id])
        else:
            parent_id = self.request_id

        self.update_options(parent_id=parent_id)

        return parent_id"
154,"def find_assign(data, varname):
    
    ASSIGN_RE = re.compile(BASE_ASSIGN_PATTERN.format(varname))

    if len(ASSIGN_RE.findall(data)) > 1:
        raise PluginError(.format(varname))

    if len(ASSIGN_RE.findall(data)) < 1:
        raise PluginError(
                          .format(varname))

    return ASSIGN_RE.search(data).group(2)"
155,"def _filter_extracted(self, extracted_list):
        
        _filtered = []
        for np in extracted_list:
            _np = np.split()
            if _np[0] in INSIGNIFICANT:
                _np.pop(0)
            try:
                if _np[-1] in INSIGNIFICANT:
                    _np.pop(-1)
                
                if _np[0] in INSIGNIFICANT:
                    _np.pop(0)
            except IndexError:
                _np = []
            if len(_np) > 0:
                _filtered.append("" "".join(_np))
        return _filtered"
156,"def onMouseOver(self, event, grid):
        
        x, y = grid.CalcUnscrolledPosition(event.GetX(), event.GetY())
        coords = grid.XYToCell(x, y)
        col = coords[1]
        row = coords[0]

        
        
        msg = grid.GetCellValue(row, col)
        if len(msg) > 15:
            event.GetEventObject().SetToolTipString(msg)
        else:
            event.GetEventObject().SetToolTipString()"
157,"def agent_reqs():
    

    echo_info(""Validating requirements-agent-release.txt..."")
    agent_reqs_content = parse_agent_req_file(read_file(get_agent_release_requirements()))
    ok_checks = 0
    unreleased_checks = 0
    failed_checks = 0
    for check_name in get_valid_checks():
        if check_name not in AGENT_V5_ONLY | NOT_CHECKS:
            package_name = get_package_name(check_name)
            check_version = get_version_string(check_name)
            pinned_version = agent_reqs_content.get(package_name)
            if package_name not in agent_reqs_content:
                unreleased_checks += 1
                echo_warning(.format(check_name))
            elif check_version != pinned_version:
                failed_checks += 1
                echo_failure(""{} has version {} but is pinned to {}"".format(check_name, check_version, pinned_version))
            else:
                ok_checks += 1

    if ok_checks:
        echo_success(""{} correctly pinned checks"".format(ok_checks))
    if unreleased_checks:
        echo_warning(""{} unreleased checks"".format(unreleased_checks))
    if failed_checks:
        echo_failure(""{} checks out of sync"".format(failed_checks))
        abort()"
158,"def getMov(self, profile):
        
        
        import mov
        return mov.MoVScoring(profile, self.getScoringVector(profile))"
159,"def write_name (self, url_data):
        
        args = (self.part(""name""), cgi.escape(url_data.name))
        self.writeln(u""<tr><td>%s</td><td>`%s'</td></tr>"" % args)"
160,"def qaaxn(mt, x, n, q, m = 1):
    
    
    q = float(q)
    j = (mt.i - q) / (1 + q)
    mtj = Actuarial(nt=mt.nt, i=j)
    return aaxn(mtj, x, n, m)"
161,"def set_redirect(self, url, status=HttpStatusCodes.HTTP_303):
        
        self.set_status(status)
        self.set_content()
        self.set_header(HttpResponseHeaders.LOCATION, url)"
162,"def offset2line(offset, linestarts):
    
    if len(linestarts) == 0 or offset < linestarts[0][0]:
        return 0
    low = 0
    high = len(linestarts) - 1
    mid = (low + high + 1) // 2
    while low <= high:
        if linestarts[mid][0] > offset:
            high = mid - 1
        elif linestarts[mid][0] < offset:
            low = mid + 1
        else:
            return linestarts[mid][1]
        mid = (low + high + 1) // 2
        pass
    
    if mid >= len(linestarts):
        return linestarts[len(linestarts)-1][1]
    return linestarts[high][1]"
163,"def save_to_object(self):
        

        tmpdir = tempfile.mkdtemp(""save_to_object"", dir=self.logdir)
        checkpoint_prefix = self.save(tmpdir)

        data = {}
        base_dir = os.path.dirname(checkpoint_prefix)
        for path in os.listdir(base_dir):
            path = os.path.join(base_dir, path)
            if path.startswith(checkpoint_prefix):
                with open(path, ""rb"") as f:
                    data[os.path.basename(path)] = f.read()

        out = io.BytesIO()
        data_dict = pickle.dumps({
            ""checkpoint_name"": os.path.basename(checkpoint_prefix),
            ""data"": data,
        })
        if len(data_dict) > 10e6:  
            logger.info(""Checkpoint size is {} bytes"".format(len(data_dict)))
        out.write(data_dict)

        shutil.rmtree(tmpdir)
        return out.getvalue()"
164,"def miller_index_from_sites(lattice, coords, coords_are_cartesian=True,
                            round_dp=4, verbose=True):
    
    if not isinstance(lattice, Lattice):
        lattice = Lattice(lattice)

    return lattice.get_miller_index_from_coords(
        coords, coords_are_cartesian=coords_are_cartesian, round_dp=round_dp,
        verbose=verbose)"
165,"def substitute(sequence, offset, ref, alt):
    
    n_ref = len(ref)
    sequence_ref = sequence[offset:offset + n_ref]
    assert str(sequence_ref) == str(ref), \
        ""Reference %s at offset %d != expected reference %s"" % \
        (sequence_ref, offset, ref)
    prefix = sequence[:offset]
    suffix = sequence[offset + n_ref:]
    return prefix + alt + suffix"
166,"def elements(self, using, value):
        
        return self._execute(Command.FIND_ELEMENTS, {
            : using,
            : value
        })"
167,"def clear_buffer(self):
        
        command_url = self.hub_url + 
        response = self.post_direct_command(command_url)
        self.logger.info(""clear_buffer: %s"", response)
        return response"
168,"def pclass_field_for_attribute(self):
        
        return self.type_model.pclass_field_for_type(
            required=self.required,
            default=self.default,
        )"
169,"def set_from_config_file(self, filename):
        
        if not os.path.exists(filename):
            raise LintConfigError(u""Invalid file path: {0}"".format(filename))
        self._config_path = os.path.abspath(filename)
        try:
            parser = ConfigParser()
            parser.read(filename)

            for section_name in parser.sections():
                for option_name, option_value in parser.items(section_name):
                    self.set_option(section_name, option_name, ustr(option_value))

        except ConfigParserError as e:
            raise LintConfigError(ustr(e))"
170,"def create(self):
        
        if os.path.isfile(self.path):
            if not os.path.exists(self.path):
                with open(self.path, ) as fileobj:
                    fileobj.write()
        else:
            os.makedirs(self.path)"
171,"def load_rc_file():
    
    
    
    
            else:
                print(line)
                print(tokens[0], ""is not a recognized resource file statement"")
        cf.close()
    except:
        print(""Unable to open resource file"", rcfile)"
172,"def connect(self, slot):
        
        self._ensure_slot_args(slot)
        if not self.is_connected(slot):
            self.slots.append(slot)"
173,"def is_ssh_available(host, opts, print_ssh_output=True):
    
    s = subprocess.Popen(
        ssh_command(opts) + [, , , ,
                              % (opts.user, host), stringify_command()],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT  
    )
    cmd_output = s.communicate()[0]  

    if s.returncode != 0 and print_ssh_output:
        
        print(textwrap.dedent().format(
            h=host,
            r=s.returncode,
            o=cmd_output.strip()
        ))

    return s.returncode == 0"
174,"def get_text_range(self, node):
    
    if not hasattr(node, ):
      return (0, 0)

    start = node.first_token.startpos
    if any(match_token(t, token.NEWLINE) for t in self.get_tokens(node)):
      
      start = self._text.rfind(, 0, start) + 1

    return (start, node.last_token.endpos)"
175,"def get_sub_area(area, xslice, yslice):
    
    new_area_extent = ((area.pixel_upper_left[0] +
                        (xslice.start - 0.5) * area.pixel_size_x),
                       (area.pixel_upper_left[1] -
                        (yslice.stop - 0.5) * area.pixel_size_y),
                       (area.pixel_upper_left[0] +
                        (xslice.stop - 0.5) * area.pixel_size_x),
                       (area.pixel_upper_left[1] -
                        (yslice.start - 0.5) * area.pixel_size_y))

    return AreaDefinition(area.area_id, area.name,
                          area.proj_id, area.proj_dict,
                          xslice.stop - xslice.start,
                          yslice.stop - yslice.start,
                          new_area_extent)"
176,"def optionIsSet(self, name):
    
    name = name.strip()
    if not self.hasOption(name):
      return False
    return self.getOption(name).isSet()"
177,"def funTransEdgeY(theta, rho):
    
    return np.matrix([[1, 0], [-np.tan(theta) / rho, 1]], dtype=np.double)"
178,"def parse(self, configManager, config):
        

        configFile = self._getConfigFile(config)

        if not configFile:
            return dict()

        yamlConfigs = yaml.load(configFile)
        if isinstance(yamlConfigs, dict):
            return yamlConfigs

        raise self.subparserException(""YAML config parsed did not result in a dictionary, but instead a: %s""
                                      % type(yamlConfigs))"
179,"def BVS(self, name, size,
            min=None, max=None, stride=None,
            uninitialized=False,
            explicit_name=None, key=None, eternal=False,
            inspect=True, events=True,
            **kwargs): 
        

        
        if key is not None and eternal and key in self.eternal_tracked_variables:
            r = self.eternal_tracked_variables[key]
            
            if size != r.length or min != r.args[1] or max != r.args[2] or stride != r.args[3] or uninitialized != r.args[4] or bool(explicit_name) ^ (r.args[0] == name):
                l.warning(""Variable %s being retrieved with differnt settings than it was tracked with"", name)
        else:
            r = claripy.BVS(name, size, min=min, max=max, stride=stride, uninitialized=uninitialized, explicit_name=explicit_name, **kwargs)
            if key is not None:
                self.register_variable(r, key, eternal)

        if inspect:
            self.state._inspect(, BP_AFTER, symbolic_name=next(iter(r.variables)), symbolic_size=size, symbolic_expr=r)
        if events:
            self.state.history.add_event(, name=next(iter(r.variables)), bits=size, **kwargs)
        if o.TRACK_SOLVER_VARIABLES in self.state.options:
            self.all_variables = list(self.all_variables)
            self.all_variables.append(r)
        return r"
180,"def decode(self, pdu):
        
        if _debug: NPCI._debug(""decode %s"", str(pdu))

        PCI.update(self, pdu)

        
        if len(pdu.pduData) < 2:
            raise DecodingError(""invalid length"")

        
        self.npduVersion = pdu.get()
        if (self.npduVersion != 0x01):
            raise DecodingError(""only version 1 messages supported"")

        
        self.npduControl = control = pdu.get()
        netLayerMessage = control & 0x80
        dnetPresent = control & 0x20
        snetPresent = control & 0x08
        self.pduExpectingReply = (control & 0x04) != 0
        self.pduNetworkPriority = control & 0x03

        
        if dnetPresent:
            dnet = pdu.get_short()
            dlen = pdu.get()
            dadr = pdu.get_data(dlen)

            if dnet == 0xFFFF:
                self.npduDADR = GlobalBroadcast()
            elif dlen == 0:
                self.npduDADR = RemoteBroadcast(dnet)
            else:
                self.npduDADR = RemoteStation(dnet, dadr)

        
        if snetPresent:
            snet = pdu.get_short()
            slen = pdu.get()
            sadr = pdu.get_data(slen)

            if snet == 0xFFFF:
                raise DecodingError(""SADR cant be a remote broadcast"")

            self.npduSADR = RemoteStation(snet, sadr)

        
        if dnetPresent:
            self.npduHopCount = pdu.get()

        
        if netLayerMessage:
            self.npduNetMessage = pdu.get()
            if (self.npduNetMessage >= 0x80) and (self.npduNetMessage <= 0xFF):
                
                self.npduVendorID = pdu.get_short()
        else:
            
            self.npduNetMessage = None"
181,"def clone(source, destination, dbs = None, verbose = False):
	

	
	if not isinstance(source, dict):
		raise ValueError()

	
	try:
		oSource	= r.connect(**source)

	
	except r.errors.RqlDriverError:
		sys.stderr.write( + str(source) + )
		return False

	
	if not isinstance(destination, dict):
		raise ValueError()

	
	try:
		oDest	= r.connect(**destination)

	
	except r.errors.RqlDriverError:
		sys.stderr.write( + str(destination) + )
		return False

	
	lSourceDBs	= r.db_list().run(oSource)

	
	if not dbs:
		dbs	= lSourceDBs

	
	if isinstance(dbs, (list,tuple)):
		dbs	= {s:None for s in dbs}

	
	for sDB,lTables in dbs.iteritems():

		
		if sDB not in lSourceDBs:
			sys.stderr.write( % sDB)
			continue

		
		if r.db_list().contains(sCopyDB).run(oDest):
			sys.stderr.write( % sDB)
			continue

		
		if verbose:
			sys.stdout.write( % sDB)

		
		r.db_create(sCopyDB).run(oDest)

		
		lSourceTables	= r.db(sDB).table_list().run(oSource)

		
		if not lTables:
			lTables	= lSourceTables

		
		for sTable in lTables:

			
				if len(oMatches) == 1:

					
					r.db(sCopyDB).table(sTable).index_create(oMatches[0][0]).run(oDest)

				
				else:

					
					lFields	= []
					for tMatch in oMatches:
						lFields.append(r.row[tMatch[1]])

					
					r.db(sCopyDB).table(sTable).index_create(sName, lFields).run(oDest)

			
			for dDoc in r.db(sDB).table(sTable).run(oSource):

				
				r.db(sCopyDB).table(sTable).insert(dDoc).run(oDest)

				
				if verbose:

					
					iCount	+= 1

					
					iTemp	= int(round(float(iCount) / fBlock))

					
					if iTemp > iTicks:
						iTicks	= iTemp

						
						sys.stdout.write( % (
							sTable,
							( * iTicks),
							( * (_PROGRESS_TICKS - iTicks)),
							(iTicks * 4)
						))
						sys.stdout.flush()

			
			if verbose:

				
				sys.stdout.write( % (sTable, ( * _PROGRESS_TICKS)))"
182,"def options(name, option, value):
    

    if not isinstance(name, list):
        name = [name]
    
    option = option.lower()
    
    for i in name:
        if i not in data_quants.keys():
            print(str(i) + "" is currently not in pytplot."")
            return
    
        if option == :
            if isinstance(value, list):
                data_quants[i].extras[] = value
            else:
                data_quants[i].extras[] = [value]
        
        if option == :
            if isinstance(value, list):
                data_quants[i].link_to_tvar(value[0], value[1])
                
        if option == :
            if isinstance(value, list):
                data_quants[i].extras[] = value
            else:
                data_quants[i].extras[] = [value]
        
        if option == :
            _reset_plots(i)
            data_quants[i].extras[] = value
        
        if option == :
            _reset_plots(i)
            data_quants[i].extras[] = value
    
        if option == :
            _reset_plots(i)
            data_quants[i].extras[] = value

        if option == :
            data_quants[i].yaxis_opt[] = value

        if option == :
            if value:
                data_quants[i].interactive_xaxis_opt[] = 
            else:
                data_quants[i].interactive_xaxis_opt[] = 

        if option == :
            negflag = _ylog_check(data_quants, value, i)
            if negflag == 0:
                data_quants[i].yaxis_opt[] = 
            else:
                data_quants[i].yaxis_opt[] = 

        if option == :
            if value:
                data_quants[i].interactive_yaxis_opt[] = 
            else:
                data_quants[i].interactive_xaxis_opt[] = 

        if option == :
            negflag = _zlog_check(data_quants, value, i)
            if negflag == 0:
                data_quants[i].zaxis_opt[] = 
            else:
                data_quants[i].zaxis_opt[] = 
        
        if option == :
            data_quants[i].line_opt[] = value
        
        if option == :
            to_be = []
            if value == 0 or value == :
                to_be = []
            elif value == 1 or value == :
                to_be = [2, 4]
            elif value == 2 or value == :
                to_be = [6]
            elif value == 3 or value == :
                to_be = [6, 4, 2, 4]
            elif value == 4 or value == :
                to_be = [6, 4, 2, 4, 2, 4, 2, 4]
            elif value == 5 or value == :
                to_be = [10]
                
            data_quants[i].line_opt[] = to_be
            
            if(value == 6 or value == ):
                data_quants[i].line_opt[] = False

        if option == :
            data_quants[i].extras[] = value
                
        if option == :
            data_quants[i].line_opt[] = value
        
        if option == ""panel_size"":
            if value > 1 or value <= 0:
                print(""Invalid value. Should be (0, 1]"")
                return
            data_quants[i].extras[] = value
        
        if option == :
            data_quants[i].extras[] = value
        
        if option == :
            if value > 1 or value < 0:
                print(""Invalid value. Should be [0, 1]"")
                return
            data_quants[i].extras[] = value
            
        if option == :
            data_quants[i].line_opt[] = value
        
        if option == ( or ):
            data_quants[i].yaxis_opt[] = [value[0], value[1]]
            
        if option == ( or ):
            data_quants[i].zaxis_opt[] = [value[0], value[1]]

        if option == :
            data_quants[i].interactive_xaxis_opt[] = [value[0], value[1]]

        if option == :
            data_quants[i].interactive_yaxis_opt[] = [value[0], value[1]]
            
        if option == :
            data_quants[i].xaxis_opt[] = value
        
        if option == :
            data_quants[i].yaxis_opt[] = value
        
        if option == :
            data_quants[i].zaxis_opt[] = value
        
        if option == :
            _reset_plots(i)
            data_quants[i].extras[] = value

        if option == :
            data_quants[i].xaxis_opt[] = value

        if option == :
            data_quants[i].yaxis_opt[] = value

        if option == :
            data_quants[i].zaxis_opt[] = value

        if option == :
            data_quants[i].extras[] = value

        if option == :
            data_quants[i].extras[] = [value[0], value[1]]

        if option == :
            data_quants[i].extras[] = value
    return"
183,"def two_values_melt(
    df,
    first_value_vars: List[str],
    second_value_vars: List[str],
    var_name: str,
    value_name: str
):
    
    value_name_first = value_name + 
    value_name_second = value_name + 

    
    melt_first_value = pd.melt(df,
                               id_vars=[col for col in list(df) if
                                        col not in first_value_vars],
                               value_vars=first_value_vars,
                               var_name=var_name,
                               value_name=value_name_first)
    melt_first_value.drop(second_value_vars, axis=1, inplace=True)

    
    melt_second_value = pd.melt(df,
                                id_vars=[col for col in list(df) if
                                         col not in second_value_vars],
                                value_vars=second_value_vars,
                                var_name=var_name,
                                value_name=value_name_second)

    
    
    normalize_types = {k: v for k, v in zip(second_value_vars, first_value_vars)}
    melt_second_value.replace(normalize_types, inplace=True)
    melt_second_value.drop(first_value_vars, axis=1, inplace=True)

    on_cols = list(melt_first_value)
    on_cols.remove(value_name_first)
    return pd.merge(melt_first_value, melt_second_value, on=on_cols, how=)"
184,"def info(self, name, description, labelnames=None, labelvalues=None, **labels):
        

        if labels and labelnames:
            raise ValueError(
                
                
            )

        if labelnames is None and labels:
            labelnames = labels.keys()

        elif labelnames and labelvalues:
            for idx, label_name in enumerate(labelnames):
                labels[label_name] = labelvalues[idx]

        gauge = Gauge(
            name, description, labelnames or tuple(),
            registry=self.registry
        )

        if labels:
            gauge = gauge.labels(**labels)

        gauge.set(1)

        return gauge"
185,"def insert(self, state, token):
        
        if token == EndSymbol():
            return self[state][EndSymbol()]
        from pydsl.check import check
        symbol_list = [x for x in self[state] if isinstance(x, TerminalSymbol) and check(x.gd, [token])]
        if not symbol_list:
            return {""action"":""Fail""}
        if len(symbol_list) > 1:
            raise Exception(""Multiple symbols matches input"")
        symbol = symbol_list[0]
        return self[state][symbol]"
186,"def _parse_mdstat(self):
        

        arrays = {}
        mdstat_array_blocks = 

        try:
            with open(self.MDSTAT_PATH, ) as f:
                lines = f.readlines()
        except IOError as err:
            self.log.exception(
                .format(
                    mdstat_path=self.MDSTAT_PATH,
                    err=err
                )
            )
            return arrays

        
        for line in lines[1:-1]:
            mdstat_array_blocks += line

        if mdstat_array_blocks == :
            
            return arrays
        for block in mdstat_array_blocks.split():
            md_device_name = self._parse_device_name(block)
            if md_device_name:
                

                
                arrays[md_device_name] = {
                    : self._parse_array_member_state(block),
                    : self._parse_array_status(block),
                }

                
                bitmap_status = self._parse_array_bitmap(block)
                recovery_status = self._parse_array_recovery(block)
                if bitmap_status:
                    arrays[md_device_name].update(
                        {: bitmap_status}
                    )
                if recovery_status:
                    arrays[md_device_name].update(
                        {: recovery_status}
                    )

        return arrays"
187,"def permanently_delete(self, user):
        
        url = self._build_url(self.endpoint.deleted(id=user))
        deleted_user = self._delete(url)
        self.cache.delete(deleted_user)
        return deleted_user"
188,"def start_kernel(self, **kwargs):
        
        kernel_id = unicode(uuid.uuid4())
        
        km = self.kernel_manager_factory(connection_file=os.path.join(
                    self.connection_dir, ""kernel-%s.json"" % kernel_id),
                    config=self.config,
        )
        km.start_kernel(**kwargs)
        
        km.start_channels(shell=True, sub=False, stdin=False, hb=False)
        self._kernels[kernel_id] = km
        return kernel_id"
189,"def create_github_client(self, project):
        
        token = self._get_installation_key(project=project)
        if not token:
            LOGGER.warning(
                ""Could not find an authentication token for . Do you ""
                ""have access to this repository?"",
                project,
            )
            return
        gh = github3.GitHubEnterprise(self.base_url)
        gh.login(token=token)
        return gh"
190,"def serialize(d):
    
    ret = {}
    for k,v in d.items():
        if not k.startswith():
            ret[k] = str(d[k])
    
    return ret"
191,"def list_private_images(self, guid=None, name=None, **kwargs):
        
        if  not in kwargs:
            kwargs[] = IMAGE_MASK

        _filter = utils.NestedDict(kwargs.get() or {})
        if name:
            _filter[][] = (
                utils.query_filter(name))

        if guid:
            _filter[][] = (
                utils.query_filter(guid))

        kwargs[] = _filter.to_dict()

        account = self.client[]
        return account.getPrivateBlockDeviceTemplateGroups(**kwargs)"
192,"def generate_value_processor(type_, collectionFormat=None, items=None, **kwargs):
    
    processors = []
    if is_non_string_iterable(type_):
        assert False, ""This should not be possible""
    else:
        if type_ == ARRAY and collectionFormat:
            if collectionFormat in DELIMETERS:
                delimeter = DELIMETERS[collectionFormat]
                
                
                processors.append(operator.methodcaller(, delimeter))
            else:
                if collectionFormat != MULTI:
                    raise TypeError(""collectionFormat not implemented"")
                processors.append(add_string_into_list)
            
            processors.append(functools.partial(filter, bool))
            
            processors.append(functools.partial(map, operator.methodcaller()))
            if items is not None:
                if isinstance(items, collections.Mapping):
                    items_processors = itertools.repeat(
                        generate_value_processor(**items)
                    )
                elif isinstance(items, collections.Sequence):
                    items_processors = itertools.chain(
                        (generate_value_processor(**item) for item in items),
                        itertools.repeat(lambda v: v),
                    )
                elif isinstance(items, six.string_types):
                    raise NotImplementedError(""Not implemented"")
                else:
                    assert False, ""Should not be possible""
                
                
                
                processors.append(
                    chain_reduce_partial(
                        functools.partial(zip, items_processors),
                        functools.partial(itertools.starmap, lambda fn, v: fn(v)),
                        list,
                    )
                )
        else:
            processors.append(
                functools.partial(cast_value_to_type, type_=type_)
            )

    def processor(value, **kwargs):
        try:
            return chain_reduce_partial(*processors)(value)
        except (ValueError, TypeError):
            return value

    return processor"
193,"def _get_page_from_path(self, path):
        
        from feincms.module.page.models import Page
        try:
            return Page.objects.best_match_for_path(path)
        except Page.DoesNotExist:
            return None"
194,"def os_script(os_, vm_=None, opts=None, minion=):
    
    if minion:
        minion = salt_config_to_yaml(minion)

    if os.path.isabs(os_):
        
        return __render_script(.format(os_), vm_, opts, minion)

    for search_path in opts[]:
        if os.path.isfile(os.path.join(search_path, os_)):
            return __render_script(
                os.path.join(search_path, os_), vm_, opts, minion
            )

        if os.path.isfile(os.path.join(search_path, .format(os_))):
            return __render_script(
                os.path.join(search_path, .format(os_)),
                vm_, opts, minion
            )
    
    return "
195,"def subclasses(self, inherited=False):
        
        data = clips.data.DataObject(self._env)

        lib.EnvClassSubclasses(self._env, self._cls, data.byref, int(inherited))

        for klass in classes(self._env, data.value):
            yield klass"
196,"def set_key(self, key):
        
        is_non_empty_string(key)

        self.policies.append((, , key))
        self.form_data[] = key
        self.key = key"
197,"def checkProjectAreaID(self, projectarea_id, archived=False):
        

        self.log.debug(""Check the validity of the ProjectArea id: %s"",
                       projectarea_id)

        proj_areas = self._getProjectAreas(archived=archived,
                                           projectarea_id=projectarea_id)
        if proj_areas is not None:
            proj_area = proj_areas[0]
            self.log.info(""Find <ProjectArea %s> whose id is: %s"",
                          proj_area,
                          projectarea_id)
            return True

        self.log.error(""No ProjectArea whose id is: %s"",
                       projectarea_id)
        return False"
198,"def get_top_level_categories(parser, token):
    
    bits = token.split_contents()
    usage =  % bits[0]
    if len(bits) == 3:
        if bits[1] != :
            raise template.TemplateSyntaxError(usage)
        varname = bits[2]
        model = ""categories.category""
    elif len(bits) == 5:
        if bits[1] not in (, ) and bits[3] not in (, ):
            raise template.TemplateSyntaxError(usage)
        if bits[1] == :
            model = bits[2].strip(""\"""")
        else:
            model = bits[4].strip(""\"""")

    return TopLevelCategoriesNode(varname, model)"
199,"def change (properties, feature, value = None):
    
    assert is_iterable_typed(properties, basestring)
    assert isinstance(feature, basestring)
    assert isinstance(value, (basestring, type(None)))
    result = []

    feature = add_grist (feature)

    for p in properties:
        if get_grist (p) == feature:
            if value:
                result.append (replace_grist (value, feature))

        else:
            result.append (p)

    return result"
200,"def encrypt_email(email):
    
    aes = SimpleAES(flask.current_app.config[""AES_KEY""])
    return aes.encrypt(email)"
201,"def run(self):
        
        logger.debug(""Running %s check"", self.name)

        try:
            result = self.perform()
        except Exception:
            logger.exception(""Error while performing %s check"", self.name)
            result = False

        logger.debug(""Result: %s"", result)

        self.results.append(result)
        if self.passing and not any(self.last_n_results(self.fall)):
            logger.info(
                ""%s check failed %d time(s), no longer passing."",
                self.name, self.fall,
            )
            self.passing = False
        if not self.passing and all(self.last_n_results(self.rise)):
            logger.info(
                ""%s check passed %d time(s), is now passing."",
                self.name, self.rise
            )
            self.passing = True"
202,"def on_idle(self, event):
        
        state = self.state
        now = time.time()
        if now - self.last_layout_send > 1:
            self.last_layout_send = now
            state.out_queue.put(win_layout.get_wx_window_layout(self))
        time.sleep(0.1)"
203,"def google_analytics_js(parser, token):
    
    bits = token.split_contents()
    if len(bits) > 1:
        raise TemplateSyntaxError("" takes no arguments"" % bits[0])
    return GoogleAnalyticsJsNode()"
204,"def _find_alphas_param(self):
        

        
        for attr in (""cv_alphas_"", ""alphas_"", ""alphas"",):
            try:
                return getattr(self.estimator, attr)
            except AttributeError:
                continue

        raise YellowbrickValueError(
            ""could not find alphas param on {} estimator"".format(
                self.estimator.__class__.__name__
            )
        )"
205,"def main():
    
    cam = HikCamObject(, 80, , )

    entities = []

    for sensor, channel_list in cam.sensors.items():
        for channel in channel_list:
            entities.append(HikSensor(sensor, channel[1], cam))"
206,"def read_bits(self, num):
        
        if num > len(self._bits):
            needed = num - len(self._bits)
            num_bytes = int(math.ceil(needed / 8.0))
            read_bytes = self._stream.read(num_bytes)

            for bit in bytes_to_bits(read_bytes):
                self._bits.append(bit)

        res = []
        while len(res) < num and len(self._bits) > 0:
            res.append(self._bits.popleft())

        return res"
207,"def ticket_skips(self, ticket_id, **kwargs):
        ""https://developer.zendesk.com/rest_api/docs/core/ticket_skips
        api_path = ""/api/v2/tickets/{ticket_id}/skips.json""
        api_path = api_path.format(ticket_id=ticket_id)
        return self.call(api_path, **kwargs)"
208,"def GetUcsChild(self, inMo=None, inDn=None, classId=None, inHierarchical=False, dumpXml=None):
		

		from UcsBase import UcsValidationException, UcsException, UcsUtils

		if not inDn and not inMo:
			raise UcsValidationException()

		if inMo:
			parentDn = inMo.getattr(""Dn"")
		elif inDn:
			parentDn = inDn

		crc = self.ConfigResolveChildren(classId, parentDn, None, inHierarchical, dumpXml)
		if crc.errorCode == 0:
			moList = UcsUtils.extractMolistFromMethodResponse(crc, inHierarchical)
			return moList
		else:
			raise UcsException(crc.errorCode, crc.error_descr)"
209,"def action_display(self):
        
        action = self[]
        annotations = []
        abbr = self.bill[settings.LEVEL_FIELD]
        if  in self:
            for entity in self[]:
                name = entity[]
                _id = entity[]

                
        return action"
210,"def is_regex(string):
    
    is_regex = False
    regex_chars = [, , , , ]
    for c in regex_chars:
        if string.find(c) > -1:
            return is_valid_regex(string)
    return is_regex"
211,"def fromPy(cls, val, typeObj, vldMask=None):
        
        assert isinstance(typeObj, Integer)
        vld = int(val is not None)
        if not vld:
            assert vldMask is None or vldMask == 0
            val = 0
        else:
            if vldMask == 0:
                val = False
                vld = 0
            else:
                val = int(val)

        return cls(val, typeObj, vld)"
212,"def get_feature(self, feature=None, **kwargs):
        
        if feature is not None:
            kwargs[] = feature
        kwargs[] = 
        return self.filter(**kwargs)"
213,"def combine(self):
        
        self.pair, setup = self.combining_setup()
        self.cube(setup)
        actual = self.combining_search()
        self.cube(actual)
        return setup + actual"
214,"def local_lambda_runner(self):
        

        layer_downloader = LayerDownloader(self._layer_cache_basedir, self.get_cwd())
        image_builder = LambdaImage(layer_downloader,
                                    self._skip_pull_image,
                                    self._force_image_build)

        lambda_runtime = LambdaRuntime(self._container_manager, image_builder)
        return LocalLambdaRunner(local_runtime=lambda_runtime,
                                 function_provider=self._function_provider,
                                 cwd=self.get_cwd(),
                                 env_vars_values=self._env_vars_value,
                                 debug_context=self._debug_context)"
215,"def visit_import(self, node):
        
        for module, as_name in node.names:
            if module in self._logging_modules:
                self._logging_names.add(as_name or module)"
216,"def split(sql, encoding=None):
    
    stack = engine.FilterStack()
    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]"
217,"def teardown_websocket(self, func: Callable) -> Callable:
        
        self.record_once(lambda state: state.app.teardown_websocket(func, self.name))
        return func"
218,"def generate_calculus_integrate_sample(vlist, ops, min_depth, max_depth,
                                       functions):
  
  var_index = random.randrange(len(vlist))
  var = vlist[var_index]
  consts = vlist[:var_index] + vlist[var_index + 1:]

  depth = random.randrange(min_depth, max_depth + 1)
  expr = random_expr_with_required_var(depth, var, consts, ops)

  expr_str = str(expr)
  sample = var + "":"" + expr_str
  target = format_sympy_expr(
      sympy.integrate(expr_str, sympy.Symbol(var)), functions=functions)
  return sample, target"
219,"def _on_prop_changed(self, instance, meth_name, res, args, kwargs):
        
        if  not self._itsme and meth_name == ""__setitem__"": self.update_widget(args[0])
        return"
220,"def p_scalar__indented_flow(self, p):
        
        scalar_group = .join(p[2])
        folded_scalar = fold(dedent(scalar_group))
        p[0] = ScalarDispatch(folded_scalar, cast=)"
221,"def compat_serializer_check_is_valid(serializer):
    
    if DRFVLIST[0] >= 3:
        serializer.is_valid(raise_exception=True)
    else:
        if not serializer.is_valid():
            serializers.ValidationError()"
222,"def object_url(self, object_t, object_id=None, relation=None, **kwargs):
        
        if object_t not in self.objects_types:
            raise TypeError(""{} is not a valid type"".format(object_t))
        request_items = (
            str(item) for item in [object_t, object_id, relation] if item is not None
        )
        request = ""/"".join(request_items)
        base_url = self.url(request)
        if self.access_token is not None:
            kwargs[""access_token""] = str(self.access_token)
        if kwargs:
            for key, value in kwargs.items():
                if not isinstance(value, str):
                    kwargs[key] = str(value)
            
            sorted_kwargs = SortedDict.from_dict(kwargs)
            result = ""{}?{}"".format(base_url, urlencode(sorted_kwargs))
        else:
            result = base_url
        return result"
223,"def _size_from_header(cls, header):
        

        
        result = []

        for data in header:
            

            
            result.append(header[data])

        
        return result"
224,"def getfnc_qual_ev(self):
        
        fnc_key = (
            self.nd_not2desc[(self._keep_nd, self._keep_not)],
            self.incexc2num[(
                self.include_evcodes is not None,
                self.exclude_evcodes is not None)],
        )
        return self.param2fnc[fnc_key]"
225,"def unregister_filter(self, attr_name):
        
        if attr_name not in self._filter_registry:
            raise NotRegistered(
                 % attr_name
            )
        else:
            del self._filter_registry[attr_name]"
226,"def find_results(project_id, **kwargs):
    
    try:
        kwargs[] = project_id
        res = _pybossa_req(, , params=kwargs)
        if type(res).__name__ == :
            return [Result(result) for result in res]
        else:
            return res
    except:  
        raise"
227,"def get_stp_mst_detail_output_cist_port_interface_name(self, **kwargs):
        
        config = ET.Element(""config"")
        get_stp_mst_detail = ET.Element(""get_stp_mst_detail"")
        config = get_stp_mst_detail
        output = ET.SubElement(get_stp_mst_detail, ""output"")
        cist = ET.SubElement(output, ""cist"")
        port = ET.SubElement(cist, ""port"")
        interface_name = ET.SubElement(port, ""interface-name"")
        interface_name.text = kwargs.pop()

        callback = kwargs.pop(, self._callback)
        return callback(config)"
228,"def show():
    
    utils.check_for_cloud_server()
    click.echo(""Using cloud server at \""{}\"""".format(
        config[""cloud_server""][""url""]
    ))
    if config[""cloud_server""][""username""]:
        click.echo(
            ""Logged in as user \""{}\"""".format(config[""cloud_server""][""username""])
        )
    if config[""cloud_server""][""farm_name""]:
        click.echo(
            ""Using farm \""{}\"""".format(config[""cloud_server""][""farm_name""])
        )"
229,"def create_refobj(self, ):
        
        n = cmds.createNode(""jb_reftrack"")
        cmds.lockNode(n, lock=True)
        return n"
230,"def recurse(self, k, wait=False, wait_index=None, timeout=):
        
        k = k.lstrip()
        url = .format(self.endpoint, k)
        params = {}
        params[] = 
        if wait:
            params[] = timeout
            if not wait_index:
                params[] = self.index(k, recursive=True)
            else:
                params[] = wait_index
        r = requests.get(url, params=params)
        if r.status_code == 404:
            raise KeyDoesNotExist(""Key "" + k + "" does not exist"")
        if r.status_code != 200:
            raise KVStoreError(.format(r.status_code))
        entries = {} 
        for e in r.json():
            if e[]:
                entries[e[]] = base64.b64decode(e[])
            else:
                entries[e[]] = 
        return entries"
231,"def main():
    
    from six import StringIO
    import eppy.iddv7 as iddv7
    IDF.setiddname(StringIO(iddv7.iddtxt))
    idf1 = IDF(StringIO())
    loopname = ""p_loop""
    sloop = [, [, , ], ]
    dloop = [, [, , ], ]
    
    loopname = ""c_loop""
    sloop = [, [, , ], ]
    dloop = [, [, , ], ]
    
    loopname = ""a_loop""
    sloop = [, [, , ], ]
    dloop = [, , ]
    makeairloop(idf1, loopname, sloop, dloop)
    idf1.savecopy(""hh1.idf"")"
232,"def type(self, type):
        
        if type is None:
            raise ValueError(""Invalid value for `type`, must not be `None`"")
        allowed_values = [""reservation"", ""reservation_release"", ""reservation_termination"", ""package_renewal"", ""package_creation"", ""package_termination""]
        if type not in allowed_values:
            raise ValueError(
                ""Invalid value for `type` ({0}), must be one of {1}""
                .format(type, allowed_values)
            )

        self._type = type"
233,"def load_match_config(self, match_config: MatchConfig, bot_config_overrides={}):
        
        self.num_participants = match_config.num_players
        self.names = [bot.name for bot in match_config.player_configs]
        self.teams = [bot.team for bot in match_config.player_configs]

        bundles = [bot_config_overrides[index] if index in bot_config_overrides else
                   get_bot_config_bundle(bot.config_path) if bot.config_path else None
                   for index, bot in enumerate(match_config.player_configs)]

        self.python_files = [bundle.python_file if bundle else None
                             for bundle in bundles]

        self.parameters = []

        for index, bot in enumerate(match_config.player_configs):
            python_config = None
            if bot.rlbot_controlled:
                python_config = load_bot_parameters(bundles[index])
            self.parameters.append(python_config)
            if bot.loadout_config is None and bundles[index]:
                looks_config = bundles[index].get_looks_config()
                bot.loadout_config = load_bot_appearance(looks_config, bot.team)

        if match_config.extension_config is not None and match_config.extension_config.python_file_path is not None:
            self.load_extension(match_config.extension_config.python_file_path)

        self.match_config = match_config
        self.start_match_configuration = match_config.create_match_settings()
        self.game_interface.start_match_configuration = self.start_match_configuration"
234,"def import_tasks(self, path):
        
        for zz in os.walk(path):
            for module in zz[2]:

                if module.endswith():

                    import importlib

                    



                    importlib.import_module( + module[:-3])

                    print , module"
235,"def _compute_function_attributes(cfg):
        
        
        attributes = dict()
        all_funcs = set(cfg.kb.callgraph.nodes())
        for function_addr in cfg.kb.functions:
            
            if cfg.kb.functions.function(function_addr) is None or cfg.kb.functions.function(function_addr).is_syscall:
                continue
            if cfg.kb.functions.function(function_addr) is not None:
                normalized_funtion = NormalizedFunction(cfg.kb.functions.function(function_addr))
                number_of_basic_blocks = len(normalized_funtion.graph.nodes())
                number_of_edges = len(normalized_funtion.graph.edges())
            else:
                number_of_basic_blocks = 0
                number_of_edges = 0
            if function_addr in all_funcs:
                number_of_subfunction_calls = len(list(cfg.kb.callgraph.successors(function_addr)))
            else:
                number_of_subfunction_calls = 0
            attributes[function_addr] = (number_of_basic_blocks, number_of_edges, number_of_subfunction_calls)

        return attributes"
236,"def manage_action(self,
                      stat_name,
                      trigger,
                      header,
                      action_key):
        
        
        try:
            command, repeat = self.get_limit_action(trigger, stat_name=stat_name)
        except KeyError:
            
            self.actions.set(stat_name, trigger)
        else:
            
            
            if action_key is None:
                action_key = header

            
            
            if isinstance(self.get_stats_action(), list):
                
                
                mustache_dict = {}
                for item in self.get_stats_action():
                    if item[self.get_key()] == action_key:
                        mustache_dict = item
                        break
            else:
                
                mustache_dict = self.get_stats_action()
            
            self.actions.run(
                stat_name, trigger,
                command, repeat, mustache_dict=mustache_dict)"
237,"def popover_helper(self):
        
        
        display_month = month_name[self.mo]

        if isinstance(display_month, six.binary_type) and self.encoding:
            display_month = display_month.decode()

        self.when = ( + display_month +  +
                     str(self.day) +  + self.event.l_start_date.strftime(
                         LEGACY_CALENDAR_TIME_FORMAT).lstrip() +  +
                     self.event.l_end_date.strftime(LEGACY_CALENDAR_TIME_FORMAT).lstrip() +
                     )

        if self.event.location.exists():  
            self.where = 
            for l in self.event.location.all():
                self.where += l.name
            self.where += 
        else:
            self.where = 

        
        self.desc =  + self.event.description[:100]
        self.desc += ( if len(self.event.description) > 100
                      else )

        self.event_url = self.event.get_absolute_url()  
        t = LEGACY_CALENDAR_TIME_FORMAT if self.event.l_start_date.minute else LEGACY_CALENDAR_HOUR_FORMAT
        self.title2 = (self.event.l_start_date.strftime(t).lstrip() +
                        + self.title)"
238,"def cubehelix_palette(n_colors=6, start=0, rot=.4, gamma=1.0, hue=0.8,
                      light=.85, dark=.15, reverse=False, as_cmap=False):
    
    cdict = mpl._cm.cubehelix(gamma, start, rot, hue)
    cmap = mpl.colors.LinearSegmentedColormap(""cubehelix"", cdict)

    x = np.linspace(light, dark, n_colors)
    pal = cmap(x)[:, :3].tolist()
    if reverse:
        pal = pal[::-1]

    if as_cmap:
        x_256 = np.linspace(light, dark, 256)
        if reverse:
            x_256 = x_256[::-1]
        pal_256 = cmap(x_256)
        cmap = mpl.colors.ListedColormap(pal_256)
        return cmap
    else:
        return pal"
239,"def annotate_bed(
        self,
        bt,
        name,
        col_name,
        complete=None,
        df_col=None,
    ):
        
        import numpy as np
        import pandas as pd
        has_name_col = len(self.annot_beds[name][0].fields) > 3
        print()
        if complete:
            res = bt.intersect(self.annot_beds[name], sorted=True, wo=True, F=1)
        else:
            res = bt.intersect(self.annot_beds[name], sorted=True, wo=True)
        print()
        try:
            df = res.to_dataframe(names=range(len(res[0].fields)))
            ind = df[3].values
            if df_col is None:
                self.df[col_name] = False
                self.df.ix[set(ind), col_name] = True
            else:
                tdf = pd.DataFrame(True, index=ind, columns=[col_name])
                self.df = self.df.merge(tdf, left_on=df_col, right_index=True,
                                        how=)
                self.df[col_name] = self.df[col_name].fillna(False)
                
            print()
            if has_name_col:
                vals = df[7].values
            else:
                vals = list(df[4].astype(str) +  +
                            df[5].astype(str) +  +
                            df[6].astype(str))
            print()
            df.index = vals
            gb = df.groupby(3)
            t = pd.Series(gb.groups)
            print()
            t = pd.DataFrame(t.apply(lambda x: set(x)))
            print()
            t.columns = [.format(col_name)]
            self.df = self.df.merge(t, left_on=df_col, right_index=True,
                                    how=)
            print()
        except IndexError:
            pass"
240,"def _fast_cross_3d(x, y):
    
    assert x.ndim == 2
    assert y.ndim == 2
    assert x.shape[1] == 3
    assert y.shape[1] == 3
    assert (x.shape[0] == 1 or y.shape[0] == 1) or x.shape[0] == y.shape[0]
    if max([x.shape[0], y.shape[0]]) >= 500:
        return np.c_[x[:, 1] * y[:, 2] - x[:, 2] * y[:, 1],
                     x[:, 2] * y[:, 0] - x[:, 0] * y[:, 2],
                     x[:, 0] * y[:, 1] - x[:, 1] * y[:, 0]]
    else:
        return np.cross(x, y)"
241,"def readme_template(readme_template_file):
    
    if not readme_template_file:
        click.secho(dtool_config.utils.get_readme_template_fpath(
            CONFIG_PATH,
        ))
    else:
        click.secho(dtool_config.utils.set_readme_template_fpath(
            CONFIG_PATH,
            readme_template_file
        ))"
242,"def p_annotation_comment_1(self, p):
        
        try:
            if six.PY2:
                value = p[2].decode(encoding=)
            else:
                value = p[2]
            self.builder.add_annotation_comment(self.document, value)
        except CardinalityError:
            self.more_than_one_error(, p.lineno(1))
        except OrderError:
            self.order_error(, , p.lineno(1))"
243,"def get_branch(self, path, project=None, include_parent=None, include_children=None):
        
        route_values = {}
        if project is not None:
            route_values[] = self._serialize.url(, project, )
        query_parameters = {}
        if path is not None:
            query_parameters[] = self._serialize.query(, path, )
        if include_parent is not None:
            query_parameters[] = self._serialize.query(, include_parent, )
        if include_children is not None:
            query_parameters[] = self._serialize.query(, include_children, )
        response = self._send(http_method=,
                              location_id=,
                              version=,
                              route_values=route_values,
                              query_parameters=query_parameters)
        return self._deserialize(, response)"
244,"def run_step(context):
    
    logger.debug(""started"")

    deprecated(context)

    ObjectRewriterStep(__name__, , context).run_step(
        YamlRepresenter())

    logger.debug(""done"")"
245,"def main(self, x, y, phase):
        
        self.initial_step(phase, x, y)

        
        for i in range(self.ITERATIONS - 1):
            if self.MODE == CordicMode.ROTATION:
                direction = self.phase[i] > 0
            elif self.MODE == CordicMode.VECTORING:
                direction = self.y[i] < 0

            if direction:
                self.x[i + 1] = self.x[i] - (self.y[i] >> i)
                self.y[i + 1] = self.y[i] + (self.x[i] >> i)
                self.phase[i + 1] = self.phase[i] - self.PHASE_LUT[i]
            else:
                self.x[i + 1] = self.x[i] + (self.y[i] >> i)
                self.y[i + 1] = self.y[i] - (self.x[i] >> i)
                self.phase[i + 1] = self.phase[i] + self.PHASE_LUT[i]

        return self.x[-1], self.y[-1], self.phase[-1]"
246,"def iter(self, dir_entries):
        
        filter = self.filter
        for entry in dir_entries:
            path = filter(Path2(entry.path))
            if path != False:
                yield path"
247,"def dumps(self):
        
        return {table_name: getattr(self, table_name).dumps() for table_name in self.TABLES}"
248,"def shutdown(self):
        
        self._storage.clear()
        self._scheduler.shutdown(wait=False)
        self._workers.shutdown(wait=False)"
249,"def list_devices(connection: ForestConnection = None):
    
    
    return sorted(get_json(session, url)[""devices""].keys())"
250,"def issubset(self, other):
        
        other = self._cast_to_frameset(other)
        if other is NotImplemented:
            return NotImplemented
        return self.items <= other.items"
251,"def sha(self):
        
        if self._sha is None:
            self._sha = compute_auth_key(self.userid, self.password)
        return self._sha"
252,"def updateStats(self, log):
        
        stdio = log.getText()

        total = passed = skipped = fails = warnings = errors = 0
        hastests = False

        
        
        

        
        

        if not hastests:
            outputs = re.findall(
                ""Ran (?P<count>[\d]+) tests with (?P<fail>[\d]+) failures and (?P<error>[\d]+) errors"",
                stdio)
            for output in outputs:
                total += int(output[0])
                fails += int(output[1])
                errors += int(output[2])
                hastests = True

        

        
        
        
        if not hastests:
            for line in stdio.split(""\n""):
                if line.startswith(""FAILED ("") or line.startswith(""PASSED (""):
                    hastests = True

                    line = line[8:][:-1]
                    stats = line.split("", "")
                    data = {}

                    for stat in stats:
                        k, v = stat.split(""="")
                        data[k] = int(v)

                    if ""successes"" not in data:
                        total = 0
                        for number in re.findall(
                                ""Ran (?P<count>[\d]+) tests in "", stdio):
                            total += int(number)
                        data[""successes""] = total - sum(data.values())

        

        
        
        
        

        if not hastests:
            fails += len(re.findall(, stdio))
            errors += len(
                re.findall(
                    ,
                    stdio))
            for number in re.findall(""Ran (?P<count>[\d]+)"", stdio):
                total += int(number)
                hastests = True

        
        "
253,"def X_length(self, new_window_length):
        
        self.parent.value(, new_window_length)
        self.parent.overview.update_position()"
254,"def get_app_dir(app):
    
    path = __app_dirs__.get(app)
    if path is not None:
        return path
    else:
        p = app.split()
        try:
            path = pkg.resource_filename(p[0], )
        except ImportError as e:
            log.error(""Can'
        if len(p) > 1:
            path = os.path.join(path, *p[1:])
        
        __app_dirs__[app] = path
        return path"
255,"def authorization_target(original_class):
    

    def can(self, action, subject):
        ability = Ability(self, get_authorization_method())
        return ability.can(action, subject)

    def cannot(self, action, subject):
        return not can(self, action, subject)

    setattr(original_class, , can)
    setattr(original_class, , cannot)

    return original_class"
256,"def eliminate_diag_dom_nodes(A, C, theta=1.02):
    r
    
    A_abs = A.copy()
    A_abs.data = np.abs(A_abs.data)
    D_abs = get_diagonal(A_abs, norm_eq=0, inv=False)
    diag_dom_rows = (D_abs > (theta*(A_abs*np.ones((A_abs.shape[0],),
                                                   dtype=A_abs) - D_abs)))

    
    bsize = blocksize(A_abs)
    if bsize > 1:
        diag_dom_rows = np.array(diag_dom_rows, dtype=int)
        diag_dom_rows = diag_dom_rows.reshape(-1, bsize)
        diag_dom_rows = np.sum(diag_dom_rows, axis=1)
        diag_dom_rows = (diag_dom_rows == bsize)

    
    Id = eye(C.shape[0], C.shape[1], format=)
    Id.data[diag_dom_rows] = 0.0
    C = Id * C * Id
    Id.data[diag_dom_rows] = 1.0
    Id.data[np.where(diag_dom_rows == 0)[0]] = 0.0
    C = C + Id

    del A_abs
    return C"
257,"def perform_matched_selection(self, event):
        
        selected = TextHelper(self.editor).match_select()
        if selected and event:
            event.accept()"
258,"def get_matching_gos(self, compiled_pattern, **kws):
        
        
        matching_gos = []
        obo_dag = self.obo_dag
        prt = kws[] if  in kws else self.log
        prt.write(.format(P=compiled_pattern.pattern))
        
        srchgos = kws[] if  in kws else self.go2items.keys()
        for go_id in srchgos:
            go_obj = obo_dag.get(go_id, None)
            if go_obj is not None:
                for hdr in self.goa_srch_hdrs:
                    if hdr in go_obj.__dict__:
                        fld_val = getattr(go_obj, hdr)
                        matches = self._search_vals(compiled_pattern, fld_val)
                        for mtch in matches:
                            prt.write(""MATCH {go_id}({NAME}) {FLD}: {M}\n"".format(
                                FLD=hdr, go_id=go_obj.id, NAME=go_obj.name, M=mtch))
                        if matches:
                            matching_gos.append(go_id)
            else:
                prt.write(""**WARNING: {GO} found in annotation is not found in obo\n"".format(
                    GO=go_id))
        matching_gos = set(matching_gos)
        
        self._summary_matching_gos(prt, compiled_pattern.pattern, matching_gos, srchgos)
        return matching_gos"
259,"def DbGetObjectList(self, argin):
        
        self._log.debug(""In DbGetObjectList()"")
        argin = replace_wildcard(argin)
        return self.db.get_object_list(argin)"
260,"def get_running_step_changes(write: bool = False) -> list:
    
    project = cd.project.get_internal_project()

    running_steps = list(filter(
        lambda step: step.is_running,
        project.steps
    ))

    def get_changes(step):
        step_data = writing.step_writer.serialize(step)

        if write:
            writing.save(project, step_data.file_writes)

        return dict(
            name=step.definition.name,
            action=,
            step=step_data._asdict(),
            written=write
        )

    return [get_changes(step) for step in running_steps]"
261,"def url_paths(self):
        
        unformatted_paths = self._url_module.url_paths

        paths = {}
        for unformatted_path, handler in unformatted_paths.items():
            path = unformatted_path.format("""")
            paths[path] = handler

        return paths"
262,"def _send_and_reconnect(self, message):
        
        try:
            self.socket.sendall(message.encode(""ascii""))
        except (AttributeError, socket.error):
            if not self.autoreconnect():
                raise
            else:
                self.socket.sendall(message.encode(""ascii""))"
263,"def _reshape_like(F, x, y):
    
    return x.reshape(y.shape) if F is ndarray else F.reshape_like(x, y)"
264,"def load(url_or_handle, cache=None, **kwargs):
    

    ext = get_extension(url_or_handle)
    try:
        loader = loaders[ext.lower()]
        message = ""Using inferred loader  due to passed file extension .""
        log.debug(message, loader.__name__[6:], ext)
        return load_using_loader(url_or_handle, loader, cache, **kwargs)

    except KeyError:

        log.warning(""Unknown extension , attempting to load as image."", ext)
        try:
            with read_handle(url_or_handle, cache=cache) as handle:
                result = _load_img(handle)
        except Exception as e:
            message = ""Could not load resource %s as image. Supported extensions: %s""
            log.error(message, url_or_handle, list(loaders))
            raise RuntimeError(message.format(url_or_handle, list(loaders)))
        else:
            log.info(""Unknown extension  successfully loaded as image."", ext)
            return result"
265,"def shares(self, token: dict = None, prot: str = ""https"") -> dict:
        
        
        shares_url = ""{}://v1.{}.isogeo.com/shares/"".format(prot, self.api_url)
        shares_req = self.get(
            shares_url, headers=self.header, proxies=self.proxies, verify=self.ssl
        )

        
        checker.check_api_response(shares_req)

        
        return shares_req.json()"
266,"def get_account_balance(address, token_type, hostport=None, proxy=None):
    
    assert proxy or hostport, 
    if proxy is None:
        proxy = connect_hostport(hostport)

    balance_schema = {
        : ,
        : {
            : {
                : ,
            },
        },
        : [
            ,
        ],
    }

    schema = json_response_schema(balance_schema)

    try:
        resp = proxy.get_account_balance(address, token_type)
        resp = json_validate(schema, resp)
        if json_is_error(resp):
            return resp

    except ValidationError as e:
        if BLOCKSTACK_DEBUG:
            log.exception(e)

        resp = {: , : 502}
        return resp

    except socket.timeout:
        log.error(""Connection timed out"")
        resp = {: , : 503}
        return resp

    except socket.error as se:
        log.error(""Connection error {}"".format(se.errno))
        resp = {: , : 502}
        return resp

    except Exception as ee:
        if BLOCKSTACK_DEBUG:
            log.exception(ee)

        log.error(""Caught exception while connecting to Blockstack node: {}"".format(ee))
        resp = {: , : 500}
        return resp

    return resp[]"
267,"def clean(self, value):
        

        if isinstance(value, (list, tuple)):
            return [super(FeatureCollectionParameter, self).clean(x) for x in value]

        raise ParameterNotValidError"
268,"def has(self, character):
        
        if not self:
            return False
        character = character if isinstance(character, int) else ord(character)
        last = self[-1][-1]
        start, end = self[bisect.bisect_right(self, (character, last)) - 1]
        return start <= character < end"
269,"def v(*args, **kwargs):
    
    if not args:
        raise ValueError(""you didn't pass any arguments to print out"")

    with Reflect.context(args, **kwargs) as r:
        instance = V_CLASS(r, stream, **kwargs)
        instance()"
270,"def start_paragraph(self, stylename=None):
        
        
        if stylename is None:
            stylename = self._next_p_style or 
        self.start_container(P, stylename=stylename)"
271,"def compute_colors_for_labels(self, labels):
        
        colors = labels[:, None] * self.palette
        colors = (colors % 255).numpy().astype(""uint8"")
        return colors"
272,"def _make_assets_key_collection(saved_model_proto, export_path):
  
  asset_filenames = {}
  used_asset_filenames = set()

  def _make_asset_filename(original_filename):
    
    if original_filename in asset_filenames:
      return asset_filenames[original_filename]

    basename = os.path.basename(original_filename)
    suggestion = basename
    index = 0
    while suggestion in used_asset_filenames:
      suggestion = ""%s%d"" % (basename, index)
      index += 1
    asset_filenames[original_filename] = suggestion
    used_asset_filenames.add(suggestion)
    return suggestion

  for meta_graph in saved_model_proto.meta_graphs:
    collection_def = meta_graph.collection_def.get(
        tf_v1.GraphKeys.ASSET_FILEPATHS)

    if collection_def is None:
      continue
    if collection_def.WhichOneof(""kind"") != ""node_list"":
      raise ValueError(
          ""MetaGraph collection ASSET_FILEPATHS is not a list of tensors."")

    for tensor in collection_def.node_list.value:
      if not tensor.endswith("":0""):
        raise ValueError(""Unexpected tensor in ASSET_FILEPATHS collection."")

    asset_nodes = set([
        _get_node_name_from_tensor(tensor)
        for tensor in collection_def.node_list.value
    ])

    tensor_filename_map = {}
    for node in meta_graph.graph_def.node:
      if node.name in asset_nodes:
        _check_asset_node_def(node)
        filename = node.attr[""value""].tensor.string_val[0]
        tensor_filename_map[node.name + "":0""] = filename
        
        node.attr[""value""].tensor.string_val[0] = (
            tf.compat.as_bytes(""SAVEDMODEL-ASSET""))

    if tensor_filename_map:
      assets_key_collection = meta_graph.collection_def[
          tf_v1.saved_model.constants.ASSETS_KEY]

      for tensor, filename in sorted(tensor_filename_map.items()):
        asset_proto = meta_graph_pb2.AssetFileDef()
        asset_proto.filename = _make_asset_filename(filename)
        asset_proto.tensor_info.name = tensor
        assets_key_collection.any_list.value.add().Pack(asset_proto)

  return {
      original_filename: _get_asset_filename(export_path, asset_filename)
      for original_filename, asset_filename in asset_filenames.items()
  }"
273,"def set_or_create_content(self, page, language, ctype, body):
        
        try:
            content = self.filter(page=page, language=language,
                                  type=ctype).latest()
            content.body = body
        except self.model.DoesNotExist:
            content = self.model(page=page, language=language, body=body,
                                 type=ctype)
        content.save()
        return content"
274,"def find(file_node, dirs=ICON_DIRS, default_name=None, file_ext=):
        
        names = []
        for attr_name in (, , ):
            attr = getattr(file_node, attr_name)
            if attr:
                names.append(attr)
        if default_name:
            names.append(default_name)
        icon_path = StaticPathFinder.find(names, dirs, file_ext)
        if icon_path:
            return StaticIconFile(file_node, icon_path)"
275,"def _move_template_to_destination(
            self,
            ignoreExisting=False):
        
        self.log.info()

        
        sourceDirectories = recursive_directory_listing(
            log=self.log,
            baseFolderPath=self.tmpPath,
            whatToList=""dirs""  
        )

        destinationDirectories = []
        destinationDirectories = []
        destinationDirectories[:] = [self.pathToDestination +
                                     d.replace(self.tmpPath, """") for d in sourceDirectories]
        for d in destinationDirectories:
            
            if not os.path.exists(d):
                os.makedirs(d)

        
        sourceFiles = recursive_directory_listing(
            log=self.log,
            baseFolderPath=self.tmpPath,
            whatToList=""files""  
        )
        destinationFiles = []
        destinationFiles = []
        destinationFiles[:] = [self.pathToDestination +
                               d.replace(self.tmpPath, """") for d in sourceFiles]

        appendText = """"
        for s, f in zip(sourceFiles, destinationFiles):
            try:
                readFile = codecs.open(f, encoding=, mode=)
                content = readFile.read()
                readFile.close()
                fileExists = True
            except IOError:
                fileExists = False
            if fileExists == True and len(content) > 1 and ignoreExisting == False:
                readFile = codecs.open(s, encoding=, mode=)
                content = readFile.read()
                readFile.close()
                appendText +=  % locals()
            else:
                try:
                    if ignoreExisting == False or (ignoreExisting == True and fileExists == False):
                        self.log.debug(""attempting to rename file %s to %s"" %
                                       (s, f))
                        shutil.move(s, f)
                    else:
                        pass
                except Exception, e:
                    self.log.error(""could not rename file %s to %s - failed with this error: %s "" %
                                   (s, f, str(e),))
                    sys.exit(0)

        
        if len(appendText) > 3:
            appendText =  % locals()
            writeFile = codecs.open(
                ""/tmp/append.md"", encoding=, mode=)
            writeFile.write(appendText)
            writeFile.close()
            try:
                cmd =  % locals()
                p = Popen(cmd, stdout=PIPE, stdin=PIPE, shell=True)
                output = p.communicate()[0]
                self.log.debug( % locals())
            except:
                pass

        
        shutil.rmtree(self.tmpPath)

        self.log.info()
        return None"
276,"def fixations(self):
        
        if not self._fixations:
            raise RuntimeError(
                +)
        return self._fixations[(self._fixations.category == self.category) &
                               (self._fixations.filenumber == self.image)]"
277,"def find_deps(obj,              
              document_loader,  
              uri,              
              basedir=None,     
              nestdirs=True     
             ):  
    
    deps = {""class"": ""File"", ""location"": uri, ""format"": CWL_IANA}  

    def loadref(base, uri):
        return document_loader.fetch(document_loader.fetcher.urljoin(base, uri))

    sfs = scandeps(
        basedir if basedir else uri, obj, {""$import"", ""run""},
        {""$include"", ""$schemas"", ""location""}, loadref, nestdirs=nestdirs)
    if sfs is not None:
        deps[""secondaryFiles""] = sfs

    return deps"
278,"def factor_hatch(field_name, patterns, factors, start=0, end=None):
    
    return field(field_name, CategoricalPatternMapper(patterns=patterns,
                                                      factors=factors,
                                                      start=start,
                                                      end=end))"
279,"def _insertSegment(self, index=None, type=None, points=None,
                       smooth=False, **kwargs):
        
        onCurve = points[-1]
        offCurve = points[:-1]
        segments = self.segments
        ptCount = sum([len(segments[s].points) for s in range(index)]) + 1
        self.insertPoint(ptCount, onCurve, type=type, smooth=smooth)
        for offCurvePoint in reversed(offCurve):
            self.insertPoint(ptCount, offCurvePoint, type=""offcurve"")"
280,"def t_VAR(self, t):
        r
        t.type = self.reserved.get(t.value.lower(), )
        return t"
281,"def wait_for_ribcl_firmware_update_to_complete(ribcl_object):
    

    def is_ilo_reset_initiated():
        
        try:
            LOG.debug(ribcl_object._())
            ribcl_object.get_product_name()
            return False
        except exception.IloError:
            LOG.debug(ribcl_object._())
            return True

    
    
    
    
    wait_for_operation_to_complete(
        is_ilo_reset_initiated,
        delay_bw_retries=6,
        delay_before_attempts=5,
        is_silent_loop_exit=True
    )
    wait_for_ilo_after_reset(ribcl_object)"
282,"def paginated_retrieval(methodname, itemtype):
    
    return compose(
        reusable,
        basic_interaction,
        map_yield(partial(_params_as_get, methodname)),
    )"
283,"def process_full_position(data, header, var_only=False):
    
    feature_type = data[header[]]
    
    if (feature_type ==  or feature_type.startswith()):
        return None
    if var_only and feature_type in [, ]:
        return None

    filters = []
    if feature_type == :
        filters.append()
    if  in header:
        if  in data[header[]]:
            filters.append()
    else:
        var_filter = data[header[]]
        if var_filter and not var_filter == ""PASS"":
            filters = filters + var_filter.split()

    chrom = data[header[]]
    start = data[header[]]
    ref_allele = data[header[]]
    alleles = [data[header[]]]
    dbsnp_data = []
    dbsnp_data = data[header[]].split()
    assert data[header[]] in [, ]
    if feature_type ==  or feature_type == :
        return [{: chrom,
                 : start,
                 : dbsnp_data,
                 : ref_allele,
                 : alleles,
                 : data[header[]],
                 : filters,
                 : data[header[]]}]
    else:
        return [{: chrom,
                 : start,
                 : dbsnp_data,
                 : ref_allele,
                 : alleles,
                 : data[header[]],
                 : filters}]"
284,"def from_gitlab(klass, repository, labor_hours=True):
        
        if not isinstance(repository, gitlab.v4.objects.Project):
            raise TypeError()

        project = klass()

        logger.debug(
            ,
            repository.id,
            repository.path_with_namespace,
        )

        

        project[] = repository.name
        project[] = repository.http_url_to_repo
        project[] = repository.description

        
        project[][] = None

        web_url = repository.web_url
        public_server = web_url.startswith()

        if repository.visibility in () and public_server:
            project[][] = 
        elif date_parse(repository.created_at) < POLICY_START_DATE:
            project[][] = 

        if labor_hours:
            project[] = labor_hours_from_url(project[])
        else:
            project[] = 0

        project[] = [] + repository.tag_list

        project[] = {
            : ,
            : web_url,
        }

        

        

        project[] = repository.namespace[]

        
        }

        _prune_dict_null_str(project)

        return project"
285,"def get(self, *args, **kwargs):
        
        step_url = kwargs.get(, None)
        if step_url is None:
            if  in self.request.GET:
                self.storage.reset()
                self.storage.current_step = self.steps.first
            if self.request.GET:
                query_string = ""?%s"" % self.request.GET.urlencode()
            else:
                query_string = """"
            next_step_url = reverse(self.url_name, kwargs={
                : self.steps.current,
            }) + query_string
            return redirect(next_step_url)

        
        elif step_url == self.done_step_name:
            last_step = self.steps.last
            return self.render_done(self.get_form(step=last_step,
                data=self.storage.get_step_data(last_step),
                files=self.storage.get_step_files(last_step)
            ), **kwargs)

        
        
        elif step_url == self.steps.current:
            
            return self.render(self.get_form(
                data=self.storage.current_step_data,
                files=self.storage.current_step_data,
            ), **kwargs)

        elif step_url in self.get_form_list():
            self.storage.current_step = step_url
            return self.render(self.get_form(
                data=self.storage.current_step_data,
                files=self.storage.current_step_data,
            ), **kwargs)

        
        else:
            self.storage.current_step = self.steps.first
            return redirect(self.url_name, step=self.steps.first)"
286,"def send_image_url(self, recipient_id, image_url, notification_type=NotificationType.regular):
        
        return self.send_attachment_url(recipient_id, ""image"", image_url, notification_type)"
287,"def dump(self, *args, **kwargs):
        
        return self.each(Q.dump(*args, **kwargs))"
288,"def _iter_flattened_categories(self):
        
        levels = self.levels
        if not levels:
            return
        leaf_level, remaining_levels = levels[0], levels[1:]
        for category in leaf_level:
            yield self._parentage((category,), remaining_levels)"
289,"def _fix_contig_orientation(contigs_fa, ref_fa, outfile, min_id=90, min_length=20, breaklen=200):
        
        if not os.path.exists(contigs_fa):
            raise Error( + contigs_fa)

        tmp_coords = os.path.join(outfile + )
        pymummer.nucmer.Runner(
            ref_fa,
            contigs_fa,
            tmp_coords,
            min_id=min_id,
            min_length=min_length,
            breaklen=breaklen,
            maxmatch=True,
        ).run()

        to_revcomp = set()
        not_revcomp = set()
        file_reader = pymummer.coords_file.reader(tmp_coords)
        for hit in file_reader:
            if hit.on_same_strand():
                not_revcomp.add(hit.qry_name)
            else:
                to_revcomp.add(hit.qry_name)

        os.unlink(tmp_coords)
        in_both = to_revcomp.intersection(not_revcomp)

        f = pyfastaq.utils.open_file_write(outfile)
        seq_reader = pyfastaq.sequences.file_reader(contigs_fa)
        for seq in seq_reader:
            if seq.id in to_revcomp and seq.id not in in_both:
                seq.revcomp()
            print(seq, file=f)
        pyfastaq.utils.close(f)

        return in_both"
290,"def get_angles(self) -> Tuple[List[float], List[float]]:
        
        stacked_params = np.hstack((self.betas, self.gammas))
        vqe = VQE(self.minimizer, minimizer_args=self.minimizer_args,
                  minimizer_kwargs=self.minimizer_kwargs)
        cost_ham = reduce(lambda x, y: x + y, self.cost_ham)
        
        param_prog = self.get_parameterized_program()
        result = vqe.vqe_run(param_prog, cost_ham, stacked_params, qc=self.qc,
                             **self.vqe_options)
        self.result = result
        betas = result.x[:self.steps]
        gammas = result.x[self.steps:]
        return betas, gammas"
291,"def com_google_fonts_check_aat(ttFont):
  
  UNWANTED_TABLES = {
    , , , , , , ,
    , , , , , , ,
    , , , , , , ,
    , 
  }
  unwanted_tables_found = []
  for table in ttFont.keys():
    if table in UNWANTED_TABLES:
      unwanted_tables_found.append(table)

  if len(unwanted_tables_found) > 0:
    yield FAIL, (""Unwanted AAT tables were found""
                 "" in the font and should be removed, either by""
                 "" fonttools/ttx or by editing them using the tool""
                 "" they built with:""
                 "" {}"").format("", "".join(unwanted_tables_found))
  else:
    yield PASS, ""There are no unwanted AAT tables."""
292,"def _set_mpls_adjust_bandwidth_lsp(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=mpls_adjust_bandwidth_lsp.mpls_adjust_bandwidth_lsp, is_leaf=True, yang_name=""mpls-adjust-bandwidth-lsp"", rest_name=""mpls-adjust-bandwidth-lsp"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=False, extensions={u: {u: u, u: u}}, namespace=, defining_module=, yang_type=, is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          : ,
          : ""rpc"",
          : ,
        })

    self.__mpls_adjust_bandwidth_lsp = t
    if hasattr(self, ):
      self._set()"
293,"def init(self, force_deploy=False):
        
        machines = self.provider_conf.machines
        networks = self.provider_conf.networks
        _networks = []
        for network in networks:
            ipnet = IPNetwork(network.cidr)
            _networks.append({
                ""netpool"": list(ipnet)[10:-10],
                ""cidr"": network.cidr,
                ""roles"": network.roles,
                ""gateway"": ipnet.ip
            })

        vagrant_machines = []
        vagrant_roles = {}
        j = 0
        for machine in machines:
            for _ in range(machine.number):
                vagrant_machine = {
                    ""name"": ""enos-%s"" % j,
                    ""cpu"": machine.flavour_desc[""core""],
                    ""mem"": machine.flavour_desc[""mem""],
                    ""ips"": [n[""netpool""].pop() for n in _networks],
                }
                vagrant_machines.append(vagrant_machine)
                
                for role in machine.roles:
                    vagrant_roles.setdefault(role, []).append(vagrant_machine)
                j = j + 1

        logger.debug(vagrant_roles)

        loader = FileSystemLoader(searchpath=TEMPLATE_DIR)
        env = Environment(loader=loader, autoescape=True)
        template = env.get_template()
        vagrantfile = template.render(machines=vagrant_machines,
                                      provider_conf=self.provider_conf)
        vagrantfile_path = os.path.join(os.getcwd(), ""Vagrantfile"")
        with open(vagrantfile_path, ) as f:
            f.write(vagrantfile)

        
        
        v_env = dict(os.environ)
        v_env[] = self.provider_conf.backend

        v = vagrant.Vagrant(root=os.getcwd(),
                            quiet_stdout=False,
                            quiet_stderr=False,
                            env=v_env)
        if force_deploy:
            v.destroy()
        v.up()
        v.provision()
        roles = {}
        for role, machines in vagrant_roles.items():
            for machine in machines:
                keyfile = v.keyfile(vm_name=machine[])
                port = v.port(vm_name=machine[])
                address = v.hostname(vm_name=machine[])
                roles.setdefault(role, []).append(
                    Host(address,
                         alias=machine[],
                         user=self.provider_conf.user,
                         port=port,
                         keyfile=keyfile))

        networks = [{
            : str(n[""cidr""]),
            : str(n[""netpool""][0]),
            : str(n[""netpool""][-1]),
            : ,
            : str(n[""gateway""]),
            : n[""roles""]
            } for n in _networks]
        logger.debug(roles)
        logger.debug(networks)

        return (roles, networks)"
294,"def get_deployment_count(self):
        

        ret = self.stage_set.annotate(num_deployments=Count()).aggregate(total_deployments=Sum())
        return ret[]"
295,"def update(self, *pkgs, **kwargs):
        
        cmd_list = [, , ]

        if not pkgs and not kwargs.get():
            raise TypeError(""Must specify at least one package to update, or ""
                            ""all=True."")

        cmd_list.extend(
            self._setup_install_commands_from_kwargs(
                kwargs,
                (, , ,
                 , , , , ,
                 )))

        cmd_list.extend(pkgs)

        return self._call_and_parse(cmd_list, abspath=kwargs.get(,
                                                                 True))"
296,"def dlogpdf_dlink_dr(self, inv_link_f, y, Y_metadata=None):
        
        
        
        c = np.zeros_like(y)
        if Y_metadata is not None and  in Y_metadata.keys():
            c = Y_metadata[]

        link_f = inv_link_f
        
        
        uncensored = (1-c)*(y**self.r*np.log(y)/link_f**2)
        censored = c*(y**self.r*np.log(y)/link_f**2)
        dlogpdf_dlink_dr = uncensored + censored
        return dlogpdf_dlink_dr"
297,"def simplegeneric(func):
    
    registry = {}
    def wrapper(*args, **kw):
        ob = args[0]
        try:
            cls = ob.__class__
        except AttributeError:
            cls = type(ob)
        try:
            mro = cls.__mro__
        except AttributeError:
            try:
                class cls(cls, object):
                    pass
                mro = cls.__mro__[1:]
            except TypeError:
                mro = object,   
        for t in mro:
            if t in registry:
                return registry[t](*args, **kw)
        else:
            return func(*args, **kw)
    try:
        wrapper.__name__ = func.__name__
    except (TypeError, AttributeError):
        pass    

    def register(typ, func=None):
        if func is None:
            return lambda f: register(typ, f)
        registry[typ] = func
        return func

    wrapper.__dict__ = func.__dict__
    wrapper.__doc__ = func.__doc__
    wrapper.register = register
    return wrapper"
298,"def generate_tile_coordinates(roi, num_tiles):
    
    
    bounds = roi.get_shape(roi.crs).bounds

    x_range = np.linspace(bounds[0], bounds[2], int(num_tiles[0]) + 1)
    y_range = np.linspace(bounds[1], bounds[3], int(num_tiles[1]) + 1)

    for y_start, y_end in zip(y_range[:-1], y_range[1:]):
        for x_start, x_end in zip(x_range[:-1], x_range[1:]):
            new_roi = GeoVector(
                Polygon.from_bounds(x_start, y_start, x_end, y_end),
                roi.crs
            )

            yield new_roi"
299,"def elliptical_annular(cls, shape, pixel_scale,inner_major_axis_radius_arcsec, inner_axis_ratio, inner_phi,
                           outer_major_axis_radius_arcsec, outer_axis_ratio, outer_phi, centre=(0.0, 0.0),
                           invert=False):
        
        mask = mask_util.mask_elliptical_annular_from_shape_pixel_scale_and_radius(shape, pixel_scale,
                           inner_major_axis_radius_arcsec, inner_axis_ratio, inner_phi,
                           outer_major_axis_radius_arcsec, outer_axis_ratio, outer_phi, centre)
        if invert: mask = np.invert(mask)
        return cls(array=mask.astype(), pixel_scale=pixel_scale)"
300,"def serialize_to_rsb(params: dict) -> bytes:
    
    header = bytearray(np.zeros(2048, np.byte).tostring())

    if ""text_header_size"" in params:
        header[0:4] = struct.pack(, params[""text_header_size""])

    if ""events_num"" in params:
        header[8:12] = struct.pack(, params[""events_num""])

    if ""start_time"" in params:
        start_time = dateutil.parser.parse(params[""start_time""]).timestamp()
        header[16:24] = struct.pack(, int(start_time))

    if ""end_time"" in params:
        end_time = dateutil.parser.parse(params[""end_time""]).timestamp()
        header[24:32] = struct.pack(, int(end_time))

    header[32:32 + len(params[""filepath""])
           ] = params[].encode()

    header[288:292] = struct.pack(, params[""num_blocks""])

    header[292:296] = struct.pack(, int(params[""aquisition_time""]))

    header[296:300] = struct.pack(, params[""blocks_in_file""])

    header[300:304] = struct.pack(, int(params[""waitTime""]))

    header[312:320] = struct.pack(, params[""threshold""])

    sync_params = params[""synchro_control""]
    sync_params_num = len(sync_params)

    header[336:340] = struct.pack(, sync_params_num)

    for i in range(sync_params_num):
        if sync_params[i] == :
            code = 0
        else:
            code = synchro_control[sync_params[i]]

        header[320 + i * 4:320 + (i + 1) * 4] = struct.pack(, code)

    header[344:352] = struct.pack(, params[""sample_freq""])
    header[352:356] = struct.pack(, params[""pre_history""])

    header[356:360] = struct.pack(, params[""packet_number""])

    header[360:364] = struct.pack(, params[""b_size""])

    header[364:368] = struct.pack(, params[""hysteresis""])

    header[368:372] = struct.pack(, params[""channel_number""])

    for i in range(params[""channel_number""]):
        off = 372 + 56 * i

        ch_param = params[][i]

        header[off + 44: off + 52] = struct.pack(, ch_param[""adjustment""])
        header[off + 52: off + 56] = struct.pack(, ch_param[""gain""])
        header[off + 36: off + 40] = struct.pack(, len(ch_param[]))

        for j, param in enumerate(ch_param[]):
            if param == :
                code = 0
            else:
                code = channel_control[param]
                header[off + 4 + j * 4:
                       off + 4 + (j + 1) * 4] = struct.pack(, code)

    synchro_channel = params[]
    header[632:636] = struct.pack(, len(synchro_channel[]))

    for i, param in enumerate(synchro_channel[]):
        if param == :
            code = 0
        else:
            code = synchro_channel_control[param]
            header[600 + i * 4: 600 + (i + 1) * 4] = struct.pack(, code)

    sync_type = synchro_channel_types[synchro_channel[]]
    header[304:308] = struct.pack(, sync_type)

    header[636:640] = struct.pack(, synchro_channel[""gain""])

    if ""err_lang"" in params:
        header[640:644] = struct.pack(, params[""err_lang""])

    if ""board_name"" in params:
        header[644:644 + len(params[""board_name""])] = \
            params[].encode()

    if ""board_id"" in params:
        header[900: 904] = struct.pack(, params[""board_id""])

    return bytes(header)"
301,"def build_inbound_messages(self, break_on_empty=False, to_tuple=False,
                               auto_decode=True):
        
        self.check_for_errors()
        while not self.is_closed:
            message = self._build_message(auto_decode=auto_decode)
            if not message:
                self.check_for_errors()
                sleep(IDLE_WAIT)
                if break_on_empty and not self._inbound:
                    break
                continue
            if to_tuple:
                yield message.to_tuple()
                continue
            yield message"
302,"def load(self, host, exact_host_match=False):
        

        
        config_string, host_string = ftr_get_config(host, exact_host_match)

        if config_string is None:
            LOGGER.error(u,
                         extra={: host_string})
            return

        self.append(ftr_string_to_instance(config_string))"
303,"def typeOf(cls, expected_type): 
        
        if isinstance(expected_type, type):
            options = {}
            options[""target_type""] = expected_type
            return Matcher(""__TYPE__"", options)
        ErrorHandler.matcher_type_error(expected_type)"
304,"def extract_endpoint_arguments(endpoint):
    

    ep_args = endpoint._arguments
    if ep_args is None:
        return None

    arg_docs = { k: format_endpoint_argument_doc(a) \
            for k, a in ep_args.iteritems() }
    return arg_docs"
305,"def is_descendant_of(self, node):
        
        return self.pk in [obj.pk for obj in node.get_descendants()]"
306,"def add_page(self, page, default = True):
        r
        self._pages[page.page_id] = page
        page._add_display(self)
        if default or not self._active_page:
            self._active_page = page"
307,"def listen_forever(
            self,
            timeout_ms: int = 30000,
            exception_handler: Callable[[Exception], None] = None,
            bad_sync_timeout: int = 5,
    ):
        
        _bad_sync_timeout = bad_sync_timeout
        self.should_listen = True
        while self.should_listen:
            try:
                
                self._sync(timeout_ms)
                _bad_sync_timeout = bad_sync_timeout
            except MatrixRequestError as e:
                log.warning()
                if e.code >= 500:
                    log.warning(
                        ,
                        wait_for=_bad_sync_timeout,
                    )
                    gevent.sleep(_bad_sync_timeout)
                    _bad_sync_timeout = min(_bad_sync_timeout * 2, self.bad_sync_timeout_limit)
                else:
                    raise
            except MatrixHttpLibError:
                log.exception()
                if self.should_listen:
                    gevent.sleep(_bad_sync_timeout)
                    _bad_sync_timeout = min(_bad_sync_timeout * 2, self.bad_sync_timeout_limit)
            except Exception as e:
                log.exception()
                if exception_handler is not None:
                    exception_handler(e)
                else:
                    raise"
308,"def cloud_of_words(path_to_bog, cluster_no, image_path=None):
        
        dictionary = GenometricSpace.best_descriptive_meta_dict(path_to_bog, cluster_no)
        GenometricSpace.visualize_cloud_of_words(dictionary, image_path)"
309,"def get_partitions_ps_with_auth(self, db_name, tbl_name, part_vals, max_parts, user_name, group_names):
    
    self.send_get_partitions_ps_with_auth(db_name, tbl_name, part_vals, max_parts, user_name, group_names)
    return self.recv_get_partitions_ps_with_auth()"
310,"def paint(self):
        
        snippet = {
            : VectorStyle.get_style_value(self.opacity),
            : VectorStyle.get_style_value(self.color),
            : VectorStyle.get_style_value(self.outline_color)
        }
        if self.translate:
            snippet[] = self.translate

        return snippet"
311,"def _get_existing_records(cr, fp, module_name):
    
    def yield_element(node, path=None):
        if node.tag not in [, , ]:
            if node.tag == :
                xmlid = node.attrib[]
                if  not in xmlid:
                    module = module_name
                else:
                    module, xmlid = xmlid.split(, 1)
                cr.execute(
                    ,
                    (module, xmlid)
                )
                if not cr.rowcount:
                    return
            result = StringIO(etree.tostring(path, encoding=))
            result.name = None
            yield result
        else:
            for child in node:
                for value in yield_element(
                        child,
                        etree.SubElement(path, node.tag, node.attrib)
                        if path else etree.Element(node.tag, node.attrib)
                ):
                    yield value
    return yield_element(etree.parse(fp).getroot())"
312,"def quick_search(limit, pretty, sort, **kw):
    
    req = search_req_from_opts(**kw)
    cl = clientv1()
    page_size = min(limit, 250)
    echo_json_response(call_and_wrap(
        cl.quick_search, req, page_size=page_size, sort=sort
    ), pretty, limit)"
313,"def word_spans(self):
        
        if not self.is_tagged(WORDS):
            self.tokenize_words()
        return self.spans(WORDS)"
314,"def dropEvent(self, event):
        

        LOGGER.debug("">  widget drop event accepted!"".format(self.__class__.__name__))
        self.content_dropped.emit(event)"
315,"def interfaces(self):
        

        self._ifaces = []
        wifi_ctrl = wifiutil.WifiUtil()

        for interface in wifi_ctrl.interfaces():
            iface = Interface(interface)
            self._ifaces.append(iface)
            self._logger.info(""Get interface: %s"", iface.name())

        if not self._ifaces:
            self._logger.error(""Can't get wifi interface"")

        return self._ifaces"
316,"def from_dataset(cls, *args, **kwargs):
        
        main = kwargs.pop(, None)
        ret = super(Project, cls).from_dataset(*args, **kwargs)
        if main is not None:
            ret.main = main
            main.extend(ret, new_name=False)
        return ret"
317,"def rebin(a, newshape):
    
        
    shape = a.shape

    lenShape = len(shape)

    factor = np.asarray(shape)/np.asarray(newshape)
    

    evList = [] + \
             [%(i,i) for i in xrange(lenShape)] + \
             [] + [%(i+1) for i in xrange(lenShape)] + \
             [%i for i in xrange(lenShape)]

    return eval(.join(evList))"
318,"def transform(self, y, exogenous=None, n_periods=0, **_):
        
        check_is_fitted(self, ""p_"")
        _, exog = self._check_y_exog(y, exogenous, null_allowed=True)

        if n_periods and exog is not None:
            if n_periods != exog.shape[0]:
                raise ValueError(""If n_periods and exog are specified, ""
                                 ""n_periods must match dims of exogenous"")

        times = np.arange(self.n_ + n_periods, dtype=np.float64) + 1
        X_fourier = _fourier_terms(self.p_, times)

        
        if n_periods:
            X_fourier = X_fourier[-n_periods:, :]

        if exog is None:
            exog = X_fourier
        else:
            exog = np.hstack([exog, X_fourier])

        return y, exog"
319,"def _split_generators(self, dl_manager):
    
    cifar_path = dl_manager.download_and_extract(self._cifar_info.url)
    cifar_info = self._cifar_info

    cifar_path = os.path.join(cifar_path, cifar_info.prefix)

    
    for label_key, label_file in zip(cifar_info.label_keys,
                                     cifar_info.label_files):
      labels_path = os.path.join(cifar_path, label_file)
      with tf.io.gfile.GFile(labels_path) as label_f:
        label_names = [name for name in label_f.read().split(""\n"") if name]
      self.info.features[label_key].names = label_names

    
    def gen_filenames(filenames):
      for f in filenames:
        yield os.path.join(cifar_path, f)

    return [
        tfds.core.SplitGenerator(
            name=tfds.Split.TRAIN,
            num_shards=10,
            gen_kwargs={""filepaths"": gen_filenames(cifar_info.train_files)}),
        tfds.core.SplitGenerator(
            name=tfds.Split.TEST,
            num_shards=1,
            gen_kwargs={""filepaths"": gen_filenames(cifar_info.test_files)}),
    ]"
320,"def _read_vmx_file(self):
        

        try:
            self._vmx_pairs = self.manager.parse_vmware_file(self._vmx_path)
        except OSError as e:
            raise VMwareError(.format(self._vmx_path, e))"
321,"def get_alias(infos):
    
    pairs = []
    for x in infos:
        for y in x:
            if ""alias"" in y:
                name = y[""name""][0]
                alias = y[""alias""][0].split()
                for name2 in alias:
                    pairs.append((name2.strip(), name))
    return pairs"
322,"def run(self, url):
        
        url = self.process_url(url)
        if not url:
            return
        response = self.make_request(self.http, url)
        selector = self.process_response(response)
        entities = []
        for root in self.get_roots(selector):
            entity = self.entity(root)
            entity = self.process_entity(entity)
            if entity:
                entities.append(entity)
        return EntityList(*entities)"
323,"def make_url(self, method):
        

        token = self.settings()[]

        return TELEGRAM_URL.format(
            token=quote(token),
            method=quote(method),
        )"
324,"def plotwrapper(f):
    

    def wrapper(pymc_obj, *args, **kwargs):
        start = 0
        if  in kwargs:
            start = kwargs.pop()

        
        try:
            
            for variable in pymc_obj._variables_to_tally:
                
                if variable._plot is not False:
                    data = pymc_obj.trace(variable.__name__)[start:]
                    if size(data[-1]) >= 10 and variable._plot != True:
                        continue
                    elif variable.dtype is dtype():
                        continue
                    name = variable.__name__
                    if args:
                        name =  % (args[0], variable.__name__)
                    f(data, name, *args, **kwargs)
            return
        except AttributeError:
            pass

        try:
            
            data = pymc_obj()[:]
            name = pymc_obj.name
            f(data, name, *args, **kwargs)
            return
        except (AttributeError, TypeError):
            pass

        try:
            
            if pymc_obj._plot is not False:
                data = pymc_obj.trace()[start:]  
                name = pymc_obj.__name__
                f(data, name, *args, **kwargs)
            return
        except AttributeError:
            pass

        if isinstance(pymc_obj, dict):
            
            for i in pymc_obj:
                data = pymc_obj[i][start:]
                if args:
                    i =  % (args[0], i)
                elif  in kwargs:
                    i =  % (kwargs.pop(), i)
                f(data, i, *args, **kwargs)
            return

        
        f(pymc_obj, *args, **kwargs)

    wrapper.__doc__ = f.__doc__
    wrapper.__name__ = f.__name__
    return wrapper"
325,"def create(state, host, ctid, template=None):
    

    "
326,"def get_description(self, with_size=True, with_index=True):
        

        desc = 

        if with_size and self.size:
            desc += .format(self.get_formatted_size())

        s = self.info.get() or self.info.get() or 
        if with_index:
            desc += .format(s, self.index)
        else:
            desc += s

        if self.info.get():
            desc += .format(self.info.get())

        if self.info.get():  
            desc += .format(self.info.get())

        return desc"
327,"def the_gui(gui_queue):
    
    layout = [ [sg.Text()],
               [sg.Text(, size=(15,1), key=)],
               [sg.Output(size=(40,6))],
               [sg.Button()],]

    window = sg.Window().Layout(layout)
    
    while True:
        event, values = window.Read(timeout=100)        
        if event is None or event == :
            break
        
        while True:                 
            try:                    
                message = gui_queue.get_nowait()
            except queue.Empty:     
                break               
            
            if message:
                window.Element().Update(message)
                window.Refresh()    

    
    window.Close()"
328,"def to_list(self, n=None):
        
        if n is None:
            self.cache()
            return self._base_sequence
        else:
            return self.cache().take(n).list()"
329,"def _pretty_format(self, statement, result):
        
        if result is None:
            return ""Success""
        ret = result
        if statement.action in (""SELECT"", ""SCAN""):
            if statement.save_file:
                filename = statement.save_file[0]
                if filename[0] in [, ""'""]:
                    filename = unwrap(filename)
                ret = ""Saved %d record%s to %s"" % (result, plural(result), filename)
            elif isinstance(result, int):
                if result == result.scanned_count:
                    ret = ""%d"" % result
                else:
                    ret = ""%d (scanned count: %d)"" % (result, result.scanned_count)
        elif statement.action == ""UPDATE"":
            if isinstance(result, int):
                ret = ""Updated %d item%s"" % (result, plural(result))
        elif statement.action == ""DELETE"":
            ret = ""Deleted %d item%s"" % (result, plural(result))
        elif statement.action == ""CREATE"":
            if result:
                ret = ""Created table %r"" % statement.table
            else:
                ret = ""Table %r already exists"" % statement.table
        elif statement.action == ""INSERT"":
            ret = ""Inserted %d item%s"" % (result, plural(result))
        elif statement.action == ""DROP"":
            if result:
                ret = ""Dropped table %r"" % statement.table
            else:
                ret = ""Table %r does not exist"" % statement.table
        elif statement.action == ""ANALYZE"":
            ret = self._pretty_format(statement[1], result)
        elif statement.action == ""LOAD"":
            ret = ""Loaded %d item%s"" % (result, plural(result))
        return ret"
330,"def lambda_function(f):
    
    @functools.wraps(f)
    def wrapper(event, context):
        global _CURRENT_LAMBDA_CONTEXT
        _CURRENT_LAMBDA_CONTEXT = context
        try:
            result = f(event, context)
            return wait(lambda: result)
        except:
            cls, exc, trace = sys.exc_info()
            report_exc_info((cls, exc, trace.tb_next))
            wait()
            raise
    return wrapper"
331,"def validate_deckspawn_p2th(provider: Provider, rawtx: dict, p2th: str) -> bool:
    

    try:
        vout = rawtx[""vout""][0][""scriptPubKey""].get(""addresses"")[0]
    except TypeError:
        NoneType
        raise InvalidDeckSpawn(""Invalid Deck P2TH."")

    if not vout == p2th:
        raise InvalidDeckSpawn(""InvalidDeck P2TH."")

    return True"
332,"def _set_unicode(self, value):
        
        values = list(self.unicodes)
        if value in values:
            values.remove(value)
        values.insert(0, value)
        self.unicodes = values"
333,"def obj(self, path=None, model=None, values=None, raise_absent=False):
        
        return self.search(path=path, unique=True, raise_absent=raise_absent, values=values,
                            vfunc=lambda x: self.path_index[x[0]].instance(model=model) if x[0] in self.path_index else None)"
334,"def getStorageLevel(self):
        
        java_storage_level = self._jrdd.getStorageLevel()
        storage_level = StorageLevel(java_storage_level.useDisk(),
                                     java_storage_level.useMemory(),
                                     java_storage_level.useOffHeap(),
                                     java_storage_level.deserialized(),
                                     java_storage_level.replication())
        return storage_level"
335,"def merged():
    
    
    base_branch = common.get_base_branch()
    branch = git.current_branch(refresh=True)

    common.assert_branch_type()

    
    common.git_checkout(base_branch)
    common.git_pull(base_branch)

    
    common.git_branch_delete(branch.name)
    common.git_prune()

    common.git_checkout(base_branch)"
336,"def update_job(self, job_record, uow, new_state):
        
        original_job_state = job_record.state
        job_record.state = new_state
        job_record.related_unit_of_work = uow.db_id
        self.job_dao.update(job_record)

        msg =  \
              .format(job_record.db_id, job_record.process_name, job_record.timeperiod, original_job_state, new_state)
        self._log_message(INFO, job_record.process_name, job_record.timeperiod, msg)"
337,"def fermion_avg(efermi, norm_hopping, func):
    
    if func == :
        func = bethe_ekin_zeroT
    elif func == :
        func = bethe_filling_zeroT

    return np.asarray([func(ef, tz) for ef, tz in zip(efermi, norm_hopping)])"
338,"def leveled(self):
        
        
        
        safe_get_queryset = (self.get_query_set if hasattr(self, ) else self.get_queryset)
        
        return safe_get_queryset.exclude(level=0)"
339,"def buscar(self, id_vlan):
        
        if not is_valid_int_param(id_vlan):
            raise InvalidParameterError(
                u)

        url =  + str(id_vlan) + 

        code, xml = self.submit(None, , url)

        return self.response(code, xml)"
340,"def load(self, laser_plugin: LaserPlugin) -> None:
        
        log.info(""Loading plugin: {}"".format(str(laser_plugin)))
        laser_plugin.initialize(self.symbolic_vm)
        self.laser_plugins.append(laser_plugin)"
341,"def assignrepr(self, prefix) -> str:
        
        prefix =  % (prefix, self.name)
        blanks = len(prefix)*
        lines = [
            objecttools.assignrepr_value(
                self.nmb_inputs,  % prefix)+,
            objecttools.assignrepr_tuple(
                self.nmb_neurons,  % blanks)+,
            objecttools.assignrepr_value(
                self.nmb_outputs,  % blanks)+,
            objecttools.assignrepr_list2(
                self.weights_input,  % blanks)+]
        if self.nmb_layers > 1:
            lines.append(objecttools.assignrepr_list3(
                self.weights_hidden,  % blanks)+)
        lines.append(objecttools.assignrepr_list2(
            self.weights_output,  % blanks)+)
        lines.append(objecttools.assignrepr_list2(
            self.intercepts_hidden,  % blanks)+)
        lines.append(objecttools.assignrepr_list(
            self.intercepts_output,  % blanks)+)
        return .join(lines)"
342,"def str_to_two_byte_iter(string):
    
    bstring = string.encode()
    bytes = bytearray()
    for char in bstring:
        bytes.append(char)
        bytes.append(0)
    return bytes"
343,"def get_input_tensors(batch_size, tf_records, num_repeats=1,
                      shuffle_records=True, shuffle_examples=True,
                      shuffle_buffer_size=None,
                      filter_amount=0.05, random_rotation=True):
    
    print(""Reading tf_records from {} inputs"".format(len(tf_records)))
    dataset = read_tf_records(
        batch_size,
        tf_records,
        num_repeats=num_repeats,
        shuffle_records=shuffle_records,
        shuffle_examples=shuffle_examples,
        shuffle_buffer_size=shuffle_buffer_size,
        filter_amount=filter_amount,
        interleave=True)
    dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))
    dataset = dataset.map(
        functools.partial(batch_parse_tf_example, batch_size))
    if random_rotation:
        dataset = dataset.map(_random_rotation_pyfunc)

    return dataset.make_one_shot_iterator().get_next()"
344,"def find_unique_points(explored_parameters):
    
    ranges = [param.f_get_range(copy=False) for param in explored_parameters]
    zipped_tuples = list(zip(*ranges))
    try:
        unique_elements = OrderedDict()
        for idx, val_tuple in enumerate(zipped_tuples):
            if val_tuple not in unique_elements:
                unique_elements[val_tuple] = []
            unique_elements[val_tuple].append(idx)
        return list(unique_elements.items())
    except TypeError:
        logger = logging.getLogger()
        logger.error(
                     )
        unique_elements = []
        for idx, val_tuple in enumerate(zipped_tuples):
            matches = False
            for added_tuple, pos_list in unique_elements:
                matches = True
                for idx2, val in enumerate(added_tuple):
                    if not explored_parameters[idx2]._equal_values(val_tuple[idx2], val):
                        matches = False
                        break
                if matches:
                    pos_list.append(idx)
                    break
            if not matches:
                unique_elements.append((val_tuple, [idx]))
        return unique_elements"
345,"def validate_one(self, loan):
        
        assert type(loan) is dict, 

        
        req = {
            : ,
            : ,
            : ,
            : ,
            : ,
            : ,
            : ,
        }

        
        for key, criteria in req.iteritems():
            if criteria in self and key not in loan:
                raise FilterValidationError(.format(key), loan, criteria)

        
        if  in self:
            loan_ids = str(self[]).split()
            if str(loan[]) not in loan_ids:
                raise FilterValidationError(.format(loan[], self[]), loan=loan, criteria=)

        
        grade = loan[][0]  
        if  in self and self[][] is not True:
            if grade not in self[]:
                raise FilterValidationError(.format(grade), loan, )
            elif self[][grade] is False:
                raise FilterValidationError(loan=loan, criteria=)

        
        if  in self and self[] is not None:
            if loan[] == 36 and self[][] is False:
                raise FilterValidationError(loan=loan, criteria=)
            elif loan[] == 60 and self[][] is False:
                raise FilterValidationError(loan=loan, criteria=)

        
        if  in self:
            loan_progress = (1 - (loan[] / loan[])) * 100
            if self[] > loan_progress:
                raise FilterValidationError(loan=loan, criteria=)

        
        if  in self:
            if self[] is True and loan[] is True:
                raise FilterValidationError(loan=loan, criteria=)

        
        if  in self and loan[] is not False:
            purpose = self[]
            if type(purpose) is not dict:
                purpose = {purpose: True}

            if  not in purpose or purpose[] is False:
                if loan[] not in purpose:
                    raise FilterValidationError(loan=loan, criteria=)

        return True"
346,"def setBins(self, bins):
        
        self._bins = bins
        self._counts = np.zeros_like(self._bins)
        bar_width = bins[0]*1.5
        self.histo.setOpts(x=bins, height=self._counts, width=bar_width)
        self.setXlim((0, bins[-1]))"
347,"def remove_foothills(self, q_data, marked, bin_num, bin_lower, centers, foothills):
        
        hills = []
        for foot in foothills:
            center = foot[0]
            hills[:] = foot[1][:]
            
            while len(hills) > 0:
                
                pt = hills.pop(-1)
                marked[pt] = self.GLOBBED
                for s_index, val in np.ndenumerate(marked[pt[0]-1:pt[0]+2,pt[1]-1:pt[1]+2]):
                    index = (s_index[0] - 1 + pt[0], s_index[1] - 1 + pt[1])
                    
                    if val == self.UNMARKED:
                        
                        if (q_data[index] >= 0) and \
                                (q_data[index] < bin_lower) and \
                                ((q_data[index] <= q_data[pt]) or self.is_closest(index, center, centers, bin_num)):
                            hills.append(index)
        del foothills[:]"
348,"def wait_until_element_stops(self, element, times=1000, timeout=None):
        
        return self._wait_until(self._expected_condition_find_element_stopped, (element, times), timeout)"
349,"def get_dot_atom_text(value):
    
    dot_atom_text = DotAtomText()
    if not value or value[0] in ATOM_ENDS:
        raise errors.HeaderParseError(""expected atom at a start of ""
            ""dot-atom-text but found "".format(value))
    while value and value[0] not in ATOM_ENDS:
        token, value = get_atext(value)
        dot_atom_text.append(token)
        if value and value[0] == :
            dot_atom_text.append(DOT)
            value = value[1:]
    if dot_atom_text[-1] is DOT:
        raise errors.HeaderParseError(""expected atom at end of dot-atom-text ""
            ""but found "".format(+value))
    return dot_atom_text, value"
350,"def densm(alt, d0, xm, tz, mn3, zn3, tn3, tgn3, mn2, zn2, tn2, tgn2):
    
    xs = [0.0]*10
    ys = [0.0]*10
    y2out = [0.0]*10
    rgas = 831.4
    
    densm_tmp=d0
    if (alt>zn2[0]): 
        if(xm==0.0):
            return tz[0]
        else:
            return d0

    
    if (alt>zn2[mn2-1]):
        z=alt
    else:
        z=zn2[mn2-1];
    mn=mn2;
    z1=zn2[0];
    z2=zn2[mn-1];
    t1=tn2[0];
    t2=tn2[mn-1];
    zg = zeta(z, z1);
    zgdif = zeta(z2, z1);

    
    for k in range(mn):
        xs[k]=zeta(zn2[k],z1)/zgdif;
        ys[k]=1.0 / tn2[k];
    yd1=-tgn2[0] / (t1*t1) * zgdif;
    yd2=-tgn2[1] / (t2*t2) * zgdif * (pow(((re[0]+z2)/(re[0]+z1)),2.0));

    
    spline (xs, ys, mn, yd1, yd2, y2out);   
    x = zg/zgdif;
    y = [0.0]
    splint (xs, ys, y2out, mn, x, y);

    
    tz[0] = 1.0 / y[0];
    if (xm!=0.0):
        
        glb = gsurf[0] / (pow((1.0 + z1/re[0]),2.0));
        gamm = xm * glb * zgdif / rgas;

        
        yi = [0.0]
        splini(xs, ys, y2out, mn, x, yi);
        expl=gamm*yi[0];
        if (expl>50.0): 
            expl=50.0

        
        densm_tmp = densm_tmp * (t1 / tz[0]) * exp(-expl);

    if (alt>zn3[0]):
        if (xm==0.0):
            return tz[0]
        else:
            return densm_tmp

    
    z = alt;
    mn = mn3;
    z1=zn3[0];
    z2=zn3[mn-1];
    t1=tn3[0];
    t2=tn3[mn-1];
    zg=zeta(z,z1);
    zgdif=zeta(z2,z1);



    
    for k in range(mn):
        xs[k] = zeta(zn3[k],z1) / zgdif;
        ys[k] = 1.0 / tn3[k];
    
    yd1=-tgn3[0] / (t1*t1) * zgdif;
    yd2=-tgn3[1] / (t2*t2) * zgdif * (pow(((re[0]+z2)/(re[0]+z1)),2.0));

    
    spline (xs, ys, mn, yd1, yd2, y2out);
    x = zg/zgdif;
    y = [0.0]
    splint (xs, ys, y2out, mn, x, y);

    
    tz[0] = 1.0 / y[0];
    if (xm!=0.0):
        
        glb = gsurf[0] / (pow((1.0 + z1/re[0]),2.0));
        gamm = xm * glb * zgdif / rgas;

        
        yi = [0.0]
        splini(xs, ys, y2out, mn, x, yi);
        expl=gamm*yi[0];
        if (expl>50.0): 
            expl=50.0;

        
        densm_tmp = densm_tmp * (t1 / tz[0]) * exp(-expl);
    
    if (xm==0.0):
        return tz[0];
    else:
        return densm_tmp;"
351,"def _fallback_to_available_output(self):
        
        if len(self.active_comb) == 1:
            self._choose_what_to_display(force_refresh=True)
            self._apply()
            self.py3.update()"
352,"def load_alerts(self):
        
        self._feed = AlertsFeed(state=self.scope, maxage=self.cachetime)
        parser = CapParser(self._feed.raw_cap(), geo=self.geo)
        self._alerts = parser.get_alerts()"
353,"def extract(self, name):
        
        if type(name) == type():
            ndx = self.toc.find(name)
            if ndx == -1:
                return None
        else:
            ndx = name
        (dpos, dlen, ulen, flag, typcd, nm) = self.toc.get(ndx)
        self.lib.seek(self.pkgstart+dpos)
        rslt = self.lib.read(dlen)
        if flag == 2:
            global AES
            import AES
            key = rslt[:32]
            
            return (1, rslt)
        return (0, rslt)"
354,"def handle_closed_task(self, task_name, record):
        
        if task_name not in self.tasks:
            return

        if self.main_failed:
            self.mark_parent_tasks_as_failed(self.cur_task)

        if self.tasks[task_name].failed:
            record.msg = ColorFormatter.colored(, END_TASK_ON_ERROR_MSG)
        else:
            record.msg = ColorFormatter.colored(, END_TASK_MSG)

        record.msg +=  % self.tasks[task_name].elapsed_time()

        if self.should_show_by_depth() or self.tasks[task_name].force_show:
            if self.tasks[task_name].force_show:
                self.handle_error()

            self.pretty_emit(record, is_header=True)

        self.close_children_tasks(task_name)
        self.tasks.pop(task_name)"
355,"def estimate_supercell_matrix(spglib_dataset,
                              max_num_atoms=120):
    

    spg_num = spglib_dataset[]
    num_atoms = len(spglib_dataset[])
    lengths = _get_lattice_parameters(spglib_dataset[])

    if spg_num <= 74:  
        multi = _get_multiplicity_abc(num_atoms, lengths, max_num_atoms)
    elif spg_num <= 194:  
        multi = _get_multiplicity_ac(num_atoms, lengths, max_num_atoms)
    else:  
        multi = _get_multiplicity_a(num_atoms, lengths, max_num_atoms)

    return multi"
356,"def read(keypath, configfile=None):
    
    if configfile in _configs:
        appconfig = _configs[configfile]
    else:
        appconfig = AppConfig(configfile=configfile)
        _configs[configfile] = appconfig

    return appconfig.read(keypath)"
357,"def send_template(self, user_id, template_id, data, url=None, mini_program=None):
        
        tpl_data = optionaldict(
            touser=user_id,
            template_id=template_id,
            url=url,
            miniprogram=mini_program,
            data=data,
        )
        return self._post(
            ,
            data=tpl_data
        )"
358,"def main(self):
        
        logger_lpd_noaa.info(""enter main"")
        

        
        

        

        self.noaa_data_sorted[""File_Last_Modified_Date""][""Modified_Date""] = generate_timestamp()

        self.__get_table_count()

        
        self.__put_tables_in_self([""paleo"", ""paleoData"", ""measurementTable""])
        self.__put_tables_in_self([""chron"", ""chronData"", ""measurementTable""])

        
        self.__get_table_pairs()

        
        self.__reorganize()

        
        
        

        self.__get_overall_data(self.lipd_data)
        self.__reorganize_sensor()
        self.__lists_to_str()
        self.__generate_study_name()

        

        
        
        self.__create_file()
        logger_lpd_noaa.info(""exit main"")
        return"
359,"def sd(line, cell=None):
  
  parser = google.datalab.utils.commands.CommandParser(prog=, description=(
      
      ))

  
  _create_monitoring_subparser(parser)
  return google.datalab.utils.commands.handle_magic_line(line, cell, parser)"
360,"def has_result(state, incorrect_msg=""Your query did not return a result.""):
    

    
    has_no_error(state)

    if not state.solution_result:
        raise NameError(
            ""You are using has_result() to verify that the student query generated an error, but the solution query did not return a result either!""
        )

    if not state.student_result:
        state.do_test(incorrect_msg)

    return state"
361,"def _estimate_count(self):
        
        if self.estimate_z == 0:
            self.estimate_z = (1.0 / self.nbr_bits)
        self.estimate_z = min(self.estimate_z, 0.999999)
        self.count = int(-(self.nbr_bits / self.nbr_slices) * np.log(1 - self.estimate_z))"
362,"def set_current_value(self, value):
        
        self._current_value = value
        self._current_rendered = self._encode_value(self._current_value)
        return self._current_rendered"
363,"def folder(self, folder):
        
        result = []
        for root, _, files in os.walk(folder):
            for file in files:
                path = os.path.join(root, file)
                if self._chooses_path(path):
                    result.append(self.path(path))
        return result"
364,"def from_url(cls, url, filename=None, chunk_size=CHUNK_SIZE):
        
        pipe = _WebPipe(url, chunk_size=chunk_size)
        if filename is None:
            filename = pipe.name

        return cls(pipe, filename, chunk_size)"
365,"def _get_file_from_iso_fp(self, outfp, blocksize, iso_path, rr_path, joliet_path):
        
        
        if joliet_path is not None:
            if self.joliet_vd is None:
                raise pycdlibexception.PyCdlibInvalidInput()
            found_record = self._find_joliet_record(joliet_path)
        elif rr_path is not None:
            if not self.rock_ridge:
                raise pycdlibexception.PyCdlibInvalidInput()
            found_record = self._find_rr_record(rr_path)
        elif iso_path is not None:
            found_record = self._find_iso_record(iso_path)
        else:
            raise pycdlibexception.PyCdlibInternalError()

        if found_record.is_dir():
            raise pycdlibexception.PyCdlibInvalidInput()

        if rr_path is not None or iso_path is not None:
            if found_record.rock_ridge is not None and found_record.rock_ridge.is_symlink():
                
                
                
                
                
                
                
                
                
                raise pycdlibexception.PyCdlibInvalidInput()

        if found_record.inode is None:
            raise pycdlibexception.PyCdlibInvalidInput()

        while found_record.get_data_length() > 0:
            with inode.InodeOpenData(found_record.inode, self.pvd.logical_block_size()) as (data_fp, data_len):
                
                
                
                
                if found_record.inode.boot_info_table is not None:
                    header_len = min(data_len, 8)
                    outfp.write(data_fp.read(header_len))
                    data_len -= header_len
                    if data_len > 0:
                        rec = found_record.inode.boot_info_table.record()
                        table_len = min(data_len, len(rec))
                        outfp.write(rec[:table_len])
                        data_len -= table_len
                        if data_len > 0:
                            data_fp.seek(len(rec), os.SEEK_CUR)
                            utils.copy_data(data_len, blocksize, data_fp, outfp)
                else:
                    utils.copy_data(data_len, blocksize, data_fp, outfp)

            if found_record.data_continuation is not None:
                found_record = found_record.data_continuation
            else:
                break"
366,"def set_attr(self, name, value):
    
    self.exec_script(""node.setAttribute(%s, %s)"" % (repr(name), repr(value)))"
367,"def split_css_for_ie_selector_limit(input_file, output_file):
    
    from .modules import bless, utils

    if not isinstance(input_file, str):
        raise RuntimeError()

    return {
        : utils.no_dependencies,
        : bless.bless_css,
        : input_file,
        : output_file,
        : {},
    }"
368,"def makeExecutable(fp):
    
    mode = ((os.stat(fp).st_mode) | 0o555) & 0o7777
    setup_log.info(""Adding executable bit to %s (mode is now %o)"", fp, mode)
    os.chmod(fp, mode)"
369,"def create_dataset(self, owner_id, **kwargs):
        
        request = self.__build_dataset_obj(
            lambda: _swagger.DatasetCreateRequest(
                title=kwargs.get(),
                visibility=kwargs.get()),
            lambda name, url, expand_archive, description, labels:
            _swagger.FileCreateRequest(
                name=name,
                source=_swagger.FileSourceCreateRequest(
                    url=url,
                    expand_archive=expand_archive),
                description=description,
                labels=labels),
            kwargs)
        try:
            (_, _, headers) = self._datasets_api.create_dataset_with_http_info(
                owner_id, request, _return_http_data_only=False)
            if  in headers:
                return headers[]
        except _swagger.rest.ApiException as e:
            raise RestApiError(cause=e)"
370,"async def _restart_on_cancel(logger, agent):
    
    while True:
        try:
            await agent.run()
        except asyncio.CancelledError:
            logger.exception(""Restarting agent"")
            pass"
371,"def get_path_directories():
    
    pth = os.environ[]
    if sys.platform ==  and os.environ.get(""BASH""):
        
        if pth[1] == :  
            pth = pth.replace(, , 1)
    return [p.strip() for p in pth.split(os.pathsep) if p.strip()]"
372,"def dropEvent(self, event):
        
        super(TrashWidget, self).dropEvent(event)
        self.itemTrashed.emit()"
373,"def get_overrides_filename(variable):
    
    filename = os.environ.get(variable)

    if filename is None:
        msg = .format(variable)
        raise EnvironmentError(msg)

    return filename"
374,"def _finalize_chunk(self, dd, offsets):
        
        
        
        if dd.entity.is_encrypted:
            dd.mark_unchecked_chunk_decrypted(offsets.chunk_num)
        
        
        dd.perform_chunked_integrity_check()
        
        with self._disk_operation_lock:
            self._disk_set.remove(
                blobxfer.operations.download.Downloader.
                create_unique_disk_operation_id(dd, offsets))
            self._download_bytes_sofar += offsets.num_bytes"
375,"def before_sleep_log(logger, log_level):
    
    def log_it(retry_state):
        if retry_state.outcome.failed:
            verb, value = , retry_state.outcome.exception()
        else:
            verb, value = , retry_state.outcome.result()

        logger.log(log_level,
                   ""Retrying %s in %s seconds as it %s %s."",
                   _utils.get_callback_name(retry_state.fn),
                   getattr(retry_state.next_action, ),
                   verb, value)
    return log_it"
376,"def status(server, output, verbose):
    
    if verbose:
        log(""Connecting to Entity Matching Server: {}"".format(server))

    service_status = server_get_status(server)
    if verbose:
        log(""Status: {}"".format(service_status[]))
    print(json.dumps(service_status), file=output)"
377,"def render_json(tree, indent):
    
    return json.dumps([{: k.as_dict(),
                        : [v.as_dict() for v in vs]}
                       for k, vs in tree.items()],
                      indent=indent)"
378,"def execute(self, fragment, pretty_format=True):
        
        self.fragments = (self.fragments + ""\n"" + fragment).lstrip()
        try:
            line_parser.parseString(self.fragments)
        except ParseException:
            pass
        else:
            self.last_query = self.fragments.strip()
            self.fragments = """"
            return super(FragmentEngine, self).execute(self.last_query, pretty_format)
        return None"
379,"def createElement(self, tag: str) -> Node:
        
        return create_element(tag, base=self._default_class)"
380,"def IsDataVisible(self, path):
    
    if path is None:
      return (False, RESPONSES[])

    if _Matches(path, self.blacklist_patterns):
      return (False, RESPONSES[])

    if not _Matches(path, self.whitelist_patterns):
      return (False, RESPONSES[])

    return (True, RESPONSES[])"
381,"def decode(self, codes):
        
        assert codes.ndim == 2
        N, M = codes.shape
        assert M == self.M
        assert codes.dtype == self.code_dtype

        vecs = np.empty((N, self.Ds * self.M), dtype=np.float32)
        for m in range(self.M):
            vecs[:, m * self.Ds : (m+1) * self.Ds] = self.codewords[m][codes[:, m], :]

        return vecs"
382,"def _build_mac_signature_key_information(self, value):
        
        if value is None:
            return None
        if not isinstance(value, dict):
            raise TypeError(
                ""MAC/signature key information must be a dictionary.""
            )

        cryptographic_parameters = value.get()
        if cryptographic_parameters:
            cryptographic_parameters = self._build_cryptographic_parameters(
                cryptographic_parameters
            )
        mac_signature_key_information = cobjects.MACSignatureKeyInformation(
            unique_identifier=value.get(),
            cryptographic_parameters=cryptographic_parameters
        )
        return mac_signature_key_information"
383,"def get_elections(self, obj):
        
        election_day = ElectionDay.objects.get(
            date=self.context[])
        elections = Election.objects.filter(
            race__office=obj,
            election_day=election_day
        )
        return ElectionSerializer(elections, many=True).data"
384,"def to_table_data(self):
        

        self._validate_source_data()

        for table_key, json_records in six.iteritems(self._buffer):
            self._loader.inc_table_count()
            self._table_key = table_key

            yield TableData(
                self._make_table_name(),
                [""key"", ""value""],
                [record for record in json_records.items()],
                dp_extractor=self._loader.dp_extractor,
                type_hints=self._extract_type_hints(),
            )"
385,"def get_handler(self, *args, **options):
        
        handler = super(Command, self).get_handler(*args, **options)
        if options[]:
            threading.Timer(1, self.livereload_request, kwargs=options).start()
        return handler"
386,"def split_name(name):
    
    s = name.rsplit(, 1)
    if len(s) == 1:
        return s[0], None
    else:
        try:
            return s[0], int(s[1])
        except ValueError:
            raise ValueError(""Invalid Filetracker filename: version must ""
                             ""be int, not %r"" % (s[1],))"
387,"def avg_dicts(dictin1, dictin2, dropmissing=True):
    
    
    dictout = dict()
    for key in dictin1:
        if key in dictin2:
            dictout[key] = (dictin1[key] + dictin2[key]) / 2
        elif not dropmissing:
            dictout[key] = dictin1[key]
    if not dropmissing:
        for key in dictin2:
            if key not in dictin1:
                dictout[key] = dictin2[key]
    return dictout"
388,"def listdir(self, pattern=None):
        
        names = os.listdir(self)
        if pattern is not None:
            names = fnmatch.filter(names, pattern)
        return [self / child for child in names]"
389,"def get_bookmark(self, bookmark_id):
        
        url = self._generate_url(.format(bookmark_id))
        return self.get(url)"
390,"def get_results_range(self):
        
        spec = self.context.getResultsRange()
        if spec:
            return dicts_to_dict(spec, ""keyword"")
        return ResultsRangeDict()"
391,"def _kl_laplace_laplace(a, b, name=None):
  
  with tf.name_scope(name or ""kl_laplace_laplace""):
    
    
    distance = tf.abs(a.loc - b.loc)
    ratio = a.scale / b.scale

    return (-tf.math.log(ratio) - 1 + distance / b.scale +
            ratio * tf.exp(-distance / a.scale))"
392,"def btc_witness_script_serialize(_stack):
    
    stack = _stack
    if encoding.json_is_base(_stack, 16):
        
        stack = encoding.json_changebase(_stack, lambda x: binascii.unhexlify(x))

    return encoding.safe_hexlify(_btc_witness_serialize_unit(len(stack)) + .join(map(lambda stack_unit: _btc_witness_serialize_unit(stack_unit), stack)))"
393,"def enum_to_yaml(cls: Type[T_EnumToYAML], representer: Representer, data: T_EnumToYAML) -> ruamel.yaml.nodes.ScalarNode:
    
    return representer.represent_scalar(
        f""!{cls.__name__}"",
        f""{str(data)}""
    )"
394,"def file_handler(self, handler_type, path, prefixed_path, source_storage):
        
        if self.faster:
            if prefixed_path not in self.found_files:
                self.found_files[prefixed_path] = (source_storage, path)

            self.task_queue.put({
                : handler_type,
                : path,
                : prefixed_path,
                : source_storage
            })
            self.counter += 1
        else:
            if handler_type == :
                super(Command, self).link_file(path, prefixed_path, source_storage)
            else:
                super(Command, self).copy_file(path, prefixed_path, source_storage)"
395,"def fetch(self):
        
        params = values.of({})

        payload = self._version.fetch(
            ,
            self._uri,
            params=params,
        )

        return MediaInstance(
            self._version,
            payload,
            account_sid=self._solution[],
            message_sid=self._solution[],
            sid=self._solution[],
        )"
396,"def channels_add_all(self, room_id, **kwargs):
        
        return self.__call_api_post(, roomId=room_id, kwargs=kwargs)"
397,"def load_dict():
    
    global g_load_java_message_filename
    global g_ok_java_messages

    if os.path.isfile(g_load_java_message_filename):
            
        with open(g_load_java_message_filename,) as ofile:
            g_ok_java_messages = pickle.load(ofile)
    else:   
        g_ok_java_messages[""general""] = []"
398,"def get_genetic_map_HapMapII_GRCh37(self):
        
        if self._genetic_map_HapMapII_GRCh37 is None:
            self._genetic_map_HapMapII_GRCh37 = self._load_genetic_map(
                self._get_path_genetic_map_HapMapII_GRCh37()
            )

        return self._genetic_map_HapMapII_GRCh37"
399,"def flip_x(self, center=None):
        
        if center is None:
            self.poly.flip()
        else:
            self.poly.flip(center[0])"
400,"def serial_udb_extra_f2_b_encode(self, sue_time, sue_pwm_input_1, sue_pwm_input_2, sue_pwm_input_3, sue_pwm_input_4, sue_pwm_input_5, sue_pwm_input_6, sue_pwm_input_7, sue_pwm_input_8, sue_pwm_input_9, sue_pwm_input_10, sue_pwm_output_1, sue_pwm_output_2, sue_pwm_output_3, sue_pwm_output_4, sue_pwm_output_5, sue_pwm_output_6, sue_pwm_output_7, sue_pwm_output_8, sue_pwm_output_9, sue_pwm_output_10, sue_imu_location_x, sue_imu_location_y, sue_imu_location_z, sue_flags, sue_osc_fails, sue_imu_velocity_x, sue_imu_velocity_y, sue_imu_velocity_z, sue_waypoint_goal_x, sue_waypoint_goal_y, sue_waypoint_goal_z, sue_memory_stack_free):
                
                return MAVLink_serial_udb_extra_f2_b_message(sue_time, sue_pwm_input_1, sue_pwm_input_2, sue_pwm_input_3, sue_pwm_input_4, sue_pwm_input_5, sue_pwm_input_6, sue_pwm_input_7, sue_pwm_input_8, sue_pwm_input_9, sue_pwm_input_10, sue_pwm_output_1, sue_pwm_output_2, sue_pwm_output_3, sue_pwm_output_4, sue_pwm_output_5, sue_pwm_output_6, sue_pwm_output_7, sue_pwm_output_8, sue_pwm_output_9, sue_pwm_output_10, sue_imu_location_x, sue_imu_location_y, sue_imu_location_z, sue_flags, sue_osc_fails, sue_imu_velocity_x, sue_imu_velocity_y, sue_imu_velocity_z, sue_waypoint_goal_x, sue_waypoint_goal_y, sue_waypoint_goal_z, sue_memory_stack_free)"
401,"def _get_rho(self, w, arr):
        
        lsb = 0
        while not (w & arr[lsb]):
            lsb += 1
        return lsb + 1"
402,"def send_api_request(self, method, url, params={}, valid_parameters=[], needs_api_key=False):
        
        if needs_api_key:
            params.update({: self.request.consumer_key})
            valid_parameters.append()

        files = {}
        if  in params:
            if isinstance(params[], list):
                for idx, data in enumerate(params[]):
                    files[+str(idx)+] =  open(params[][idx], )
            else:
                files = {: open(params[], )}
            del params[]

        validate_params(valid_parameters, params)
        if method == ""get"":
            return self.request.get(url, params)
        else:
            return self.request.post(url, params, files)"
403,"def spkw02(handle, body, center, inframe, first, last, segid, intlen, n, polydg,
           cdata, btime):
    
    handle = ctypes.c_int(handle)
    body = ctypes.c_int(body)
    center = ctypes.c_int(center)
    inframe = stypes.stringToCharP(inframe)
    first = ctypes.c_double(first)
    last = ctypes.c_double(last)
    segid = stypes.stringToCharP(segid)
    intlen = ctypes.c_double(intlen)
    n = ctypes.c_int(n)
    polydg = ctypes.c_int(polydg)
    cdata = stypes.toDoubleVector(cdata)
    btime = ctypes.c_double(btime)
    libspice.spkw02_c(handle, body, center, inframe, first, last, segid, intlen,
                      n, polydg, cdata, btime)"
404,"def sys_register_SDL_renderer(callback: Callable[[Any], None]) -> None:
    
    with _PropagateException() as propagate:

        @ffi.def_extern(onerror=propagate)  
        def _pycall_sdl_hook(sdl_surface: Any) -> None:
            callback(sdl_surface)

        lib.TCOD_sys_register_SDL_renderer(lib._pycall_sdl_hook)"
405,"def load_config(files=None, root_path=None, local_path=None):
    

    config = cfg.ConfigOpts()
    config.register_opts([
        cfg.Opt(, default=root_path),
        cfg.Opt(, default=local_path),
    ])
    
    
    if files is not None:
        config(args=[], default_config_files=files)
    return config"
406,"def get_snmp_service(self):
        
        return SnmpContextManager(self.enable_flow, self.disable_flow, self._snmp_parameters, self._logger)"
407,"async def parse_command(self, stream):
        
        line = await stream.readline()
        if not line:
            raise ConnectionResetError
        s = line.decode(encoding=self.encoding).rstrip()
        logger.info(s)
        cmd, _, rest = s.partition("" "")
        return cmd.lower(), rest"
408,"def hook_wnd_proc(self):
        
        self.__local_wnd_proc_wrapped = WndProcType(self.local_wnd_proc)
        self.__old_wnd_proc = SetWindowLong(self.__local_win_handle,
                                        GWL_WNDPROC,
                                        self.__local_wnd_proc_wrapped)"
409,"def check_password(self, raw_password):
        
        def setter(raw_password):
            self.set_password(raw_password)
            self.save(update_fields=[self.PASSWORD_FIELD])
        return check_password(raw_password, getattr(self, self.PASSWORD_FIELD), setter)"
410,"def _set_state(self, state):
        
        if state != self._association_state:
            self.__log_debug(, self._association_state, state)
            self._association_state = state

        if state == self.State.ESTABLISHED:
            self.__state = 
            for channel in list(self._data_channels.values()):
                if channel.negotiated and channel.readyState != :
                    channel._setReadyState()
            asyncio.ensure_future(self._data_channel_flush())
        elif state == self.State.CLOSED:
            self._t1_cancel()
            self._t2_cancel()
            self._t3_cancel()
            self.__state = 

            
            for stream_id in list(self._data_channels.keys()):
                self._data_channel_closed(stream_id)

            
            
            self.remove_all_listeners()"
411,"def add_fields(self, **fields):
        
        self.__class__ = type(self.__class__.__name__,
                            (self.__class__,), fields)
        for k, v in fields.items():
            v.init_inst(self)"
412,"def add_group(self, name: str, **kwargs) -> None:
        
        group = {
            name: deserializer.inventory.InventoryElement.deserialize_group(
                name=name, defaults=self.defaults, **kwargs
            )
        }
        self.groups.update(group)"
413,"def unquote(string, encoding=, errors=):
    
    if  not in string:
        string.split
        return string
    if encoding is None:
        encoding = 
    if errors is None:
        errors = 
    bits = _asciire.split(string)
    res = [bits[0]]
    append = res.append
    for i in range(1, len(bits), 2):
        append(unquote_to_bytes(bits[i]).decode(encoding, errors))
        append(bits[i + 1])
    return .join(res)"
414,"def serial_periodicfeatures(pfpkl_list,
                            lcbasedir,
                            outdir,
                            starfeaturesdir=None,
                            fourierorder=5,
                            
                            transitparams=(-0.01,0.1,0.1),
                            
                            ebparams=(-0.2,0.3,0.7,0.5),
                            pdiff_threshold=1.0e-4,
                            sidereal_threshold=1.0e-4,
                            sampling_peak_multiplier=5.0,
                            sampling_startp=None,
                            sampling_endp=None,
                            starfeatures=None,
                            timecols=None,
                            magcols=None,
                            errcols=None,
                            lcformat=,
                            lcformatdir=None,
                            sigclip=10.0,
                            verbose=False,
                            maxobjects=None):
    bestbests
        normalized periodogram peak over the sampling periodogram peak at the
        same period required to accept the  period as possibly real.

    sampling_startp, sampling_endp : float
        If the `pgramlist` doesnve stored
        your lcformat description JSONs, other than the usual directories lcproc
        knows to search for them in. Use this along with `lcformat` to specify
        an LC format JSON file thatasymmetric

    try:
        formatinfo = get_lcformat(lcformat,
                                  use_lcformat_dir=lcformatdir)
        if formatinfo:
            (fileglob, readerfunc,
             dtimecols, dmagcols, derrcols,
             magsarefluxes, normfunc) = formatinfo
        else:
            LOGERROR(""cant figure out the light curve format"")
        return None

    

    for task in tqdm(tasks):
        _periodicfeatures_worker(task)"
415,"def register():
    

    if _security.confirmable or request.is_json:
        form_class = _security.confirm_register_form
    else:
        form_class = _security.register_form

    if request.is_json:
        form_data = MultiDict(request.get_json())
    else:
        form_data = request.form

    form = form_class(form_data)

    if form.validate_on_submit():
        user = register_user(**form.to_dict())
        form.user = user

        if not _security.confirmable or _security.login_without_confirmation:
            after_this_request(_commit)
            login_user(user)

        if not request.is_json:
            if  in form:
                redirect_url = get_post_register_redirect(form.next.data)
            else:
                redirect_url = get_post_register_redirect()

            return redirect(redirect_url)
        return _render_json(form, include_auth_token=True)

    if request.is_json:
        return _render_json(form)

    return _security.render_template(config_value(),
                                     register_user_form=form,
                                     **_ctx())"
416,"def distL1(x1,y1,x2,y2):
    
    return int(abs(x2-x1) + abs(y2-y1)+.5)"
417,"def set_policy_attack_strength(self, policy_ids, attack_strength):
        
        for policy_id in policy_ids:
            self.logger.debug(.format(policy_id, attack_strength))
            result = self.zap.ascan.set_policy_attack_strength(policy_id, attack_strength)
            if result != :
                raise ZAPError(.format(policy_id, result))"
418,"def wr_py_sections_new(self, fout_py, doc=None):
        
        sections = self.grprobj.get_sections_2d()
        return self.wr_py_sections(fout_py, sections, doc)"
419,"def _copy_problem_hparams(p_hparams):
  
  p = p_hparams
  
  p.modality[""targets""] = p.modality[""inputs""]
  
  p.vocab_size[""targets""] = p.vocab_size[""inputs""]
  
  p.vocabulary[""targets""] = p.vocabulary[""inputs""]
  
  p.target_space_id = p.input_space_id
  
  p.was_copy = True"
420,"def merged_gasmap(self, **kwargs):
        
        kwargs_copy = self.base_dict.copy()
        kwargs_copy.update(**kwargs)
        self._replace_none(kwargs_copy)        
        localpath = NameFactory.merged_gasmap_format.format(**kwargs_copy)
        if kwargs.get(, False):
            return self.fullpath(localpath=localpath)
        return localpath"
421,"def _error_handler(data, unique_id):
        
        if data.get() == :
            logger.error( % (unique_id, data.get(), data.get()))
            if data.get():
                return True
        if data.get():
            "
422,"def build_default_logger(
    logger_name=,
    log_level=None,
    log_dir=None,
    console_enabled=True,
    max_log_size=5*1024*1024,
    max_backup_logs=5):
    
    old_logger_class = logging.getLoggerClass()
    logging.setLoggerClass(DefaultLogger)
    logger = logging.getLogger(logger_name)
    logging.setLoggerClass(old_logger_class)

    if log_level:
        logger.setLevel(log_level)
    logger.apply_default_handlers(log_dir, console_enabled, max_log_size, max_backup_logs)
    return logger"
423,"def exit(self):
        
        if self.client is not None:
            self.client.close()
        if self.context is not None:
            self.context.destroy()"
424,"def clean(self, data):
        
        try:
            value = data[self.column_name]
        except KeyError:
            raise KeyError(""Column  not found in dataset. Available ""
                           ""columns are: %s"" % (self.column_name, list(data)))

        
        value = self.widget.clean(value, row=data)

        if value in self.empty_values and self.default != NOT_PROVIDED:
            if callable(self.default):
                return self.default()
            return self.default

        return value"
425,"def auth_basic(check, realm=""private"", text=""Access denied""):
    
    def decorator(func):
      def wrapper(*a, **ka):
        user, password = request.auth or (None, None)
        if user is None or not check(user, password):
          response.headers[] =  % realm
          return HTTPError(401, text)
        return func(*a, **ka)
      return wrapper
    return decorator"
426,"def tech_constraints_satisfied(self):
        

        
        load_area_count_per_ring = float(cfg_ding0.get(,
                                                       ))

        max_half_ring_length = float(cfg_ding0.get(,
                                                   ))

        if self._problem._branch_kind == :
            load_factor_normal = float(cfg_ding0.get(,
                                                     ))
            load_factor_malfunc = float(cfg_ding0.get(,
                                                      ))
        elif self._problem._branch_kind == :
            load_factor_normal = float(cfg_ding0.get(,
                                                     ))
            load_factor_malfunc = float(cfg_ding0.get(,
                                                      ))
        else:
            raise ValueError(s _branch_kind is invalid, could not use branch parameters.mv_routing_tech_constraintsmv_max_v_level_lc_diff_normalmv_routing_tech_constraintsmv_max_v_level_lc_diff_malfuncassumptionscos_phi_loadRLI_max_thI_max_thI_max_th'] * load_factor_malfunc):
            return False

        
        

        
        v_level_hring1 =\
            v_level_hring2 =\
            v_level_ring_dir1 =\
            v_level_ring_dir2 =\
            v_level_op =\
            self._problem._v_level * 1e3

        
        r_hring1 =\
            r_hring2 =\
            x_hring1 =\
            x_hring2 =\
            r_ring_dir1 =\
            r_ring_dir2 =\
            x_ring_dir1 =\
            x_ring_dir2 = 0

        for n1, n2 in zip(nodes_hring1[0:len(nodes_hring1)-1], nodes_hring1[1:len(nodes_hring1)]):
            r_hring1 += self._problem.distance(n1, n2) * r
            x_hring1 += self._problem.distance(n1, n2) * x
            v_level_hring1 -= n2.demand() * 1e3 * (r_hring1 + x_hring1*Q_factor) / v_level_op
            if (v_level_op - v_level_hring1) > (v_level_op * mv_max_v_level_lc_diff_normal):
                return False

        for n1, n2 in zip(nodes_hring2[0:len(nodes_hring2)-1], nodes_hring2[1:len(nodes_hring2)]):
            r_hring2 += self._problem.distance(n1, n2) * r
            x_hring2 += self._problem.distance(n1, n2) * x
            v_level_hring2 -= n2.demand() * 1e3 * (r_hring2 + x_hring2 * Q_factor) / v_level_op
            if (v_level_op - v_level_hring2) > (v_level_op * mv_max_v_level_lc_diff_normal):
                return False

        
        
        for (n1, n2), (n3, n4) in zip(zip(nodes_ring1[0:len(nodes_ring1)-1], nodes_ring1[1:len(nodes_ring1)]),
                                      zip(nodes_ring2[0:len(nodes_ring2)-1], nodes_ring2[1:len(nodes_ring2)])):
            r_ring_dir1 += self._problem.distance(n1, n2) * r
            r_ring_dir2 += self._problem.distance(n3, n4) * r
            x_ring_dir1 += self._problem.distance(n1, n2) * x
            x_ring_dir2 += self._problem.distance(n3, n4) * x
            v_level_ring_dir1 -= (n2.demand() * 1e3 * (r_ring_dir1 + x_ring_dir1 * Q_factor) / v_level_op)
            v_level_ring_dir2 -= (n4.demand() * 1e3 * (r_ring_dir2 + x_ring_dir2 * Q_factor) / v_level_op)
            if ((v_level_op - v_level_ring_dir1) > (v_level_op * mv_max_v_level_lc_diff_malfunc) or
                (v_level_op - v_level_ring_dir2) > (v_level_op * mv_max_v_level_lc_diff_malfunc)):
                return False

        return True"
427,"def update_thumbnail_via_upload(api_key, api_secret, video_key, local_video_image_path=, api_format=,
                                **kwargs):
    
    jwplatform_client = jwplatform.Client(api_key, api_secret)
    logging.info(""Updating video thumbnail."")
    try:
        response = jwplatform_client.videos.thumbnails.update(
            video_key=video_key,
            **kwargs)
    except jwplatform.errors.JWPlatformError as e:
        logging.error(""Encountered an error updating thumbnail.\n{}"".format(e))
        sys.exit(e.message)
    logging.info(response)

    
    upload_url = .format(
        response[][],
        response[][],
        response[][]
    )

    
    query_parameters = response[][]
    query_parameters[] = api_format

    with open(local_video_image_path, ) as f:
        files = {: f}
        r = requests.post(upload_url, params=query_parameters, files=files)
        logging.info(.format(local_video_image_path, r.url))
        logging.info(.format(r.text))"
428,"def from_json(cls, json):
    
    return cls(json[cls.BLOB_KEY_PARAM],
               json[cls.INITIAL_POSITION_PARAM],
               json[cls.END_POSITION_PARAM])"
429,"def delete_registry(self, registry):
        
        
        if re.match("".*\\/.*"", registry):
            return [False, ""input registry name cannot contain  characters - valid registry names are of the form <host>:<port> where :<port> is optional""]

        url = self.url + ""/api/scanning/v1/anchore/registries/"" + registry
        res = requests.delete(url, headers=self.hdrs, verify=self.ssl_verify)
        if not self._checkResponse(res):
            return [False, self.lasterr]

        return [True, res.json()]"
430,"def fit(self, mol1, mol2):
        
        return self.get_rmsd(mol1, mol2) < self._tolerance"
431,"def switchDisplay(self, display):
        
        if display in self.displays:
            self.setWidget(self.displays[display])
            self._current = display
        else:
            raise Exception(""Undefined display type ""+ display)"
432,"def _start(self):
        
        params = self._translate(self._options)
        self._resp = self._r_session.get(self._url, params=params, stream=True)
        self._resp.raise_for_status()
        self._lines = self._resp.iter_lines(self._chunk_size)"
433,"def get_text(self, locator, params=None, timeout=None, visible=True):
        
        element = locator
        if not isinstance(element, WebElement):
            element = self.get_present_element(locator, params, timeout, visible)

        if element and element.text:
            return element.text
        else:
            try:
                return element.get_attribute()
            except AttributeError:
                return """""
434,"async def fetch_signatures(endpoint, protocol, idgen):
        
        async with aiohttp.ClientSession() as session:
            req = {
                ""method"": ""getMethodTypes"",
                ""params"": [],
                ""version"": ""1.0"",
                ""id"": next(idgen),
            }

            if protocol == ProtocolType.WebSocket:
                async with session.ws_connect(endpoint, timeout=2) as s:
                    await s.send_json(req)
                    res = await s.receive_json()
                    return res
            else:
                res = await session.post(endpoint, json=req)
                json = await res.json()

                return json"
435,"def dockermachine_ip() -> Optional[str]:
    
    if not check_dockermachine():
        return None

    
    try:
        out = subprocess.check_output([, ])
        return out.decode(""utf-8"").strip()
    except Exception:
        logger.debug(f""docker machine not present"")
        return None"
436,"def create_osd_keyring(conn, cluster, key):
    
    logger = conn.logger
    path = .format(
        cluster=cluster,
    )
    if not conn.remote_module.path_exists(path):
        logger.warning()
        conn.remote_module.write_keyring(path, key)"
437,"def with_zero_or_one(cls, converter, pattern=None):
        
        cardinality = Cardinality.zero_or_one
        if not pattern:
            pattern = getattr(converter, ""pattern"", cls.default_pattern)
        optional_pattern = cardinality.make_pattern(pattern)
        group_count = cardinality.compute_group_count(pattern)

        def convert_optional(text, m=None):
            if text:
                text = text.strip()
            if not text:
                return None
            return converter(text)
        convert_optional.pattern = optional_pattern
        
        convert_optional.regex_group_count = group_count
        return convert_optional"
438,"def is_vagrant_plugin_installed(plugin, use_sudo=False):
    

    cmd = 

    if use_sudo:
        results = sudo(cmd)
    else:
        results = run(cmd)

    installed_plugins = []
    for line in results:
        plugin = re.search(, line)
        installed_plugins.append({: plugin.group(0),
                                  : plugin.group(1)})
        return installed_plugins"
439,"def clear_dcnm_out_part(self, tenant_id, fw_dict, is_fw_virt=False):
        
        res = fw_const.DCNM_OUT_PART_UPDDEL_SUCCESS
        self.update_fw_db_result(tenant_id, dcnm_status=res)
        LOG.info(""Out partition cleared -noop- with service ip addr"")
        return True"
440,"def noise_power_spectrum(data, ground_truth, radial=False,
                         radial_binning_factor=2.0):
    
    try:
        space = data.space
        assert isinstance(space, odl.DiscreteLp)
    except (AttributeError, AssertionError):
        data = np.asarray(data)
        space = odl.uniform_discr(
            [0] * data.ndim, data.shape, data.shape, data.dtype
        )
        data = space.element(data)

    ft = odl.trafos.FourierTransform(space, halfcomplex=False)
    nps = np.abs(ft(data - ground_truth)).real ** 2

    if radial:
        return spherical_sum(nps, binning_factor=radial_binning_factor)
    else:
        return nps"
441,"def state_category(value):
        
        if value == re.sre_parse.CATEGORY_DIGIT:
            return (yield )

        if value == re.sre_parse.CATEGORY_WORD:
            return (yield )"
442,"def get(cls, ns, key):
        
        return getattr(db, cls.__name__).find_one(
            ConfigItem.namespace_prefix == ns,
            ConfigItem.key == key
        )"
443,"def get_proficiencies_by_ids(self, proficiency_ids):
        
        
        
        
        collection = JSONClientValidated(,
                                         collection=,
                                         runtime=self._runtime)
        object_id_list = []
        for i in proficiency_ids:
            object_id_list.append(ObjectId(self._get_id(i, ).get_identifier()))
        result = collection.find(
            dict({: {: object_id_list}},
                 **self._view_filter()))
        result = list(result)
        sorted_result = []
        for object_id in object_id_list:
            for object_map in result:
                if object_map[] == object_id:
                    sorted_result.append(object_map)
                    break
        return objects.ProficiencyList(sorted_result, runtime=self._runtime, proxy=self._proxy)"
444,"def add_var_condor_cmd(self, command):
    
    if command not in self.__var_cmds:
        self.__var_cmds.append(command)
        macro = self.__bad_macro_chars.sub( r, command )
        self.add_condor_cmd(command,  + macro + )"
445,"def get_template_names(self):
        
        names = super(CommandDatagridView, self).get_template_names()
        names.append()
        return names"
446,"def _get_zk_path_children(self, zk_conn, zk_path, name_for_error):
        
        children = []
        try:
            children = zk_conn.get_children(zk_path)
        except NoNodeError:
            self.log.info(, zk_path)
        except Exception:
            self.log.exception(, name_for_error, zk_path)
        return children"
447,"def with_subtype(self, subtype):
        
        self._validate_subtype(subtype)
        self.subtype = subtype
        return self"
448,"def rvs(self, random_state=None):
        r
        
        if self.dist is None:
            return self.value

        
        rs = check_random_state(random_state)
        samples = self.dist.rvs(size=self.shape, random_state=rs)

        
        samples = self.bounds.clip(samples)

        return samples"
449,"def read(self, length=0, timeout_ms=None):
    
    return self._transport.read(
        length, timeouts.PolledTimeout.from_millis(timeout_ms))"
450,"def _delete_sourcesystem_cd(conn: Connection, table: Table, sourcesystem_cd: str) -> int:
        
        return conn.execute(delete(table).where(table.c.sourcesystem_cd == sourcesystem_cd)).rowcount \
            if sourcesystem_cd else 0"
451,"def mk_operation(metaclass, o_tfr):
    
    o_obj = one(o_tfr).O_OBJ[115]()
    action = o_tfr.Action_Semantics_internal
    label =  % (o_obj.Name, o_tfr.Name)
    run = interpret.run_operation
    
    if o_tfr.Instance_Based:
        return lambda self, **kwargs: run(metaclass, label, action, kwargs, self)
    else:
        fn = lambda cls, **kwargs: run(metaclass, label, action, kwargs, None)
        return classmethod(fn)"
452,"def id_to_object(self, line):
        
        result = Range.get(line, ignore=404)
        if not result:
            result = Range(range=line)
            result.save()
        return result"
453,"def dont_cache():
    
    def decorate_func(func):
        @wraps(func)
        def decorate_func_call(*a, **kw):
            callback = SetCacheControlHeadersForNoCachingCallback()
            registry_provider = AfterThisRequestCallbackRegistryProvider()
            registry = registry_provider.provide()
            registry.add(callback)
            return func(*a, **kw)
        return decorate_func_call
    return decorate_func"
454,"def unordered_storage(config, name=None):
    typedictredisredisenvREDIS_HOSTNAMEdefaultlocalhost
    tp = config[]
    if tp == :
        return DictSetStorage(config)
    if tp == :
        return RedisSetStorage(config, name=name)"
455,"def add_observer(self, callback):
        
        if callback in self._observers:
            raise ValueError(
                             .format(callback, self))
        self._observers.append(callback)"
456,"def fully_expanded_path(self):
        
        return os.path.abspath(
            os.path.normpath(
                os.path.normcase(
                    os.path.expandvars(
                        os.path.expanduser(self.path)))))"
457,"def default_config_filename(root_dir=None):
        
        root_dir = Path(root_dir) if root_dir else Path().abspath()
        locale_dir = root_dir / 
        if not os.path.exists(locale_dir):
            locale_dir = root_dir /  / 
        return locale_dir / BASE_CONFIG_FILENAME"
458,"def _download_repodata(self, checked_repos):
        
        self._files_downloaded = []
        self._repodata_files = []
        self.__counter = -1

        if checked_repos:
            for repo in checked_repos:
                path = self._repo_url_to_path(repo)
                self._files_downloaded.append(path)
                self._repodata_files.append(path)
                worker = self.download_async(repo, path)
                worker.url = repo
                worker.path = path
                worker.sig_finished.connect(self._repodata_downloaded)
        else:
            
            
            path = self._get_repodata_from_meta()
            self._repodata_files = [path]
            self._repodata_downloaded()"
459,"def create(self, create_missing=None):
        
        return HostGroup(
            self._server_config,
            id=self.create_json(create_missing)[],
        ).read()"
460,"def end_date(self) -> Optional[datetime.date]:
        
        if not self.intervals:
            return None
        return self.end_datetime().date()"
461,"def removeComments(element):
    
    global _num_bytes_saved_in_comments
    num = 0

    if isinstance(element, xml.dom.minidom.Comment):
        _num_bytes_saved_in_comments += len(element.data)
        element.parentNode.removeChild(element)
        num += 1
    else:
        for subelement in element.childNodes[:]:
            num += removeComments(subelement)

    return num"
462,"def fs_r_sup(self, DF, N=None):
		
		if not hasattr(self, ):
			self.fs_c(N=self.rank)  

		if N and (not isinstance(N, int) or N <= 0):
			raise ValueError(""ncols should be a positive integer."")
		s = -sqrt(self.E) if self.cor else self.s
		N = min(N, self.rank) if N else self.rank
		S_inv = diagsvd(-1/s[:N], len(self.G.T), N)
		
		return _mul(DF.div(DF.sum(axis=1), axis=0), self.G, S_inv)[:, :N]"
463,"def get_drive(self, drive_id):
        
        if not drive_id:
            return None

        url = self.build_url(
            self._endpoints.get().format(id=drive_id))

        response = self.con.get(url)
        if not response:
            return None

        drive = response.json()

        
        return self.drive_constructor(con=self.con, protocol=self.protocol,
                                      main_resource=self.main_resource,
                                      **{self._cloud_data_key: drive})"
464,"def _make_x_title(self):
        
        y = (self.height - self.margin_box.bottom + self._x_labels_height)
        if self._x_title:
            for i, title_line in enumerate(self._x_title, 1):
                text = self.svg.node(
                    self.nodes[],
                    ,
                    class_=,
                    x=self.margin_box.left + self.view.width / 2,
                    y=y + i * (self.style.title_font_size + self.spacing)
                )
                text.text = title_line"
465,"def connection_made(self):
        
        assert self.state == BGP_FSM_CONNECT
        
        open_msg = self._peer.create_open_msg()
        self._holdtime = open_msg.hold_time
        self.state = BGP_FSM_OPEN_SENT
        if not self.is_reactive:
            self._peer.state.bgp_state = self.state
        self.sent_open_msg = open_msg
        self.send(open_msg)
        self._peer.connection_made()"
466,"def print_version():
    
    latest, current = get_versions()
    if latest is None:
        safeprint(
            (""Installed Version: {0}\n"" ""Failed to lookup latest version."").format(
                current
            )
        )
    else:
        safeprint(
            (""Installed Version: {0}\n"" ""Latest Version:    {1}\n"" ""\n{2}"").format(
                current,
                latest,
                ""You are running the latest version of the Globus CLI""
                if current == latest
                else (
                    ""You should update your version of the Globus CLI with\n""
                    ""  globus update""
                )
                if current < latest
                else ""You are running a preview version of the Globus CLI"",
            )
        )

    
    
    if is_verbose():
        moddata = _get_package_data()

        safeprint(""\nVerbose Data\n---"")

        safeprint(""platform:"")
        safeprint(""  platform: {}"".format(platform.platform()))
        safeprint(""  py_implementation: {}"".format(platform.python_implementation()))
        safeprint(""  py_version: {}"".format(platform.python_version()))
        safeprint(""  sys.executable: {}"".format(sys.executable))
        safeprint(""  site.USER_BASE: {}"".format(site.USER_BASE))

        safeprint(""modules:"")
        for mod, modversion, modfile, modpath in moddata:
            safeprint(""  {}:"".format(mod))
            safeprint(""    __version__: {}"".format(modversion))
            safeprint(""    __file__: {}"".format(modfile))
            safeprint(""    __path__: {}"".format(modpath))"
467,"def _save_model(self, section):
        
        checkpoint_name = self.flags[] +  % section + 
        save_path = self.saver.save(self.sess, checkpoint_name)
        print(""Model saved in file: %s"" % save_path)"
468,"def generate_report(book_url):
    
    with piecash.open_book(book_url, readonly=True, open_if_lock=True) as book:
        accounts = [acc.fullname for acc in book.accounts]

        return f"
469,"def create_network_configuration_files(self, file_path, guest_networks,
                                           first, active=False):
        
        cfg_files = []
        cmd_strings = 
        network_config_file_name = self._get_network_file()
        network_cfg_str = 
        network_cfg_str += 
        net_enable_cmd = 
        if first:
            clean_cmd = self._get_clean_command()
        else:
            clean_cmd = 
            network_cfg_str = 

        for network in guest_networks:
            base_vdev = network[].lower()
            network_hw_config_fname = self._get_device_filename(base_vdev)
            network_hw_config_str = self._get_network_hw_config_str(base_vdev)
            cfg_files.append((network_hw_config_fname, network_hw_config_str))
            (cfg_str, dns_str) = self._generate_network_configuration(network,
                                    base_vdev)
            LOG.debug(, cfg_str)
            network_cfg_str += cfg_str
            if len(dns_str) > 0:
                network_cfg_str += dns_str
        if first:
            cfg_files.append((network_config_file_name, network_cfg_str))
        else:
            cmd_strings = ( % (network_cfg_str,
                                                 network_config_file_name))
        return cfg_files, cmd_strings, clean_cmd, net_enable_cmd"
470,"def set_auth_service(self, auth_service: BaseAuthService):
        
        if issubclass(auth_service.__class__, BaseAuthService):
            self.auth_service = auth_service
            return self
        else:
            raise TypeError(""Your auth service object must be a subclass of rinzler.auth.BaseAuthService."")"
471,"def __execute_undef(self, instr):
        
        op2_val = random.randint(0, instr.operands[2].size)

        self.write_operand(instr.operands[2], op2_val)

        return None"
472,"def _display_controls(self):
        

        def print_command(char, info):
            char += "" "" * (10 - len(char))
            print(""{}\t{}"".format(char, info))

        print("""")
        print_command(""Keys"", ""Command"")
        print_command(""q"", ""reset simulation"")
        print_command(""spacebar"", ""toggle gripper (open/close)"")
        print_command(""w-a-s-d"", ""move arm horizontally in x-y plane"")
        print_command(""r-f"", ""move arm vertically"")
        print_command(""z-x"", ""rotate arm about x-axis"")
        print_command(""t-g"", ""rotate arm about y-axis"")
        print_command(""c-v"", ""rotate arm about z-axis"")
        print_command(""ESC"", ""quit"")
        print("""")"
473,"def find_partial_link_text(self, partial_link_text,
                               timeout=settings.LARGE_TIMEOUT):
        
        if self.timeout_multiplier and timeout == settings.LARGE_TIMEOUT:
            timeout = self.__get_new_timeout(timeout)
        return self.wait_for_partial_link_text(
            partial_link_text, timeout=timeout)"
474,"def due(self):
        
        invoice_charges = Charge.objects.filter(invoice=self)
        invoice_transactions = Transaction.successful.filter(invoice=self)
        return total_amount(invoice_charges) - total_amount(invoice_transactions)"
475,"def Geometric(p, tag=None):
    
    assert (
        0 < p < 1
    ), 
    return uv(ss.geom(p), tag=tag)"
476,"def updateAllKeys(self):
        
        for kf, key in zip(self.kf_list, self.sorted_key_list()):
            kf.update(key, self.dct[key])"
477,"def span_tokenize_sents(self, strings):
        
        raise NotImplementedError(""span_tokenizer and span_tokenzie_sents not yet implemented. ;)"")
        for s in strings:
            yield list(self.span_tokenize(s))"
478,"def load(self, format=None, *, kwargs={}):
        
        return load(self, format=format, kwargs=kwargs)"
479,"def render(msgpack_data, saltenv=, sls=, **kws):
    
    if not isinstance(msgpack_data, six.string_types):
        msgpack_data = msgpack_data.read()

    if msgpack_data.startswith():
        msgpack_data = msgpack_data[(msgpack_data.find() + 1):]
    if not msgpack_data.strip():
        return {}
    return salt.utils.msgpack.loads(msgpack_data)"
480,"def reparse(self, filepath):
        
        
        
        self.tramp.touch(filepath)
        self.parse(filepath)"
481,"def force_ascii(value):
    
    if not isinstance(value, (str, unicode)):
        return unicode(value)
    if isinstance(value, unicode):
        return unidecode.unidecode(value)
    else:
        return unidecode.unidecode(force_unicode(value))"
482,"def get_search(self):
        
        old_search = self.get_session_value(, )
        search = self.get_and_save_value(, )
        if old_search != search:
            self.page = 1
            self.get_session_value(, self.page)
        return search"
483,"def create_tags(self, resource_ids, tags):
        
        params = {}
        self.build_list_params(params, resource_ids, )
        self.build_tag_param_list(params, tags)
        return self.get_status(, params, verb=)"
484,"def create_table(self, table_name, schema, provisioned_throughput):
        
        data = { : table_name,
                 : schema,
                : provisioned_throughput}
        json_input = json.dumps(data)
        response_dict = self.make_request(, json_input)
        return response_dict"
485,"def overlap(self, other):
        
        if self._start < other.end and self._end > other.start:
            return True
        return False"
486,"def undistortPoints(self, points, keepSize=False):
        
        s = self.img.shape
        cam = self.coeffs[]
        d = self.coeffs[]

        pts = np.asarray(points, dtype=np.float32)
        if pts.ndim == 2:
            pts = np.expand_dims(pts, axis=0)

        (newCameraMatrix, roi) = cv2.getOptimalNewCameraMatrix(cam,
                                                               d, s[::-1], 1,
                                                               s[::-1])
        if not keepSize:
            xx, yy = roi[:2]
            pts[0, 0] -= xx
            pts[0, 1] -= yy

        return cv2.undistortPoints(pts,
                                   cam, d, P=newCameraMatrix)"
487,"def get_sql(self):
        
        
        self.arg_index = 0
        self.args = {}

        
        if len(self.wheres):
            where = self.build_where_part(self.wheres)
            return .format(where)
        return "
488,"def warn(self, message, *args, **kwargs):
        
        self._log(logging.WARNING, message, *args, **kwargs)"
489,"def load_bioassembly_info_from_file(self, biomol_num):
        

        
        bioass_to_chain_stoich = defaultdict(int)
        for model in structure:
            for chain in model:
                bioass_to_chain_stoich[chain.id] += 1
        return dict(bioass_to_chain_stoich)"
490,"def fit(self, X, y):
        
        self._X = X  
        self._y = y  

        
        self._datalen = len(self._X)  

        
        if hasattr(self, ) and type(self.n_neighbors) is float:
            
            self.n_neighbors = int(self.n_neighbors * self._datalen * 0.5)

        
        self._label_list = list(set(self._y))
        
        discrete_label = (len(self._label_list) <= self.discrete_threshold)

        
        if discrete_label:
            if len(self._label_list) == 2:
                self._class_type = 
                self.mcmap = 0
            elif len(self._label_list) > 2:
                self._class_type = 
                self.mcmap = self._getMultiClassMap()
            else:
                raise ValueError()

        else:
            self._class_type = 
            self.mcmap = 0

        
        self._labels_std = 0.
        if len(self._label_list) > self.discrete_threshold:
            self._labels_std = np.std(self._y, ddof=1)

        self._num_attributes = len(self._X[0])  
        
        
        self._missing_data_count = np.isnan(self._X).sum()

        
        xlen = len(self._X[0])
        mxlen = len(str(xlen + 1))
        self._headers = [.format(str(i).zfill(mxlen)) for i in range(1, xlen + 1)]

        start = time.time()  

        
        C = D = False
        
        self.attr = self._get_attribute_info()
        for key in self.attr.keys():
            if self.attr[key][0] == :
                D = True
            if self.attr[key][0] == :
                C = True

        
        if C and D:
            self.data_type = 
        elif D and not C:
            self.data_type = 
        elif C and not D:
            self.data_type = 
        else:
            raise ValueError()
        

        
        
        diffs, cidx, didx = self._dtype_array()
        cdiffs = diffs[cidx]  

        xc = self._X[:, cidx]  
        xd = self._X[:, didx]  

        
        if self._missing_data_count > 0:
            self._distance_array = self._distarray_missing(xc, xd, cdiffs)
        else:
            self._distance_array = self._distarray_no_missing(xc, xd)

        if self.verbose:
            elapsed = time.time() - start
            print(.format(elapsed))
            print()

        start = time.time()
       

       
        
        self.feature_importances_ = self._run_algorithm()

        
        del self._distance_array

        if self.verbose:
            elapsed = time.time() - start
            print(.format(elapsed))

        
        self.top_features_ = np.argsort(self.feature_importances_)[::-1]

        return self"
491,"def download_bundle_view(self, request, pk):
        

        return self._download_response(request, pk, bundle=True)"
492,"def silhouette_score(self, data,  metric=, sample_size=None, random_state=None, **kwds):
        
        return silhouette_score(data, self.get_labels(self), metric, sample_size, random_state, **kwds)"
493,"def overview(index, start, end):
    

    results = {
        ""activity_metrics"": [SubmittedPRs(index, start, end),
                             ClosedPRs(index, start, end)],
        ""author_metrics"": [],
        ""bmi_metrics"": [BMIPR(index, start, end)],
        ""time_to_close_metrics"": [DaysToClosePRMedian(index, start, end)],
        ""projects_metrics"": []
    }

    return results"
494,"def equals(self, other):
        
        self._run(unittest_case.assertEqual, (self._subject, other))
        return ChainInspector(self._subject)"
495,"def get_pixbeam_pixel(self, x, y):
        
        ra, dec = self.pix2sky((x, y))
        return self.get_pixbeam(ra, dec)"
496,"def evaluate(self, x, y, batch_size=32, sample_weight=None, is_distributed=False):
        
        if sample_weight:
            unsupport_exp(""sample_weight"")
        if is_distributed:
            if isinstance(x, np.ndarray):
                input = to_sample_rdd(x, y)
            elif isinstance(x, RDD):
                input = x
            if self.metrics:
                sc = get_spark_context()
                return [r.result for r in
                        self.bmodel.evaluate(input, batch_size, self.metrics)]
            else:
                raise Exception(""No Metrics found."")
        else:
            raise Exception(""We only support evaluation in distributed mode"")"
497,"def calcMassFromMz(mz, charge):
    
    mass = (mz - maspy.constants.atomicMassProton) * charge
    return mass"
498,"def name(self):
        
        name = super(Interface, self).name
        return name if name else self.data.get()"
499,"def feature_info(self):
        
        feature_list = self.prop(, None)
        if feature_list is None:
            raise ValueError(""Firmware features are not supported on CPC %s"" %
                             self.manager.cpc.name)
        return feature_list"
500,"def remove_xml_element(name, tree):
    
    
    remove = tree.findall(
        "".//{{http://soap.sforce.com/2006/04/metadata}}{}"".format(name)
    )
    if not remove:
        return tree

    parent_map = {c: p for p in tree.iter() for c in p}

    for elem in remove:
        parent = parent_map[elem]
        parent.remove(elem)

    return tree"
