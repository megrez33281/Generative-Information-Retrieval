# 專案問題回答

## 問題一：稀疏檢索(TF-IDF vs. BM25)
**在稀疏檢索方法中，比較 TF-IDF 和 BM25 的檢索性能。在此次作業中，哪種方法表現更好？並分析造成差異的可能原因（例如：詞頻處理、文件長度正規化）。**
在此次作業的實驗中，經過多輪優化的**TF-IDF表現顯著優於BM25**  
在自建的本地驗證集上，基準TF-IDF的`Recall@10`分數為`0.7400`，而BM25僅為 `0.6720`  
即使在對BM25的`k1`和`b`參數進行多輪調整後，其最高分也只達到`0.6780`，仍未超過 TF-IDF  
最終經由對TF-IDF進行次線性詞頻縮放 (Sublinear TF Scaling)和查詢擴展 (Query Expansion)兩項優化，使其本地驗證分數達到了 `0.7860`，在Kaggle上取得了 **`0.73600`** 的成績，成功超越了Strong Baseline  


造成此差異（TF-IDF優於BM25）的原因與理論預期相反，分析主要有以下幾點：
1.  **文件長度相對一致**
    BM25的核心優勢之一是其精細的「文件長度正規化」，它在處理長度差異懸殊的文件時特別有效  
    然而，本次作業的程式碼片段長度可能相對平均，這使得BM25的該項優勢無法發揮，甚至可能因不當的懲罰而降低了性能

2.  **查詢詞與文件詞頻特性**
    BM25的另一個優勢是詞頻飽和度機制，即一個詞在文件中出現次數的邊際效益會遞減  
    但在本次實驗中發現對TF-IDF採用類似的次線性詞頻縮放 (`1 + log(tf)`)策略後，性能得到了巨大提升  
    這表明，雖然詞頻飽和度的思想是正確的，但TF-IDF配合簡單的對數縮放，其組合效果在本資料集上恰好優於BM25更複雜的正規化公式  
3.  **參數敏感度**  
    BM25的表現高度依賴 `k1`和`b`兩個參數，儘管進行了調整但可能仍未找到全局最優解  
    而TF-IDF沒有這麼敏感的超參數（特別是經過對數縮放後），使其表現更為穩健  

總結來說，雖然BM25在理論上更先進，但在本次特定的資料集和任務上，一個經過優化（特別是詞頻處理）的TF-IDF模型展現了更強的實用性和性能  


## 問題二：密集檢索(預訓練 vs. 微調)  
**在密集檢索方法中，比較直接使用預訓練模型與使用訓練資料進行微調後的性能。哪種方法表現更好？並解釋造成差異的可能原因。**  

**使用訓練資料進行微調後的模型，其性能遠遠優於直接使用的預訓練模型。**
多次的實驗清晰地證明了這一點  
在實驗初期，直接使用預訓練的CodeBERT在本地驗證集上的`Recall@10`僅有`0.2600`  
然而，在更換為性能更強的`microsoft/unixcoder-base`模型，並採用了**困難負樣本挖掘 (Hard Negative Mining)**策略進行微調後，模型在Kaggle上的分數達上升到了**`0.87200`**  

造成這種巨大差異的原因如下：
1.  **任務適應性 (Task-Specific Adaptation)**
    預訓練模型（如 Unixcoder）雖然從海量程式碼中學到了通用的語法和語義結構，但它並不理解我們這個特定的檢索任務  
    它不知道要如何將一句自然語言查詢，精準地映射到解決該問題的程式碼片段上  
    **微調的核心目的，就是讓模型去學習這個特定的映射關係**

2.  **優化向量空間**
    在微調中使用了三元組損失(Triplet Loss)  
    這個損失函數的目標非常明確：在向量空間中，將「查詢向量」與「正確的程式碼向量（正樣本）」的距離拉近，同時將其與「錯誤的程式碼向量（負樣本）」的距離推遠  
    這使得最終生成的**向量空間**是為檢索此語料庫量身打造的，極大地提升了區分相似程式碼的能力  

3.  **高質量的訓練信號**
    為了提高模型表現，不僅進行了微調，還採用了困難負樣本挖掘策略  
    相比於隨機找一個負樣本，使用TF-IDF預先找出那些與查詢在相似度高但實際是錯誤的程式碼作為負樣本
    用這種高質量的「難題」去訓練模型，強迫它去學習更深層次、更細微的語義差別，從而使其在面對模稜兩可的查詢時，具備更強的判斷力  
    這也是分數能從`0.85200`進一步提升到`0.87200`的關鍵  

因此，微調不僅是有效的，而且微調的「策略」也至關重要，它直接決定了模型性能的上限  


## 問題三：稀疏 vs. 密集檢索及未來改進
**在文字到程式碼檢索任務中，比較稀疏檢索和密集檢索的差異與性能。除了這些方法，還有哪些方法（例如：Retrieve-and-Re-rank）可以進一步提升檢索性能？**

**性能與差異比較**：
在此次作業中，**密集檢索的性能全面且顯著地超越了稀疏檢索**  
*   最強的**稀疏模型**（TF-IDF 優化版）在 Kaggle 上的分數為 **`0.73600`**  
*   最強的**密集模型**（Unixcoder + 困難負樣本挖掘）在 Kaggle 上的分數達到了 **`0.87200`**  

兩者的核心差異在於：
*   **稀疏檢索 (TF-IDF)**
    基於「關鍵字匹配」  
    它快速、高效、可解釋性強，但無法理解語義  
    如果查詢中的詞彙（如 "add"）沒有出現在目標程式碼中（用了 "sum"），它就可能失敗  

*   **密集檢索 (Unixcoder)**
    基於「語義理解」  
    通過深度學習模型將查詢和程式碼映射到同一個語義空間，即使沒有共享的關鍵字，只要語義相關，就能成功檢索  
    這是其性能遠超稀疏模型的根本原因  

**未來可嘗試的改進方法**：

1.  **混合檢索 (Hybrid Retrieval)**
    這是在實驗中嘗試過的方法  
    理論上，它可以結合稀疏模型的「精準匹配」能力和密集模型的「語義理解」能力  
    實驗中嘗試使用了更穩健的RRF演算法進行融合，但在本地驗證集上，混合後的結果(`0.8400`)反而低於單獨的密集模型(`0.9600`)  
    經過分析得到的結論是：**當其中一個模型（密集模型）的性能過於強大時，另一個較弱模型（稀疏模型）的貢獻會成為「噪音」而非「補充」，從而導致性能下降**  


2.  **檢索再排序 (Retrieve-and-Re-rank)**
    這是目前業界最主流、最高效的頂級性能方案，它分為兩階段：  
    *   **第一階段：召回 (Retrieve)**
        使用一個**快速**的模型（如TF-IDF）從數百萬的文檔庫中，快速篩選出一個較大的候選集（例如 Top 100） 
        這個階段追求「快」和「全」，目標是確保正確答案大概率在這個候選集裡  
    *   **第二階段：精排 (Re-rank)**
        使用一個**強大但緩慢**的**Cross-Encoder**模型，對這100個候選者進行精細的二次排序  
        Cross-Encoder會將 `(查詢, 候選程式碼)`對同時輸入模型，進行深度的注意力交互後輸出兩者是否是同一類的分數，其排序精度遠高於目前使用的Bi-Encoder（分開編碼查詢和程式碼）  
        最終從這100個中選出Top 10作為最終答案  

