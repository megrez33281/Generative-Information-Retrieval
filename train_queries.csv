code,query
"def foreachBatch(self, func):
        

        from pyspark.java_gateway import ensure_callback_server_started
        gw = self._spark._sc._gateway
        java_import(gw.jvm, ""org.apache.spark.sql.execution.streaming.sources.*"")

        wrapped_func = ForeachBatchFunction(self._spark, func)
        gw.jvm.PythonForeachBatchHelper.callForeachBatch(self._jwrite, wrapped_func)
        ensure_callback_server_started(gw)
        return self","Sets the output of the streaming query to be processed using the provided
        function. This is supported only the in the micro-batch execution modes (that is, when the
        trigger is not continuous). In every micro-batch, the provided function will be called in
        every micro-batch with (i) the output rows as a DataFrame and (ii) the batch identifier.
        The batchId can be used deduplicate and transactionally write the output
        (that is, the provided Dataset) to external systems. The output DataFrame is guaranteed
        to exactly same for the same batchId (assuming all operations are deterministic in the
        query).

        .. note:: Evolving.

        >>> def func(batch_df, batch_id):
        ...     batch_df.collect()
        ...
        >>> writer = sdf.writeStream.foreach(func)"
"def _organize_variants(samples, batch_id):
    
    caller_names = [x[""variantcaller""] for x in samples[0][""variants""]]
    calls = collections.defaultdict(list)
    for data in samples:
        for vrn in data[""variants""]:
            calls[vrn[""variantcaller""]].append(vrn[""vrn_file""])
    data = samples[0]
    vrn_files = []
    for caller in caller_names:
        fnames = calls[caller]
        if len(fnames) == 1:
            vrn_files.append(fnames[0])
        else:
            vrn_files.append(population.get_multisample_vcf(fnames, batch_id, caller, data))
    return caller_names, vrn_files","Retrieve variant calls for all samples, merging batched samples into single VCF."
"def resolve_aliases(data_type):
    
    if not is_alias(data_type):
        return data_type

    resolved = resolve_aliases(data_type.data_type)
    data_type.data_type = resolved

    return resolved","Resolve all chained / nested aliases. This will recursively point
    nested aliases to their resolved data type (first non-alias in the chain).

    Note: This differs from unwrap_alias which simply identifies/returns
    the resolved data type.

    Args:
        data_type (DataType): The target DataType/Alias to resolve.
    Return:
        DataType: The resolved type."
"def do_where(self, taskid: int) -> None:
        
        task = task_by_id(taskid, self._loop)
        if task:
            self._sout.write(_format_stack(task))
            self._sout.write()
        else:
            self._sout.write( % taskid)",Show stack frames for a task
"def check_import():
    
    try:
        import aeneas
        print_success(u""aeneas         OK"")
        return False
    except ImportError:
        print_error(u""aeneas         ERROR"")
        print_info(u""  Unable to load the aeneas Python package"")
        print_info(u""  This error is probably caused by:"")
        print_info(u""    A. you did not download/git-clone the aeneas package properly; or"")
        print_info(u""    B. you did not install the required Python packages:"")
        print_info(u""      1. BeautifulSoup4"")
        print_info(u""      2. lxml"")
        print_info(u""      3. numpy"")
    except Exception as e:
        print_error(e)
    return True",Try to import the aeneas package and return ``True`` if that fails.
"def make_systemrestoreitem_originalfilename(original_filename, condition=, negate=False, preserve_case=False):
    
    document = 
    search = 
    content_type = 
    content = original_filename
    ii_node = ioc_api.make_indicatoritem_node(condition, document, search, content_type, content,
                                              negate=negate, preserve_case=preserve_case)
    return ii_node","Create a node for SystemRestoreItem/OriginalFileName
    
    :return: A IndicatorItem represented as an Element node"
"def oai_bucket_policy_present(name, Bucket, OAI, Policy,
                              region=None, key=None, keyid=None, profile=None):
    
    ret = {: name, : True, : , : {}}
    oais = __salt__[](
            Comment=OAI, region=region, key=key, keyid=keyid, profile=profile)
    if len(oais) > 1:
        msg = .format(OAI)
        log.error(msg)
        ret[] = msg
        ret[] = False
        return ret
    if not oais:
        msg = .format(OAI)
        log.error(msg)
        ret[] = msg
        ret[] = False
        return ret
    canonical_user = oais[0].get()
    oai_id = oais[0].get()
    if isinstance(Policy, six.string_types):
        Policy = json.loads(Policy)
    for stanza in range(len(Policy.get(, []))):
        if  not in Policy[][stanza]:
            Policy[][stanza][] = {""CanonicalUser"": canonical_user}
    bucket = __salt__[](Bucket=Bucket, region=region, key=key,
            keyid=keyid, profile=profile)
    if not bucket or  not in bucket:
        msg = .format(Bucket)
        log.error(msg)
        ret[] = msg
        ret[] = False
        return ret
    curr_policy = bucket[].get(, {}).get(, {})  
    return ret","Ensure the given policy exists on an S3 bucket, granting access for the given origin access
    identity to do the things specified in the policy.

    name
        The name of the state definition

    Bucket
        The S3 bucket which CloudFront needs access to. Note that this policy
        is exclusive - it will be the only policy definition on the bucket (and
        objects inside the bucket if you specify such permissions in the
        policy). Note that this likely SHOULD reflect the bucket mentioned in
        the Resource section of the Policy, but this is not enforced...

    OAI
        The value of `Name` passed to the state definition for the origin
        access identity which will be accessing the bucket.

    Policy
        The full policy document which should be set on the S3 bucket. If a
        ``Principal`` clause is not provided in the policy, one will be
        automatically added, and pointed at the correct value as dereferenced
        from the OAI provided above. If one IS provided, then this is not
        done, and you are responsible for providing the correct values.

    region (string)
        Region to connect to.

    key (string)
        Secret key to use.

    keyid (string)
        Access key to use.

    profile (dict or string)
        Dict, or pillar key pointing to a dict, containing AWS region/key/keyid.

    Example:

    .. code-block:: yaml

        my_oai_s3_policy:
          boto_cloudfront.oai_bucket_policy_present:
          - Bucket: the_bucket_for_my_distribution
          - OAI: the_OAI_I_just_created_and_attached_to_my_distribution
          - Policy:
              Version: 2012-10-17
              Statement:
              - Effect: Allow
                Action: s3:GetObject
                Resource: arn:aws:s3:::the_bucket_for_my_distribution/*"
"def _set_ipv6_phy_intf_cmds(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=ipv6_phy_intf_cmds.ipv6_phy_intf_cmds, is_container=, presence=False, yang_name=""ipv6-phy-intf-cmds"", rest_name="""", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u: {u: u, u: None, u: u}}, namespace=, defining_module=, yang_type=, is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          : ,
          : ""container"",
          : ,
        })

    self.__ipv6_phy_intf_cmds = t
    if hasattr(self, ):
      self._set()","Setter method for ipv6_phy_intf_cmds, mapped from YANG variable /interface/fortygigabitethernet/ipv6/ipv6_phy_intf_cmds (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_ipv6_phy_intf_cmds is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_ipv6_phy_intf_cmds() directly."
"def log_formatter(request=None):
    

    if request:
        format_str = (
                      
                      )
    else:
        format_str = 

    try:
        hostname = socket.gethostname()
    except socket.gaierror:
        hostname = 

    try:
        ip = socket.gethostbyname(hostname)
    except socket.gaierror:
        ip = 

    formatter = logging.Formatter(
            format_str.format(ip=ip, name=options.name, env=options.env),
            datefmt=)
    logging.Formatter.converter = time.gmtime

    return formatter","Log formatter used in our syslog

    :param request: a request object
    :returns: logging.Formatter"
"def resolve(self, _):
        
        if self.default_value == DUMMY_VALUE:
            if self.name in os.environ:
                return os.environ[self.name]
            else:
                raise VelException(f""Undefined environment variable: {self.name}"")
        else:
            return os.environ.get(self.name, self.default_value)",Resolve given variable
"def printTemporalMemory(tm, outFile):
  
  table = PrettyTable([""Parameter name"", ""Value"", ])

  table.add_row([""columnDimensions"", tm.getColumnDimensions()])
  table.add_row([""cellsPerColumn"", tm.getCellsPerColumn()])
  table.add_row([""activationThreshold"", tm.getActivationThreshold()])
  table.add_row([""minThreshold"", tm.getMinThreshold()])
  table.add_row([""maxNewSynapseCount"", tm.getMaxNewSynapseCount()])
  table.add_row([""permanenceIncrement"", tm.getPermanenceIncrement()])
  table.add_row([""permanenceDecrement"", tm.getPermanenceDecrement()])
  table.add_row([""initialPermanence"", tm.getInitialPermanence()])
  table.add_row([""connectedPermanence"", tm.getConnectedPermanence()])
  table.add_row([""predictedSegmentDecrement"", tm.getPredictedSegmentDecrement()])

  print >>outFile, table.get_string().encode(""utf-8"")","Given an instance of TemporalMemory, print out the relevant parameters"
"def confirm_key(name, confirm_name, trafaret):
    
    def check_(value):
        first, second = None, None
        if name in value:
            first = value[name]
        else:
            yield name, t.DataError(), (name,)
        if confirm_name in value:
            second = value[confirm_name]
        else:
            yield confirm_name, t.DataError(), (confirm_name,)
        if not (first and second):
            return
        yield name, t.catch_error(trafaret, first), (name,)
        yield confirm_name, t.catch_error(trafaret, second), (confirm_name,)
        if first != second:
            yield confirm_name, t.DataError(.format(name)), (confirm_name,)
    return check_","confirm_key - takes `name`, `confirm_name` and `trafaret`.

    Checks if data['name'] equals data['confirm_name'] and both
    are valid against `trafaret`."
"def dehydrate_datetime(value):
    

    def seconds_and_nanoseconds(dt):
        if isinstance(dt, datetime):
            dt = DateTime.from_native(dt)
        zone_epoch = DateTime(1970, 1, 1, tzinfo=dt.tzinfo)
        t = dt.to_clock_time() - zone_epoch.to_clock_time()
        return t.seconds, t.nanoseconds

    tz = value.tzinfo
    if tz is None:
        
        value = utc.localize(value)
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b""d"", seconds, nanoseconds)
    elif hasattr(tz, ""zone"") and tz.zone:
        
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b""f"", seconds, nanoseconds, tz.zone)
    else:
        
        seconds, nanoseconds = seconds_and_nanoseconds(value)
        return Structure(b""F"", seconds, nanoseconds, tz.utcoffset(value).seconds)","Dehydrator for `datetime` values.

    :param value:
    :type value: datetime
    :return:"
"def _traverse_unobserved(stream,negative_filter,of):
  
  observed = set()
  for line in stream:
    name = PacBioReadName(_nameprog.match(line).group(1))
    if name.get_molecule() not in negative_filter: of.write(line)
    observed.add(name.get_molecule())
  return negative_filter|observed",Go through a stream and print out anything not in observed set
"def inject_scheme(self, b16_scheme):
        
        
        
        content_lines = self.content.splitlines()
        b16_scheme_lines = b16_scheme.splitlines()
        start_line = None
        for num, line in enumerate(content_lines):
            if not start_line:
                match = TEMP_NEEDLE.match(line)
                if match:
                    start_line = num + 1
            else:
                match = TEMP_END_NEEDLE.match(line)
                if match:
                    end_line = num

        
        new_content_lines = (content_lines[0:start_line]
                             + b16_scheme_lines
                             + content_lines[end_line:])
        self.content = .join(new_content_lines)",Inject string $b16_scheme into self.content.
"def GetInput(self):
    
    client_list = GetAllClients(token=self.token)
    logging.debug(""Got %d clients"", len(client_list))
    for client_group in collection.Batch(client_list, self.client_chunksize):
      for fd in aff4.FACTORY.MultiOpen(
          client_group,
          mode=""r"",
          aff4_type=aff4_grr.VFSGRRClient,
          token=self.token):
        if isinstance(fd, aff4_grr.VFSGRRClient):
          
          oldest_time = (time.time() - self.max_age) * 1e6
        if fd.Get(aff4_grr.VFSGRRClient.SchemaCls.PING) >= oldest_time:
          yield fd",Yield client urns.
"def _PromptUserForEncryptedVolumeCredential(
      self, scan_context, locked_scan_node, output_writer):
    
    credentials = credentials_manager.CredentialsManager.GetCredentials(
        locked_scan_node.path_spec)

    
    if locked_scan_node.type_indicator == (
        definitions.TYPE_INDICATOR_APFS_CONTAINER):
      line = 
    elif locked_scan_node.type_indicator == definitions.TYPE_INDICATOR_BDE:
      line = 
    elif locked_scan_node.type_indicator == definitions.TYPE_INDICATOR_FVDE:
      line = 
    else:
      line = 

    output_writer.WriteLine(line)

    credentials_list = list(credentials.CREDENTIALS)
    credentials_list.append()

    
    output_writer.WriteLine()
    output_writer.WriteLine()
    for index, name in enumerate(credentials_list):
      output_writer.WriteLine(.format(index + 1, name))
    output_writer.WriteLine()

    result = False
    while not result:
      output_writer.WriteString(
          )
      
      input_line = sys.stdin.readline()
      input_line = input_line.strip()

      if input_line in credentials_list:
        credential_identifier = input_line
      else:
        try:
          credential_identifier = int(input_line, 10)
          credential_identifier = credentials_list[credential_identifier - 1]
        except (IndexError, ValueError):
          output_writer.WriteLine(
              .format(input_line))
          continue

      if credential_identifier == :
        break

      getpass_string = 
      if sys.platform.startswith() and sys.version_info[0] < 3:
        
        
        getpass_string = self._EncodeString(getpass_string)

      credential_data = getpass.getpass(getpass_string)
      output_writer.WriteLine()

      result = self._source_scanner.Unlock(
          scan_context, locked_scan_node.path_spec, credential_identifier,
          credential_data)

      if not result:
        output_writer.WriteLine()
        output_writer.WriteLine()","Prompts the user to provide a credential for an encrypted volume.

    Args:
      scan_context (SourceScannerContext): the source scanner context.
      locked_scan_node (SourceScanNode): the locked scan node.
      output_writer (StdoutWriter): the output writer."
"def main(using_locale=(, ,)):
    
    all_ucs = (ucs for ucs in
               [unichr(val) for val in range(sys.maxunicode)]
               if is_named(ucs) and isnt_combining(ucs))

    libc_name = ctypes.util.find_library()
    if not libc_name:
        raise ImportError(""Canwcwidthwcswidth', None) is not None

    locale.setlocale(locale.LC_ALL, using_locale)

    for ucs in all_ucs:
        try:
            _is_equal_wcwidth(libc, ucs)
        except AssertionError as err:
            print(err)","Program entry point.

    Load the entire Unicode table into memory, excluding those that:

        - are not named (func unicodedata.name returns empty string),
        - are combining characters.

    Using ``locale``, for each unicode character string compare libc's
    wcwidth with local wcwidth.wcwidth() function; when they differ,
    report a detailed AssertionError to stdout."
"def get_submodules(module):
    
    if not inspect.ismodule(module):
        raise RuntimeError(
            
            )

    submodules = get_members(module, inspect.ismodule)

    module_path = list(getattr(module, , [None]))[0]

    if module_path is not None:
        for item in listdir(module_path):
            module_name = extract_module_name(osp.join(module_path, item))

            if module_name is not None:
                try:
                    submodules[module_name] = importlib.import_module(
                        .format(module_name), package=module.__name__)
                except ImportError:
                    
                    
                    pass

    return submodules","This function imports all sub-modules of the supplied module and returns a dictionary
    with module names as keys and the sub-module objects as values. If the supplied parameter
    is not a module object, a RuntimeError is raised.

    :param module: Module object from which to import sub-modules.
    :return: Dict with name-module pairs."
"def build(self, pre=None, shortest=False):
        
        if pre is None:
            pre = []

        if self.value is not None and rand.maybe():
            return utils.val(self.value, pre, shortest=shortest)

        length = super(String, self).build(pre, shortest=shortest)
        res = rand.data(length, self.charset)
        return res","Build the String instance

        :param list pre: The prerequisites list (optional, default=None)
        :param bool shortest: Whether or not the shortest reference-chain (most minimal) version of the field should be generated."
"def assess(model, reaction, flux_coefficient_cutoff=0.001, solver=None):
    
    reaction = model.reactions.get_by_any(reaction)[0]
    with model as m:
        m.objective = reaction
        if _optimize_or_value(m, solver=solver) >= flux_coefficient_cutoff:
            return True
        else:
            results = dict()
            results[] = assess_component(
                model, reaction, , flux_coefficient_cutoff)
            results[] = assess_component(
                model, reaction, , flux_coefficient_cutoff)
            return results","Assesses production capacity.

    Assesses the capacity of the model to produce the precursors for the
    reaction and absorb the production of the reaction while the reaction is
    operating at, or above, the specified cutoff.

    Parameters
    ----------
    model : cobra.Model
        The cobra model to assess production capacity for

    reaction : reaction identifier or cobra.Reaction
        The reaction to assess

    flux_coefficient_cutoff :  float
        The minimum flux that reaction must carry to be considered active.

    solver : basestring
        Solver name. If None, the default solver will be used.

    Returns
    -------
    bool or dict
        True if the model can produce the precursors and absorb the products
        for the reaction operating at, or above, flux_coefficient_cutoff.
        Otherwise, a dictionary of {'precursor': Status, 'product': Status}.
        Where Status is the results from assess_precursors and
        assess_products, respectively."
"def set_archive_layout_url(self, archive_id):
        
        url = self.api_url +  + self.api_key +  + archive_id + 
        return url",this method returns the url to set the archive layout
"def has_mixture(val: Any) -> bool:
    
    getter = getattr(val, , None)
    result = NotImplemented if getter is None else getter()
    if result is not NotImplemented:
        return result

    
    return mixture(val, None) is not None","Returns whether the value has a mixture representation.

    Returns:
        If `val` has a `_has_mixture_` method and its result is not
        NotImplemented, that result is returned. Otherwise, if the value
        has a `_mixture_` method return True if that has a non-default value.
        Returns False if neither function exists."
"def apt_add_repository_from_apt_string(apt_string, apt_file):
    

    apt_file_path =  % apt_file

    if not file_contains(apt_file_path, apt_string.lower(), use_sudo=True):
        file_append(apt_file_path, apt_string.lower(), use_sudo=True)

        with hide(, ):
            sudo(""DEBIAN_FRONTEND=noninteractive apt-get update"")",adds a new repository file for apt
"def linkify_contactgroups_contacts(self, contacts):
        
        for contactgroup in self:
            mbrs = contactgroup.get_contacts()

            
            new_mbrs = []
            for mbr in mbrs:
                mbr = mbr.strip()  
                    continue
                member = contacts.find_by_name(mbr)
                
                if member is not None:
                    new_mbrs.append(member.uuid)
                else:
                    contactgroup.add_unknown_members(mbr)

            
            new_mbrs = list(set(new_mbrs))

            
            contactgroup.replace_members(new_mbrs)","Link the contacts with contactgroups

        :param contacts: realms object to link with
        :type contacts: alignak.objects.contact.Contacts
        :return: None"
"def sag_entropic_transport(a, b, M, reg, numItermax=10000, lr=None):
    

    if lr is None:
        lr = 1. / max(a / reg)
    n_source = np.shape(M)[0]
    n_target = np.shape(M)[1]
    cur_beta = np.zeros(n_target)
    stored_gradient = np.zeros((n_source, n_target))
    sum_stored_gradient = np.zeros(n_target)
    for _ in range(numItermax):
        i = np.random.randint(n_source)
        cur_coord_grad = a[i] * coordinate_grad_semi_dual(b, M, reg,
                                                          cur_beta, i)
        sum_stored_gradient += (cur_coord_grad - stored_gradient[i])
        stored_gradient[i] = cur_coord_grad
        cur_beta += lr * (1. / n_source) * sum_stored_gradient
    return cur_beta","Compute the SAG algorithm to solve the regularized discrete measures
        optimal transport max problem

    The function solves the following optimization problem:

    .. math::
        \gamma = arg\min_\gamma <\gamma,M>_F + reg\cdot\Omega(\gamma)

        s.t. \gamma 1 = a

             \gamma^T 1 = b

             \gamma \geq 0

    Where :

    - M is the (ns,nt) metric cost matrix
    - :math:`\Omega` is the entropic regularization term with :math:`\Omega(\gamma)=\sum_{i,j} \gamma_{i,j}\log(\gamma_{i,j})`
    - a and b are source and target weights (sum to 1)

    The algorithm used for solving the problem is the SAG algorithm
    as proposed in [18]_ [alg.1]


    Parameters
    ----------

    a : np.ndarray(ns,),
        source measure
    b : np.ndarray(nt,),
        target measure
    M : np.ndarray(ns, nt),
        cost matrix
    reg : float number,
        Regularization term > 0
    numItermax : int number
        number of iteration
    lr : float number
        learning rate

    Returns
    -------

    v : np.ndarray(nt,)
        dual variable

    Examples
    --------

    >>> n_source = 7
    >>> n_target = 4
    >>> reg = 1
    >>> numItermax = 300000
    >>> a = ot.utils.unif(n_source)
    >>> b = ot.utils.unif(n_target)
    >>> rng = np.random.RandomState(0)
    >>> X_source = rng.randn(n_source, 2)
    >>> Y_target = rng.randn(n_target, 2)
    >>> M = ot.dist(X_source, Y_target)
    >>> method = ""ASGD""
    >>> asgd_pi = stochastic.solve_semi_dual_entropic(a, b, M, reg,
                                                      method, numItermax)
    >>> print(asgd_pi)

    References
    ----------

    [Genevay et al., 2016] :
                    Stochastic Optimization for Large-scale Optimal Transport,
                     Advances in Neural Information Processing Systems (2016),
                      arXiv preprint arxiv:1605.08527."
"def count_levels(value):
    
    if not isinstance(value, dict) or len(value) == 0:
        return 0
    elif len(value) == 0:
        return 0 
    else:
        nextval = list(value.values())[0]
        return 1 + count_levels(nextval)","Count how many levels are in a dict:
        scalar, list etc = 0
        {} = 0
        {'a':1} = 1
        {'a' : {'b' : 1}} = 2
        etc..."
"def cmd_logcat(self, *args):
        
        self.check_requirements()
        serial = self.serials[0:]
        if not serial:
            return
        filters = self.buildozer.config.getrawdefault(
            ""app"", ""android.logcat_filters"", """", section_sep="":"", split_char="" "")
        filters = "" "".join(filters)
        self.buildozer.environ[] = serial[0]
        self.buildozer.cmd(.format(adb=self.adb_cmd,
                                                           filters=filters),
                           cwd=self.buildozer.global_platform_dir,
                           show_output=True)
        self.buildozer.environ.pop(, None)",Show the log from the device
"def cmd_daemon(opts):
    
    if opts.data_dir is None:
        raise BlockadeError(""You must supply a data directory for the daemon"")
    rest.start(data_dir=opts.data_dir, port=opts.port, debug=opts.debug,
        host_exec=get_host_exec())",Start the Blockade REST API
"def lat_to_deg(lat):
    
    if isinstance(lat, str) and ( in lat):
        
        lat_deg = dmsStrToDeg(lat)
    else:
        lat_deg = float(lat)
    return lat_deg",Convert latitude to degrees.
"def pastdate(self, prompt, default=None):
        
        prompt = prompt if prompt is not None else ""Enter a past date""
        if default is not None:
            prompt += "" ["" + default.strftime() + ""]""
        prompt += 
        return self.input(curry(filter_pastdate, default=default), prompt)",Prompts user to input a date in the past.
"def children(self):
        
        children = []
        child_nodes = getattr(self.element, )
        for child in getattr(child_nodes, , []):
                children.append(AmazonBrowseNode(child))
        return children","This browse node's children in the browse node tree.

    :return:
    A list of this browse node's children in the browse node tree."
"def columnCount(self, qindex=QModelIndex()):
        
        if self.total_cols <= self.cols_loaded:
            return self.total_cols
        else:
            return self.cols_loaded",Array column number
"def maxlike(self,nseeds=50):
        
        mA_0,age0,feh0 = self.ic.random_points(nseeds)
        mB_0,foo1,foo2 = self.ic.random_points(nseeds)
        mC_0,foo3,foo4 = self.ic.random_points(nseeds)
        m_all = np.sort(np.array([mA_0, mB_0, mC_0]), axis=0)
        mA_0, mB_0, mC_0 = (m_all[0,:], m_all[1,:], m_all[2,:])

        d0 = 10**(rand.uniform(0,np.log10(self.max_distance),size=nseeds))
        AV0 = rand.uniform(0,self.maxAV,size=nseeds)

        

        costs = np.zeros(nseeds)

        if self.fit_for_distance:
            pfits = np.zeros((nseeds,7))
        else:
            pfits = np.zeros((nseeds,5))
            
        def fn(p): 
            return -1*self.lnpost(p)
        
        for i,mA,mB,mC,age,feh,d,AV in zip(range(nseeds),
                                    mA_0,mB_0,mC_0,age0,feh0,d0,AV0):
                if self.fit_for_distance:
                    pfit = scipy.optimize.fmin(fn,[mA,mB,mC,age,feh,d,AV],disp=False)
                else:
                    pfit = scipy.optimize.fmin(fn,[mA,mB,mC,age,feh],disp=False)
                pfits[i,:] = pfit
                costs[i] = self.lnpost(pfit)

        return pfits[np.argmax(costs),:]","Returns the best-fit parameters, choosing the best of multiple starting guesses

        :param nseeds: (optional)
            Number of starting guesses, uniformly distributed throughout
            allowed ranges.  Default=50.

        :return:
            list of best-fit parameters: ``[mA,mB,age,feh,[distance,A_V]]``.
            Note that distance and A_V values will be meaningless unless
            magnitudes are present in ``self.properties``."
"def get_value_from_tag(self, tag, attribute):
        

        
        
        value = tag.get(self._ns(attribute))
        if value is None:
            value = tag.get(attribute)

            if value:
                
                log.warning(""Failed to get the attribute  on tag  with namespace. ""
                            ""But found the same attribute without namespace!"".format(attribute, tag.tag))
        return value","Return the value of the android prefixed attribute in a specific tag.

        This function will always try to get the attribute with a android: prefix first,
        and will try to return the attribute without the prefix, if the attribute could not be found.
        This is useful for some broken AndroidManifest.xml, where no android namespace is set,
        but could also indicate malicious activity (i.e. wrongly repackaged files).
        A warning is printed if the attribute is found without a namespace prefix.

        If you require to get the exact result you need to query the tag directly:

        example::
            >>> from lxml.etree import Element
            >>> tag = Element('bar', nsmap={'android': 'http://schemas.android.com/apk/res/android'})
            >>> tag.set('{http://schemas.android.com/apk/res/android}foobar', 'barfoo')
            >>> tag.set('name', 'baz')
            # Assume that `a` is some APK object
            >>> a.get_value_from_tag(tag, 'name')
            'baz'
            >>> tag.get('name')
            'baz'
            >>> tag.get('foobar')
            None
            >>> a.get_value_from_tag(tag, 'foobar')
            'barfoo'

        :param lxml.etree.Element tag: specify the tag element
        :param str attribute: specify the attribute name
        :returns: the attribute's value, or None if the attribute is not present"
"def _fix_uncontracted(basis):
    

    for el in basis[].values():
        if  not in el:
            continue

        for sh in el[]:
            if len(sh[]) == 1 and len(sh[][0]) == 1:
                sh[][0][0] = 

            

    return basis",Forces the contraction coefficient of uncontracted shells to 1.0
"def _update_labels(self, label, crop_box, height, width):
        
        xmin = float(crop_box[0]) / width
        ymin = float(crop_box[1]) / height
        w = float(crop_box[2]) / width
        h = float(crop_box[3]) / height
        out = label.copy()
        out[:, (1, 3)] -= xmin
        out[:, (2, 4)] -= ymin
        out[:, (1, 3)] /= w
        out[:, (2, 4)] /= h
        out[:, 1:5] = np.maximum(0, out[:, 1:5])
        out[:, 1:5] = np.minimum(1, out[:, 1:5])
        coverage = self._calculate_areas(out[:, 1:]) * w * h / self._calculate_areas(label[:, 1:])
        valid = np.logical_and(out[:, 3] > out[:, 1], out[:, 4] > out[:, 2])
        valid = np.logical_and(valid, coverage > self.min_eject_coverage)
        valid = np.where(valid)[0]
        if valid.size < 1:
            return None
        out = out[valid, :]
        return out",Convert labels according to crop box
"def make_pilothole_cutter(self):
        
        
        
        
            .circle(pilothole_radius) \
            .extrude(self.length)",Make a solid to subtract from an interfacing solid to bore a pilot-hole.
"def force_ascii_values(data):
    
    return {
        k: v.encode().decode(, )
        for k, v in data.items()
    }",Ensures each value is ascii-only
"def start_timer(self, timer_id, timeout):
        
        self._logger.debug(.format(timer_id, self.id))
        self._driver._start_timer(timer_id, timeout, self)","Start a timer or restart an active one.

        The timeout is given in milliseconds. If a timer with the
        same name already exists, it is restarted with the specified timeout.
        Note that the timeout is intended as the minimum time until the timer's
        expiration, but may vary due to the state of the event queue and the
        load of the system."
"def wage(return_X_y=True):
    
    
    
    wage = pd.read_csv(PATH + , index_col=0)
    if return_X_y:
        X = wage[[, , ]].values
        X[:,-1] = np.unique(X[:,-1], return_inverse=True)[1]
        y = wage[].values
        return _clean_X_y(X, y)
    return wage","wage dataset

    Parameters
    ----------
    return_X_y : bool,
        if True, returns a model-ready tuple of data (X, y)
        otherwise, returns a Pandas DataFrame

    Returns
    -------
    model-ready tuple of data (X, y)
        OR
    Pandas DataFrame

    Notes
    -----
    X contains the year, age and education of each sampled person.
    The education category has been transformed to integers.

    y contains the wage.

    Source:
    https://github.com/JWarmenhoven/ISLR-python/blob/master/Notebooks/Data/Wage.csv"
"def decode_param(data):
    
    logger.debug(, data)
    header_len = struct.calcsize()
    partype, parlen = struct.unpack(, data[:header_len])

    pardata = data[header_len:parlen]
    logger.debug(, pardata)

    ret = {
        : partype,
    }

    if partype == 1023:
        vsfmt = 
        vendor, subtype = struct.unpack(vsfmt, pardata[:struct.calcsize(vsfmt)])
        ret[] = vendor
        ret[] = subtype
        ret[] = pardata[struct.calcsize(vsfmt):]
    else:
        ret[] = pardata,

    return ret, data[parlen:]","Decode any parameter to a byte sequence.

    :param data: byte sequence representing an LLRP parameter.
    :returns dict, bytes: where dict is {'Type': <decoded type>, 'Data':
        <decoded data>} and bytes is the remaining bytes trailing the bytes we
        could decode."
"def _html_find_urls(bytes, mimetype, base_url=None):
    

    def _recursive_tag_values(tag, values=[]):
        

        if hasattr(tag, ):
            for child in tag.children:
                if hasattr(child, ):
                    for key in child.attrs:
                        if isinstance(child.attrs[key], list):
                            for value in child.attrs[key]:
                                values.append(value)
                        elif isinstance(child.attrs[key], str):
                            values.append(child.attrs[key])

                    values = _recursive_tag_values(child, values)

        return values

    
    ascii_bytes = b.join(re.compile(b).findall(bytes))
    ascii_bytes = ascii_bytes.replace(b, b)

    
    urls = []

    
    soups = []
    soups.append(BeautifulSoup(ascii_bytes, ))
    try:
        soups.append(BeautifulSoup(urllib.parse.unquote(str(ascii_bytes)), ))
    except:
        pass

    
    for soup in soups:

        
        
        script_urls = []
        script_tags = soup.find_all()
        for script_tag in script_tags:
            for tag_content in script_tag.contents:
                if  in tag_content.lower():
                    
                    
                    
                    code_begin = tag_content.rfind()
                    code_end = tag_content.find()
                    code = tag_content[code_begin+1:code_end]
                    
        
        else:
            urls = _recursive_tag_values(soup)
            urls += css_urls
            urls += meta_urls
            urls += script_urls

        
        
        
        
        for s in soup.stripped_strings:
            if is_valid(s):
                urls.append(s)

        
        
        
        
        urls = [u.strip() for u in urls]

        

    return urls",This function finds URLs inside of valid HTML bytes.
"def sqllab(self):
        
        d = {
            : config.get(),
            : self.common_bootsrap_payload(),
        }
        return self.render_template(
            ,
            entry=,
            bootstrap_data=json.dumps(d, default=utils.json_iso_dttm_ser),
        )",SQL Editor
"def plot_lifetimes(
    durations,
    event_observed=None,
    entry=None,
    left_truncated=False,
    sort_by_duration=True,
    event_observed_color=""
    event_censored_color=""
    **kwargs
):
    
    set_kwargs_ax(kwargs)
    ax = kwargs.pop(""ax"")

    N = durations.shape[0]
    if N > 80:
        warnings.warn(""For less visual clutter, you may want to subsample to less than 80 individuals."")

    if event_observed is None:
        event_observed = np.ones(N, dtype=bool)

    if entry is None:
        entry = np.zeros(N)

    assert durations.shape[0] == N
    assert event_observed.shape[0] == N

    if sort_by_duration:
        
        ix = np.argsort(entry + durations, 0)
        durations = durations[ix]
        event_observed = event_observed[ix]
        entry = entry[ix]

    for i in range(N):
        c = event_observed_color if event_observed[i] else event_censored_color
        ax.hlines(i, entry[i], entry[i] + durations[i], color=c, lw=1.5)
        if left_truncated:
            ax.hlines(i, 0, entry[i], color=c, lw=1.0, linestyle=""--"")
        m = """" if not event_observed[i] else ""o""
        ax.scatter(entry[i] + durations[i], i, color=c, marker=m, s=10)

    ax.set_ylim(-0.5, N)
    return ax","Returns a lifetime plot, see examples: https://lifelines.readthedocs.io/en/latest/Survival%20Analysis%20intro.html#Censoring

    Parameters
    -----------
    durations: (n,) numpy array or pd.Series
      duration subject was observed for.
    event_observed: (n,) numpy array or pd.Series
      array of booleans: True if event observed, else False.
    entry: (n,) numpy array or pd.Series
      offsetting the births away from t=0. This could be from left-truncation, or delayed entry into study.
    left_truncated: boolean
      if entry is provided, and the data is left-truncated, this will display additional information in the plot to reflect this.
    sort_by_duration: boolean
      sort by the duration vector
    event_observed_color: str
      default: ""#A60628""
    event_censored_color: str
      default: ""#348ABD""

    Returns
    -------
    ax

    Examples
    ---------
    >>> from lifelines.datasets import load_waltons
    >>> from lifelines.plotting import plot_lifetimes
    >>> T, E = load_waltons()[""T""], load_waltons()[""E""]
    >>> ax = plot_lifetimes(T.loc[:50], event_observed=E.loc[:50])"
"def key_value(self, **kwargs):
        
        field_name = self.name
        new_df = copy_df(self)
        new_df._perform_operation(op.FieldKVConfigOperation({field_name: KVConfig(**kwargs)}))
        return new_df","Set fields to be key-value represented.

        :rtype: Column

        :Example:

        >>> new_ds = df.key_value('f1 f2', kv=':', item=',')"
"def wrap_arguments(args=None):
        
        if args is None:
            args = []

        tags = []
        for name, value in args:
            tag = ""<{name}>{value}</{name}>"".format(
                name=name, value=escape(""%s"" % value, {: ""&quot;""}))
            
            
            tags.append(tag)

        xml = """".join(tags)
        return xml","Wrap a list of tuples in xml ready to pass into a SOAP request.

        Args:
            args (list):  a list of (name, value) tuples specifying the
                name of each argument and its value, eg
                ``[('InstanceID', 0), ('Speed', 1)]``. The value
                can be a string or something with a string representation. The
                arguments are escaped and wrapped in <name> and <value> tags.

        Example:

            >>> from soco import SoCo
            >>> device = SoCo('192.168.1.101')
            >>> s = Service(device)
            >>> print(s.wrap_arguments([('InstanceID', 0), ('Speed', 1)]))
            <InstanceID>0</InstanceID><Speed>1</Speed>'"
"def creating_schema_and_index(self, models, func):
        
        waiting_models = []
        self.base_thread.do_with_submit(func, models, waiting_models, threads=self.threads)
        if waiting_models:
            print(""WAITING MODELS ARE CHECKING..."")
            self.creating_schema_and_index(waiting_models, func)","Executes given functions with given models.

        Args:
            models: models to execute
            func: function name to execute

        Returns:"
"def hydrate_callable_with_edge_node_map(
            self,
            edge_node_map,
            callable_function,
            parameter_lambda
        ):
        

        def extract_kwargs_dict(*args, **kwargs):
            return kwargs

        def extract_args_list(*args, **kwargs):
            return list(args)

        args = parameter_lambda(extract_args_list)
        kwargs = parameter_lambda(extract_kwargs_dict)

        arg_list = [edge_node_map[node_id] for node_id in list(args)]

        kwarg_map = {}

        for kwarg in kwargs:
            kwarg_map[kwarg] = edge_node_map[kwargs[kwarg]]

        return callable_function(*arg_list, **kwarg_map)",args and kwargs intentionally not *args and **kwargs
"def _ParseShellItem(self, parser_mediator, shell_item):
    
    path_segment = self._ParseShellItemPathSegment(shell_item)
    self._path_segments.append(path_segment)

    event_data = shell_item_events.ShellItemFileEntryEventData()
    event_data.origin = self._origin
    event_data.shell_item_path = self.CopyToPath()

    if isinstance(shell_item, pyfwsi.file_entry):
      event_data.name = shell_item.name

      for extension_block in shell_item.extension_blocks:
        if isinstance(extension_block, pyfwsi.file_entry_extension):
          long_name = extension_block.long_name
          localized_name = extension_block.localized_name
          file_reference = extension_block.file_reference
          if file_reference:
            file_reference = .format(
                file_reference & 0xffffffffffff, file_reference >> 48)

          event_data.file_reference = file_reference
          event_data.localized_name = localized_name
          event_data.long_name = long_name

          fat_date_time = extension_block.get_creation_time_as_integer()
          if fat_date_time != 0:
            date_time = dfdatetime_fat_date_time.FATDateTime(
                fat_date_time=fat_date_time)
            event = time_events.DateTimeValuesEvent(
                date_time, definitions.TIME_DESCRIPTION_CREATION)
            parser_mediator.ProduceEventWithEventData(event, event_data)

          fat_date_time = extension_block.get_access_time_as_integer()
          if fat_date_time != 0:
            date_time = dfdatetime_fat_date_time.FATDateTime(
                fat_date_time=fat_date_time)
            event = time_events.DateTimeValuesEvent(
                date_time, definitions.TIME_DESCRIPTION_LAST_ACCESS)
            parser_mediator.ProduceEventWithEventData(event, event_data)

      fat_date_time = shell_item.get_modification_time_as_integer()
      if fat_date_time != 0:
        date_time = dfdatetime_fat_date_time.FATDateTime(
            fat_date_time=fat_date_time)
        event = time_events.DateTimeValuesEvent(
            date_time, definitions.TIME_DESCRIPTION_MODIFICATION)
        parser_mediator.ProduceEventWithEventData(event, event_data)","Parses a shell item.

    Args:
      parser_mediator (ParserMediator): mediates interactions between parsers
          and other components, such as storage and dfvfs.
      shell_item (pyfwsi.item): shell item."
"def _get_rev_num(self, rev=None):
		
		
		cmd = [, ]
		
		cmd.extend([, ])
		if rev:
			cmd.extend([, rev])
		res = self._invoke(*cmd)
		return res.strip()",Determine the revision number for a given revision specifier.
"def solve(self, verbose=False, allow_brute_force=True):
        
        while not self.is_solved:
            
            self._update()

            
            singles_found = False or self._fill_naked_singles() or self._fill_hidden_singles()

            
            
            
            if not singles_found:
                if allow_brute_force:
                    solution = None
                    try:
                        dlxs = DancingLinksSolver(copy.deepcopy(self._matrix))
                        solutions = dlxs.solve()
                        solution = next(solutions)
                        more_solutions = next(solutions)
                    except StopIteration as e:
                        if solution is not None:
                            self._matrix = solution
                        else:
                            raise SudokuHasNoSolutionError(""Dancing Links solver could not find any solution."")
                    except Exception as e:
                        raise SudokuHasNoSolutionError(""Brute Force method failed."")
                    else:
                        
                        
                        raise SudokuHasMultipleSolutionsError(""This Sudoku has multiple solutions!"")
                    self.solution_steps.append(""BRUTE FORCE - Dancing Links"")
                    break
                else:
                    print(self)
                    raise SudokuTooDifficultError(""This Sudoku requires more advanced methods!"")
        if verbose:
            print(""Sudoku solved in {0} iterations!\n{1}"".format(len(self.solution_steps), self))
            for step in self.solution_steps:
                print(step)","Solve the Sudoku.

        :param verbose: If the steps used for solving the Sudoku
                        should be printed. Default is `False`
        :type verbose: bool
        :param allow_brute_force: If Dancing Links Brute Force method
                                  should be used if necessary. Default is `True`
        :type allow_brute_force: bool"
"def update_allocated_node_name(self, base_name):
        

        if base_name is None:
            return None
        base_name = re.sub(r""[ ]"", """", base_name)
        if base_name in self._allocated_node_names:
            base_name = re.sub(r""[0-9]+$"", ""{0}"", base_name)

        if  in base_name or  in base_name:
            
            for number in range(1, 1000000):
                try:
                    name = base_name.format(number, id=number, name=""Node"")
                except KeyError as e:
                    raise aiohttp.web.HTTPConflict(text=""{"" + e.args[0] + ""} is not a valid replacement string in the node name"")
                except (ValueError, IndexError) as e:
                    raise aiohttp.web.HTTPConflict(text=""{} is not a valid replacement string in the node name"".format(base_name))
                if name not in self._allocated_node_names:
                    self._allocated_node_names.add(name)
                    return name
        else:
            if base_name not in self._allocated_node_names:
                self._allocated_node_names.add(base_name)
                return base_name
            
            for number in range(1, 1000000):
                name = base_name + str(number)
                if name not in self._allocated_node_names:
                    self._allocated_node_names.add(name)
                    return name
        raise aiohttp.web.HTTPConflict(text=""A node name could not be allocated (node limit reached?)"")","Updates a node name or generate a new if no node
        name is available.

        :param base_name: new node base name"
"def arch():
    
    with settings(hide(, , , ),
                  warn_only=True, capture=True):
        result = sudo().strip()
    return result",returns the current cpu archictecture
"def file_fingerprint(fullpath):
    
    stat = os.stat(fullpath)
    return .join([str(value) for value in [stat.st_ino, stat.st_mtime, stat.st_size] if value])",Get a metadata fingerprint for a file
"def maximum_likelihood(Ms, Mu, Mus, Mss, fit_offset=False, fit_offset2=False):
    
    n_obs, n_var = Ms.shape
    offset, offset_ss = np.zeros(n_var, dtype=""float32""), np.zeros(n_var, dtype=""float32"")
    gamma = np.ones(n_var, dtype=""float32"")

    def sse(A, data, b):
        sigma = (A.dot(data) - b).std(1)
        return np.log(sigma).sum()  

    if fit_offset and fit_offset2:
        for i in range(n_var):
            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))
            offset[i], offset_ss[i], gamma[i] = \
                minimize(lambda m: sse(np.array([[1, -m[2], 0, 0], [1, m[2], 2, -2 * m[2]]]),
                                       data, b=np.array(m[0], m[1])), x0=(1e-4, 1e-4, 1), method=""L-BFGS-B"").x
    elif fit_offset:
        for i in range(n_var):
            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))
            offset[i], gamma[i] = \
                minimize(lambda m: sse(np.array([[1, -m[1], 0, 0], [1, m[1], 2, -2 * m[1]]]),
                                       data, b=np.array(m[0], 0)), x0=(1e-4, 1), method=""L-BFGS-B"").x
    elif fit_offset2:
        for i in range(n_var):
            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))
            offset_ss[i], gamma[i] = \
                minimize(lambda m: sse(np.array([[1, -m[1], 0, 0], [1, m[1], 2, -2 * m[1]]]),
                                       data, b=np.array(0, m[0])), x0=(1e-4, 1), method=""L-BFGS-B"").x
    else:
        for i in range(n_var):
            data = np.vstack((Mu[:, i], Ms[:, i], Mus[:, i], Mss[:, i]))
            gamma[i] = \
                minimize(lambda m: sse(np.array([[1, -m, 0, 0], [1, m, 2, -2 * m]]), data, b=0),
                         x0=gamma[i], method=""L-BFGS-B"").x
    return offset, offset_ss, gamma",Maximizing the log likelihood using weights according to empirical bayes
"def _seconds_to_section_split(record, sections):
    

    next_section = sections[
        bisect_right(sections, _find_weektime(record.datetime))] * 60
    return next_section - _find_weektime(record.datetime, time_type=)",Finds the seconds to the next section from the datetime of a record.
"def cumulative_distribution(self, X):
        
        self.check_fit()

        U, V = self.split_matrix(X)

        num = np.multiply(
            np.exp(np.multiply(-self.theta, U)) - 1,
            np.exp(np.multiply(-self.theta, V)) - 1
        )
        den = np.exp(-self.theta) - 1

        return -1.0 / self.theta * np.log(1 + num / den)","Computes the cumulative distribution function for the copula, :math:`C(u, v)`

        Args:
            X: `np.ndarray`

        Returns:
            np.array: cumulative distribution"
"def create(cls, name, abr_type=, auto_cost_bandwidth=100,
               deprecated_algorithm=False, initial_delay=200,
               initial_hold_time=1000, max_hold_time=10000,
               shutdown_max_metric_lsa=0, startup_max_metric_lsa=0):
        
        json = {: name,
                : abr_type,
                : auto_cost_bandwidth,
                : deprecated_algorithm,
                : initial_delay,
                : initial_hold_time,
                : max_hold_time,
                : shutdown_max_metric_lsa,
                : startup_max_metric_lsa}

        return ElementCreator(cls, json)","Create custom Domain Settings

        Domain settings are referenced by an OSPFProfile

        :param str name: name of custom domain settings
        :param str abr_type: cisco|shortcut|standard
        :param int auto_cost_bandwidth: Mbits/s
        :param bool deprecated_algorithm: RFC 1518 compatibility
        :param int initial_delay: in milliseconds
        :param int initial_hold_type: in milliseconds
        :param int max_hold_time: in milliseconds
        :param int shutdown_max_metric_lsa: in seconds
        :param int startup_max_metric_lsa: in seconds
        :raises CreateElementFailed: create failed with reason
        :return: instance with meta
        :rtype: OSPFDomainSetting"
"def _effective_path(self):
        
        
        
        path = ([ % (self.group_name, self.app_name)] +
                self.get_xdg_dirs() +
                [expanduser( % (self.group_name, self.app_name)),
                 self.get_xdg_home(),
                 join(getcwd(), .format(self.group_name), self.app_name)])

        
        if self.search_path:
            path = self.search_path.split(pathsep)

        
        env_path = getenv(self.env_path_name)

        if env_path and env_path.startswith():
            
            additional_paths = env_path[1:].split(pathsep)
            self._log.info(
                           ,
                           additional_paths,
                           self.env_path_name)
            path.extend(additional_paths)
        elif env_path:
            
            self._log.info(""Configuration search path was overridden with ""
                           ""%r by the environment variable %r."",
                           env_path,
                           self.env_path_name)
            path = env_path.split(pathsep)

        return path","Returns a list of paths to search for config files in reverse order of
        precedence.  In other words: the last path element will override the
        settings from the first one."
"def clear(self, key=None):
        
        if not self.options.enabled:
            return CACHE_DISABLED
        logger.debug(.format(repr(key)))
        if key is not None and key in self._dict.keys():
            del self._dict[key]
            logger.info( + repr(key))
        elif not key:
            for cached_key in [k for k in self._dict.keys()]:
                del self._dict[cached_key]
            logger.info()
        return True","Clear a cache entry, or the entire cache if no key is given

        Returns CACHE_DISABLED if the cache is disabled
        Returns True on successful operation

        :param key: optional key to limit the clear operation to (defaults to None)"
"def _build_literal(self, nm, clsdata):
      
      cls = type(str(nm), tuple((LiteralValue,)), {
        : {
            : clsdata,
            : clsdata.get(),
            : clsdata.get()}
        })

      return cls","@todo: Docstring for _build_literal

      :nm: @todo
      :clsdata: @todo
      :returns: @todo"
"def rank_pv(self,dt,dt2,memlimit=-1):
	
	if self.lib is None:
		raise ValueError(""Not initialized."")
	import numpy as np
	from .auto import ftype_np,gtype_np
	from .types import isint
	if dt.dtype.char!=ftype_np or dt2.dtype.char!=ftype_np:
		raise ValueError()
	if len(dt.shape)!=2 or len(dt2.shape)!=2:
		raise ValueError()
	if not isint(memlimit):
		raise ValueError()
	ng=dt.shape[0]
	nt=dt2.shape[0]
	ns=dt.shape[1]
	
	if dt2.shape[1]!=ns:
		raise ValueError()
	if np.isnan(dt).sum()+np.isnan(dt2).sum()>0:
		raise ValueError()

	dp=np.require(np.zeros((ng,nt),dtype=dt.dtype),requirements=[,,,])
	dtr=np.require(dt,requirements=[,,,])
	dt2r=np.require(dt2,requirements=[,,,])
	arglist=[,,,]
	args=[dtr,dt2r,dp,memlimit]
	func=self.cfunc(,rettype=,argtypes=arglist)
	ret=func(*args)
	ans={:ret,:dp}
	return ans","Calculates p-values of gene i correlating with gene j by converting log likelihoods into probabilities per A for all B.
	dt:	numpy.ndarray(nt,ns,dtype=ftype(='=f4' by default)) Gene expression data for A
		Entry dt[i,j] is gene i's expression level for sample j.
	dt2:numpy.ndarray(nt2,ns,dtype=ftype(='=f4' by default)) Gene expression data for B.
		dt2 has the same format as dt, and can be identical with, different from, a subset of, or a superset of dt.
	memlimit:	The approximate memory usage limit in bytes for the library.  For datasets require a larger memory, calculation will fail with an error message. memlimit=0 defaults to unlimited memory usage.
	Return:	dictionary with following keys:
	ret:0 iff execution succeeded.
	p:	numpy.ndarray((nt,nt2),dtype=ftype(='=f4' by default)). P-values for A--B.
	ftype and gtype can be found in auto.py.
	
	Example: see findr.examples.geuvadis1 (similar format)"
"def late_filling(target, pressure=,
                 Pc_star=,
                 Swp_star=0.2, eta=3):
    r
    element = pressure.split()[0]
    network = target.project.network
    phase = target.project.find_phase(target)
    pc_star = phase[Pc_star]
    Pc = phase[pressure]
    
        Ts = network.map_throats(throats=target.Ts, origin=target)
        values = values[Ts]
    else:
        Ps = network.map_pores(pores=target.Ps, origin=target)
        values = values[Ps]
    return values","r""""""
    Calculates the fraction of a pore or throat filled with invading fluid
    based on the capillary pressure in the invading phase.  The invading phase
    volume is calculated from:

        .. math::
            S_{nwp} = 1 - S_{wp}^{*} (P^{*}/P_{c})^{\eta}

    Parameters
    ----------
    pressure : string
        The capillary pressure in the non-wetting phase (Pc > 0).

    Pc_star : string
        The minimum pressure required to create an interface within the pore
        body or throat.  Typically this would be calculated using the Washburn
        equation.

    Swp_star : float
        The residual wetting phase in an invaded pore or throat at a pressure
        of ``pc_star``.

    eta : float
        Exponent controlling the rate at which wetting phase is displaced with
        increasing pressure.

    Returns
    -------
    An array containing the fraction of each pore or throat that would be
    filled with non-wetting phase at the given phase pressure.  This does not
    account for whether or not the element is actually invaded, which requires
    a percolation algorithm of some sort."
"def group(epilog=None, help=None, width=140, **attrs):
    
    if epilog is None:
        epilog = _get_caller_doc()
    attrs = settings(epilog=epilog, help=help, width=width, **attrs)
    return click.group(**attrs)","Same as `@click.group()`, but with common settings (ie: ""-h"" for help, epilog, slightly larger help display)"
"async def start(self, connection: ) -> :
        
        self._closed = False
        self._protocol = connection.protocol
        self._connection = connection

        with self._timer:
            while True:
                
                try:
                    message, payload = await self._protocol.read()  
                except http.HttpProcessingError as exc:
                    raise ClientResponseError(
                        self.request_info, self.history,
                        status=exc.code,
                        message=exc.message, headers=exc.headers) from exc

                if (message.code < 100 or
                        message.code > 199 or message.code == 101):
                    break

                if self._continue is not None:
                    set_result(self._continue, True)
                    self._continue = None

        
        payload.on_eof(self._response_eof)

        
        self.version = message.version
        self.status = message.code
        self.reason = message.reason

        
        self._headers = message.headers  
        self._raw_headers = message.raw_headers  

        
        self.content = payload

        
        for hdr in self.headers.getall(hdrs.SET_COOKIE, ()):
            try:
                self.cookies.load(hdr)
            except CookieError as exc:
                client_logger.warning(
                    , exc)
        return self",Start response processing.
"def get_userstory_by_ref(self, ref):
        
        response = self.requester.get(
            ,
            endpoint=UserStory.endpoint,
            us_ref=ref,
            project_id=self.id
        )
        return UserStory.parse(self.requester, response.json())","Get a :class:`UserStory` by ref.

        :param ref: :class:`UserStory` reference"
"def Zs(self):
        rpalladium
        Vms = self.Vms
        if Vms:
            return Z(self.T, self.P, Vms)
        return None","r'''Compressibility factor of the chemical in the solid phase at the
        current temperature and pressure, [dimensionless].

        Utilizes the object oriented interface and
        :obj:`thermo.volume.VolumeSolid` to perform the actual calculation of
        molar volume.

        Examples
        --------
        >>> Chemical('palladium').Z
        0.00036248477437931853"
"def add_inline_interface(self, interface_id, second_interface_id,
        logical_interface_ref=None, vlan_id=None, second_vlan_id=None, zone_ref=None,
        second_zone_ref=None, failure_mode=, comment=None, **kw):
        
        interface_spec = {: interface_id, : second_interface_id,
            : kw.get() if self._engine.type in (, )
            else }
        
        _interface = {: logical_interface_ref,
            : failure_mode, : zone_ref, : second_zone_ref,
            : comment}
        
        vlan = {: vlan_id, : second_vlan_id}
        
        try:
            inline_id = .format(interface_id, second_interface_id)
            interface = self._engine.interface.get(inline_id)
            _interface.update(vlan)
            interface_spec.update(interfaces=[_interface])
            interface._add_interface(**interface_spec)
            return interface.update()
            
        except InterfaceNotFound:
            _interface.update(interfaces=[vlan])
            interface_spec.update(_interface)
            interface = Layer2PhysicalInterface(**interface_spec)
            return self._engine.add_interface(interface)","Add an inline interface pair. This method is only for IPS or L2FW engine
        types.
        
        :param str interface_id: interface id of first interface
        :param str second_interface_id: second interface pair id
        :param str, href logical_interface_ref: logical interface by href or name
        :param str vlan_id: vlan ID for first interface in pair
        :param str second_vlan_id: vlan ID for second interface in pair
        :param str, href zone_ref: zone reference by name or href for first interface
        :param str, href second_zone_ref: zone reference by nae or href for second interface
        :param str failure_mode: normal or bypass
        :param str comment: optional comment
        :raises EngineCommandFailed: failure creating interface
        :return: None"
"def get_report_overview(self, month, file_path):
        
        api = self._get_api(billing.DefaultApi)
        month = self._month_converter(month)
        response = api.get_billing_report(month=month)
        if file_path and response:
            content = api.api_client.sanitize_for_serialization(response.to_dict())
            with open(file_path, ) as fh:
                fh.write(
                    json.dumps(
                        content,
                        sort_keys=True,
                        indent=2,
                    )
                )
        return response","Downloads a report overview

        :param month: month as datetime instance, or string in YYYY-MM format
        :type month: str or datetime
        :param str file_path: location to store output file
        :return: outcome
        :rtype: True or None"
"def insertwordleft(self, newword, nextword, **kwargs):
        
        if nextword:
            if isstring(nextword):
                nextword = self.doc[u(nextword)]
            if not nextword in self or not isinstance(nextword, Word):
                raise Exception(""Next word not found or not instance of Word!"")
            if isinstance(newword, list) or isinstance(newword, tuple):
                if not all([ isinstance(x, Word) for x in newword ]):
                    raise Exception(""New word (iterable) constains non-Word instances!"")
            elif not isinstance(newword, Word):
                raise Exception(""New word no instance of Word!"")

            kwargs[] = self.getindex(nextword)
        else:
            kwargs[] = 0
        kwargs[] = True
        if isinstance(newword, list) or isinstance(newword, tuple):
            return self.correctwords([], newword, **kwargs)
        else:
            return self.correctwords([], [newword], **kwargs)","Inserts a word **as a correction** before an existing word.

        Reverse of :meth:`Sentence.insertword`."
"def identical(args):
    
    from jcvi.utils.cbook import AutoVivification

    allowed_checksum = [""MD5"", ""GCG""]

    p = OptionParser(identical.__doc__)
    p.add_option(""--ignore_case"", default=False, action=""store_true"",
            help=""ignore case when comparing sequences [default: %default]"")
    p.add_option(""--ignore_N"", default=False, action=""store_true"",
            help=""ignore N and X seqcountnamesnamesnamesnamesnamescountnamescountseq']), id=seqid, description="""")
            SeqIO.write([rec], uniqfw, ""fasta"")

    fw.close()
    if opts.output_uniq:
        logging.debug(""Uniq sequences written to `{0}`"".format(uniqfile))
        uniqfw.close()","%prog identical *.fasta

    Given multiple fasta files, find all the exactly identical records
    based on the computed md5 hexdigest or GCG checksum of each sequence.

    Output is an N + 1 column file (where N = number of input fasta files).
    If there are duplicates within a given fasta file, they will all be
    listed out in the same row separated by a comma.

    Example output:
    ---------------------------
	       tta1.fsa    tta2.fsa
	t0         2131          na
	t1         3420          na
	t2    3836,3847         852
	t3          148         890
	t4          584         614
	t5          623         684
	t6         1281         470
	t7         3367          na"
"def update(self, docs, golds, drop=0.0, sgd=None, losses=None, component_cfg=None):
        
        if len(docs) != len(golds):
            raise IndexError(Errors.E009.format(n_docs=len(docs), n_golds=len(golds)))
        if len(docs) == 0:
            return
        if sgd is None:
            if self._optimizer is None:
                self._optimizer = create_default_optimizer(Model.ops)
            sgd = self._optimizer
        
        gold_objs = []
        doc_objs = []
        for doc, gold in zip(docs, golds):
            if isinstance(doc, basestring_):
                doc = self.make_doc(doc)
            if not isinstance(gold, GoldParse):
                gold = GoldParse(doc, **gold)
            doc_objs.append(doc)
            gold_objs.append(gold)
        golds = gold_objs
        docs = doc_objs
        grads = {}

        def get_grads(W, dW, key=None):
            grads[key] = (W, dW)

        get_grads.alpha = sgd.alpha
        get_grads.b1 = sgd.b1
        get_grads.b2 = sgd.b2
        pipes = list(self.pipeline)
        random.shuffle(pipes)
        if component_cfg is None:
            component_cfg = {}
        for name, proc in pipes:
            if not hasattr(proc, ""update""):
                continue
            grads = {}
            kwargs = component_cfg.get(name, {})
            kwargs.setdefault(""drop"", drop)
            proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)
            for key, (W, dW) in grads.items():
                sgd(W, dW, key=key)","Update the models in the pipeline.

        docs (iterable): A batch of `Doc` objects.
        golds (iterable): A batch of `GoldParse` objects.
        drop (float): The droput rate.
        sgd (callable): An optimizer.
        RETURNS (dict): Results from the update.

        DOCS: https://spacy.io/api/language#update"
"def subcellular_locations(self):
        
        return [x[0] for x in self.session.query(models.SubcellularLocation.location).all()]","Distinct subcellular locations (``location`` in :class:`.models.SubcellularLocation`)

        :return: all distinct subcellular locations
        :rtype: list[str]"
"def cluster_join(username, hostname):
    *
    ret = {: , : False}

    cmd = __execute_cmd(
        , .format(username, hostname)
    )

    if cmd[] != 0:
        ret[] = cmd[]
    else:
        ret[] = cmd[]
        ret[] = True

    return ret","Join a Riak cluster

    .. versionchanged:: 2015.8.0

    CLI Example:

    .. code-block:: bash

        salt '*' riak.cluster_join <user> <host>

    username - The riak username to join the cluster
    hostname - The riak hostname you are connecting to"
"def NewFromCmy(c, m, y, alpha=1.0, wref=_DEFAULT_WREF):
    
    return Color(Color.CmyToRgb(c, m, y), , alpha, wref)","Create a new instance based on the specifed CMY values.

    Parameters:
      :c:
        The Cyan component value [0...1]
      :m:
        The Magenta component value [0...1]
      :y:
        The Yellow component value [0...1]
      :alpha:
        The color transparency [0...1], default is opaque
      :wref:
        The whitepoint reference, default is 2 D65.

    Returns:
      A grapefruit.Color instance.

    >>> Color.NewFromCmy(0, 0.5, 1)
    (1, 0.5, 0, 1.0)
    >>> Color.NewFromCmy(0, 0.5, 1, 0.5)
    (1, 0.5, 0, 0.5)"
"def archive_info(database: Database, archive_case: dict) -> dict:
    
    data = {
        : archive_case[],
        : archive_case.get(),
        : [],
        : [],
        : [],
        : [],
        : [],
    }
    if archive_case.get():
        archive_user = database.user.find_one({: archive_case[]})
        data[].append(archive_user[])

    for key in [, ]:
        for variant_id in archive_case.get(key, []):
            archive_variant = database.variant.find_one({: variant_id})
            data[key].append({
                : archive_variant[],
                : archive_variant[],
                : archive_variant[],
                : archive_variant[],
                : archive_variant[],
            })

    for key in [, ]:
        for archive_term in archive_case.get(key, []):
            data[key].append({
                : archive_term[],
                : archive_term[],
            })

    return data",Get information about a case from archive.
"def plot_diagram(ax, x, y, label=""S"", title=""syntenic"", gradient=True):
    
    trackgap = .06
    tracklen = .12
    xa, xb = x - tracklen, x + tracklen
    ya, yb = y + trackgap, y - trackgap
    hsps = (((60, 150), (50, 130)),
           ((190, 225), (200, 240)),
           ((330, 280), (360, 310)))

    for yy in (ya, yb):
        ax.plot((xa, xb), (yy, yy), ""-"", color=""gray"", lw=2, zorder=1)

    ytip = .015
    mrange = 400
    m = lambda t: xa + t * 1. / mrange * tracklen * 2

    for i, ((a, b), (c, d)) in enumerate(hsps):
        fb = False
        if label == ""FB"" and i == 1:
            c, d = 270, 280
            fb = True
        if label == ""G"" and i == 0:
            c, d = 120, 65

        a, b, c, d = [m(t) for t in (a, b, c, d)]
        color = ""g"" if i == 1 else ""r""
        GeneGlyph(ax, a, b, ya, 2 * ytip, fc=color,
                  gradient=gradient, zorder=10)

        if i == 1 and label in (""F"", ""G"", ""FN""):
            pass
        else:
            if fb:
                GeneGlyph(ax, c, d, yb, 2 * ytip, fc=, tip=0,
                          gradient=gradient, zorder=10)
            else:
                GeneGlyph(ax, c, d, yb, 2 * ytip, fc=,
                          gradient=gradient, zorder=10)

        r = Polygon(((a, ya - ytip), (c, yb + ytip),
                      (d, yb + ytip), (b, ya - ytip)),
                      fc=, alpha=.2)

        if i == 1 and label not in (""S"", ""FB""):
            pass
        elif i == 0 and label == ""G"":
            pass
        else:
            ax.add_patch(r)

    if label == ""FN"":
        ax.text(x + .005, yb, ""NNNNN"", ha=""center"", size=7)

    title = ""{0}: {1}"".format(label, title)
    ax.text(x, ya + 5 * ytip, title, size=8, ha=""center"")","Part of the diagrams that are re-used. (x, y) marks the center of the
    diagram. Label determines the modification to the ""S"" graph."
"def create_resource(cls, request_json):
        r
        response = dict()
        new_resource, location = None, None
        try:
            data = request_json[]
            if data[] != cls.__type__:
                raise WrongTypeError()

            attributes = data.get()
            if attributes:
                for x in attributes.keys():
                    if x in cls.dates:
                        dt = datetime.strptime(attributes[x], )
                        attributes[x] = dt

                new_resource = cls(**attributes)
                new_resource.save()

                enum_keys = new_resource.enums.keys()
                for key in attributes.keys():
                    if key in enum_keys:
                        if attributes[key] in new_resource.enums[key]:
                            setattr(new_resource, key, attributes[key])
                        else:
                            raise EnumeratedTypeError
                    else:
                        setattr(new_resource, key, attributes[key])
                    new_resource.save()

                for r in new_resource.hashed:
                    unhashed = getattr(new_resource, r)
                    if unhashed:
                        setattr(new_resource, r, hashlib.sha256(unhashed).hexdigest())
                        new_resource.save()

            relationships = data.get()
            if relationships:
                for relation_name in relationships.keys():
                    relations = relationships.get(relation_name)
                    if relations:
                        relations = relations[]
                        if isinstance(relations, list):
                            for relation in relations:
                                the_type = relation[]  
                                the_id = relation[]
                                the_class = cls.get_class_from_type(the_type)
                                new_resources_relation = the_class.nodes.get(id=the_id, active=True)
                                meta = relation.get()
                                eval(.format(
                                    relation_name=relation_name)
                                )
                                new_resource.save()
                        else:
                            relation = relations
                            the_type = relation[]
                            the_id = relation[]
                            the_class = cls.get_class_from_type(the_type)
                            new_resources_relation = the_class.nodes.get(id=the_id, active=True)
                            meta = relation.get()
                            eval(.format(
                                relation_name=relation_name)
                            )
                            new_resource.save()

            response[] = new_resource.get_resource_object()
            response[] = {: new_resource.get_self_link()}
            status_code = http_error_codes.CREATED
            location = new_resource.get_self_link()

            r = make_response(jsonify(response))
            r.headers[] = ""application/vnd.api+json; charset=utf-8""
            if location and new_resource:
                r.headers[] = location

            r.status_code = status_code

        except UniqueProperty:
            r = application_codes.error_response([application_codes.UNIQUE_KEY_VIOLATION])
            try:
                new_resource.delete()
            except:
                pass

        except DoesNotExist:
            r = application_codes.error_response([application_codes.RESOURCE_NOT_FOUND])
            try:
                new_resource.delete()
            except:
                pass

        except WrongTypeError as e:
            r = application_codes.error_response([application_codes.WRONG_TYPE_VIOLATION])
            try:
                new_resource.delete()
            except:
                pass

        except KeyError as e:
            r = application_codes.error_response([application_codes.BAD_FORMAT_VIOLATION])
            print e
            try:
                new_resource.delete()
            except:
                pass

        except EnumeratedTypeError:
            r = application_codes.error_response([application_codes.ENUMERATED_TYPE_VIOLATION])
            try:
                new_resource.delete()
            except:
                pass

        except ParameterMissing:
            r = application_codes.error_response([application_codes.BAD_PARAMETER_VIOLATION])
            try:
                new_resource.delete()
            except:
                pass

        return r","r""""""
        Used to create a node in the database of type 'cls' in response to a POST request. create_resource should only \
        be invoked on a resource when the client specifies a POST request.

        :param request_json: a dictionary formatted according to the specification at \
        http://jsonapi.org/format/#crud-creating
        :return: An HTTP response object in accordance with the same specification"
"def unbound_dimensions(streams, kdims, no_duplicates=True):
    
    params = stream_parameters(streams, no_duplicates)
    return [d for d in kdims if d not in params]","Return a list of dimensions that have not been associated with
    any streams."
"def generate_output_prov(self,
                             final_output,    
                             process_run_id,  
                             name             
                            ):   
        
        
        timestamp = datetime.datetime.now()

        
        
        
        for output, value in final_output.items():
            entity = self.declare_artefact(value)
            if name is not None:
                name = urllib.parse.quote(str(name), safe="":/,
                
                role = self.wf_ns[""main/%s/%s"" % (name, output)]
            else:
                role = self.wf_ns[""main/%s"" % output]

            if not process_run_id:
                process_run_id = self.workflow_run_uri

            self.document.wasGeneratedBy(
                entity, process_run_id, timestamp, None, {""prov:role"": role})","Call wasGeneratedBy() for each output,copy the files into the RO."
"def ported_string(raw_data, encoding=, errors=):
    

    if not raw_data:
        return six.text_type()

    if isinstance(raw_data, six.text_type):
        return raw_data.strip()

    if six.PY2:
        try:
            return six.text_type(raw_data, encoding, errors).strip()
        except LookupError:
            return six.text_type(raw_data, ""utf-8"", errors).strip()

    if six.PY3:
        try:
            return six.text_type(raw_data, encoding).strip()
        except (LookupError, UnicodeDecodeError):
            return six.text_type(raw_data, ""utf-8"", errors).strip()","Give as input raw data and output a str in Python 3
    and unicode in Python 2.

    Args:
        raw_data: Python 2 str, Python 3 bytes or str to porting
        encoding: string giving the name of an encoding
        errors: his specifies the treatment of characters
            which are invalid in the input encoding

    Returns:
        str (Python 3) or unicode (Python 2)"
"def edit_files(patterns, expressions=None,
               functions=None, executables=None,
               start_dirs=None, max_depth=1, dry_run=True,
               output=sys.stdout, encoding=None, newline=None):
    
    if not is_list(patterns):
        raise TypeError(""patterns should be a list"")
    if expressions and not is_list(expressions):
        raise TypeError(""expressions should be a list of exec expressions"")
    if functions and not is_list(functions):
        raise TypeError(""functions should be a list of functions"")
    if executables and not is_list(executables):
        raise TypeError(""executables should be a list of program names"")

    editor = MassEdit(dry_run=dry_run, encoding=encoding, newline=newline)
    if expressions:
        editor.set_code_exprs(expressions)
    if functions:
        editor.set_functions(functions)
    if executables:
        editor.set_executables(executables)

    processed_paths = []
    for path in get_paths(patterns, start_dirs=start_dirs,
                          max_depth=max_depth):
        try:
            diffs = list(editor.edit_file(path))
            if dry_run:
                
                diff = """".join(diffs)
                if not diff:
                    continue
                
                
                    diff = bytes_diff.decode(encoding=output.encoding)
                output.write(diff)
        except UnicodeDecodeError as err:
            log.error(""failed to process %s: %s"", path, err)
            continue
        processed_paths.append(os.path.abspath(path))
    return processed_paths","Process patterns with MassEdit.

    Arguments:
      patterns: file pattern to identify the files to be processed.
      expressions: single python expression to be applied line by line.
      functions: functions to process files contents.
      executables: os executables to execute on the argument files.

    Keyword arguments:
      max_depth: maximum recursion level when looking for file matches.
      start_dirs: workspace(ies) where to start the file search.
      dry_run: only display differences if True. Save modified file otherwise.
      output: handle where the output should be redirected.

    Return:
      list of files processed."
"def create_extended_model(model, db_penalty=None, ex_penalty=None,
                          tp_penalty=None, penalties=None):
    

    
    model_extended = model.create_metabolic_model()
    extra_compartment = model.extracellular_compartment

    compartment_ids = set(c.id for c in model.compartments)

    
    if len(compartment_ids) > 0:
        logger.info(
            .format(
                .join(.format(c) for c in compartment_ids)))
        db_added = add_all_database_reactions(model_extended, compartment_ids)
    else:
        logger.warning(
            
            
            )
        db_added = set()

    
    logger.info(
        .format(
            extra_compartment))
    ex_added = add_all_exchange_reactions(
        model_extended, extra_compartment, allow_duplicates=True)

    
    boundaries = model.compartment_boundaries
    if len(boundaries) > 0:
        logger.info(
            
            .format(
                .join(.format(c1, c2) for c1, c2 in boundaries)))
        tp_added = add_all_transport_reactions(
            model_extended, boundaries, allow_duplicates=True)
    else:
        logger.warning(
            
            )
        tp_added = set()

    
    weights = {}
    if db_penalty is not None:
        weights.update((rxnid, db_penalty) for rxnid in db_added)
    if tp_penalty is not None:
        weights.update((rxnid, tp_penalty) for rxnid in tp_added)
    if ex_penalty is not None:
        weights.update((rxnid, ex_penalty) for rxnid in ex_added)

    if penalties is not None:
        for rxnid, penalty in iteritems(penalties):
            weights[rxnid] = penalty
    return model_extended, weights","Create an extended model for gap-filling.

    Create a :class:`psamm.metabolicmodel.MetabolicModel` with
    all reactions added (the reaction database in the model is taken
    to be the universal database) and also with artificial exchange
    and transport reactions added. Return the extended
    :class:`psamm.metabolicmodel.MetabolicModel`
    and a weight dictionary for added reactions in that model.

    Args:
        model: :class:`psamm.datasource.native.NativeModel`.
        db_penalty: penalty score for database reactions, default is `None`.
        ex_penalty: penalty score for exchange reactions, default is `None`.
        tb_penalty: penalty score for transport reactions, default is `None`.
        penalties: a dictionary of penalty scores for database reactions."
"def register_callback(self):
        

        cid = str(self.__cid)
        self.__cid += 1
        event = queue.Queue()
        self.__callbacks[cid] = event
        return cid, event",Register callback that we will have to wait for
"def is_modified(self) -> bool:
        
        if self.is_new or self.is_deleted:
            return False
        return self.left.md5 != self.right.md5","Find whether the files on the left and right are different. Note,
        modified implies the contents of the file have changed, which is
        predicated on the file existing on both the left and right. Therefore
        this will be false if the file on the left has been deleted, or the
        file on the right is new.

        :return: Whether the file has been modified."
"def child(self, local_name=None, name=None, ns_uri=None, node_type=None,
            filter_fn=None):
        
        return self.children(name=name, local_name=local_name, ns_uri=ns_uri,
            node_type=node_type, filter_fn=filter_fn, first_only=True)",":return: the first child node matching the given constraints, or \
                 *None* if there are no matching child nodes.

        Delegates to :meth:`NodeList.filter`."
"def if_none_match(self):
        
        for option in self.options:
            if option.number == defines.OptionRegistry.IF_NONE_MATCH.number:
                return True
        return False","Get the if-none-match option of a request.

        :return: True, if if-none-match is present
        :rtype : bool"
"def select(data, trial=None, invert=False, **axes_to_select):
    
    if trial is not None and not isinstance(trial, Iterable):
        raise TypeError()

    for axis_to_select, values_to_select in axes_to_select.items():
        if (not isinstance(values_to_select, Iterable) or
           isinstance(values_to_select, str)):
            raise TypeError(axis_to_select + )

    if trial is None:
        trial = range(data.number_of())
    else:
        trial = trial
        if invert:
            trial = setdiff1d(range(data.number_of()), trial)

    
    output = data._copy(axis=False)
    for one_axis in output.axis:
        output.axis[one_axis] = empty(len(trial), dtype=)
    output.data = empty(len(trial), dtype=)

    to_select = {}
    for cnt, i in enumerate(trial):
        lg.debug(.format(i))
        for one_axis in output.axis:
            values = data.axis[one_axis][i]

            if one_axis in axes_to_select.keys():
                values_to_select = axes_to_select[one_axis]

                if len(values_to_select) == 0:
                    selected_values = ()

                elif isinstance(values_to_select[0], str):
                    selected_values = asarray(values_to_select, dtype=)

                else:
                    if (values_to_select[0] is None and
                       values_to_select[1] is None):
                        bool_values = ones(len(values), dtype=bool)
                    elif values_to_select[0] is None:
                        bool_values = values < values_to_select[1]
                    elif values_to_select[1] is None:
                        bool_values = values_to_select[0] <= values
                    else:
                        bool_values = ((values_to_select[0] <= values) &
                                       (values < values_to_select[1]))
                    selected_values = values[bool_values]

                if invert:
                    selected_values = setdiff1d(values, selected_values)

                lg.debug(
                         .format(one_axis,
                                         len(selected_values)))
                to_select[one_axis] = selected_values
            else:
                lg.debug( + one_axis + 
                         )
                selected_values = data.axis[one_axis][i]

            output.axis[one_axis][cnt] = selected_values

        output.data[cnt] = data(trial=i, **to_select)

    return output","Define the selection of trials, using ranges or actual values.

    Parameters
    ----------
    data : instance of Data
        data to select from.
    trial : list of int or ndarray (dtype='i'), optional
        index of trials of interest
    **axes_to_select, optional
        Values need to be tuple or list. If the values in one axis are string,
        then you need to specify all the strings that you want. If the values
        are numeric, then you should specify the range (you cannot specify
        single values, nor multiple values). To select only up to one point,
        you can use (None, value_of_interest)
    invert : bool
        take the opposite selection

    Returns
    -------
    instance, same class as input
        data where selection has been applied."
"def semantic_version(tag):
    
    try:
        version = list(map(int, tag.split()))
        assert len(version) == 3
        return tuple(version)
    except Exception as exc:
        raise CommandError(
            
             % tag
        ) from exc",Get a valid semantic version for tag
"def _prepare_corerelation_data(self, X1, X2,
                                   start_voxel=0,
                                   num_processed_voxels=None):
        
        num_samples = len(X1)
        assert num_samples > 0, \
            
        num_voxels1 = X1[0].shape[1]
        num_voxels2 = X2[0].shape[1]
        assert num_voxels1 * num_voxels2 == self.num_features_, \
             \
            
        assert X1[0].shape[0] == X2[0].shape[0], \
            
        if num_processed_voxels is None:
            num_processed_voxels = num_voxels1
        corr_data = np.zeros((num_samples, num_processed_voxels, num_voxels2),
                             np.float32, order=)
        
        for idx, data in enumerate(X1):
            data2 = X2[idx]
            num_TRs = data.shape[0]
            blas.compute_corr_vectors(, ,
                                      num_voxels2, num_processed_voxels,
                                      num_TRs,
                                      1.0, data2, num_voxels2,
                                      data, num_voxels1,
                                      0.0, corr_data, num_voxels2,
                                      start_voxel, idx)
        logger.debug(
            
        )
        return corr_data","Compute auto-correlation for the input data X1 and X2.

        it will generate the correlation between some voxels and all voxels

        Parameters
        ----------
        X1: a list of numpy array in shape [num_TRs, num_voxels1]
            X1 contains the activity data filtered by ROIs
            and prepared for correlation computation.
            All elements of X1 must have the same num_voxels value.
        X2: a list of numpy array in shape [num_TRs, num_voxels2]
            len(X1) equals len(X2).
            All elements of X2 must have the same num_voxels value.
            X2 can be identical to X1; if not, X1 must have more voxels
            than X2 (guaranteed by self.fit and/or self.predict).
        start_voxel: int, default 0
            the starting voxel id for correlation computation
        num_processed_voxels: int, default None
            the number of voxels it computes for correlation computation
            if it is None, it is set to self.num_voxels

        Returns
        -------
        corr_data: the correlation data
                    in shape [len(X), num_processed_voxels, num_voxels2]"
"def _similarity_matrix(self, concepts):
        
        n_cons = len(concepts)
        sim_mat = np.zeros((n_cons, n_cons))
        for i, c1 in enumerate(concepts):
            for j, c2 in enumerate(concepts):
                
                if i >= j:
                    sim_mat[i,j] = self._semsim(c1, c2) if i != j else 1.
        return sim_mat + sim_mat.T - np.diag(sim_mat.diagonal())",Computes a semantic similarity matrix for a set of concepts.
"def modify_tag(self, name, description=None, servers=None, new_name=None):
        
        res = self._modify_tag(name, description, servers, new_name)
        return Tag(cloud_manager=self, **res[])",PUT /tag/name. Returns a new Tag object based on the API response.
"def _send_merge_commands(self, config, file_config):
        
        if self.loaded is False:
            if self._save_backup() is False:
                raise MergeConfigException(
                                           )
        if self.ssh_connection is False:
            self._open_ssh()

        if file_config:
            if isinstance(config, str):
                config = config.splitlines()
        else:
            if isinstance(config, str):
                config = str(config).split()

        self.ssh_device.send_config_set(config)
        self.loaded = True
        self.merge_config = True",Netmiko is being used to push set commands.
"def clean(self):
        
        logger.debug(""Cleaning configuration objects before configuration sending:"")
        types_creations = self.__class__.types_creations
        for o_type in types_creations:
            (_, _, inner_property, _, _) = types_creations[o_type]
            logger.debug(""  . for %s"", inner_property, )
            inner_object = getattr(self, inner_property)
            inner_object.clean()","Wrapper for calling the clean method of services attribute

        :return: None"
"def get_template_parameters_file(template_full_path):
    
    for suffix in EFConfig.PARAMETER_FILE_SUFFIXES:
      parameters_file = template_full_path.replace(""/templates"", ""/parameters"") + suffix
      if exists(parameters_file):
        return parameters_file
      else:
        continue
    return None","Checks for existance of parameters file against supported suffixes and returns parameters file path if found
    Args:
      template_full_path: full filepath for template file
    Returns:
      filename of parameters file if it exists"
"def _where_clause(where):
        
        assert isinstance(where, tuple)
        if len(where) == 3:
            where_col, operator, where_val = where
        else:
            where_col, where_val = where
            operator = 
        assert operator in SELECT_WHERE_OPERATORS

        
        return ""{0}{1}"".format(where_col, operator, where_val)","Unpack a where clause tuple and concatenate a MySQL WHERE statement.

        :param where: 2 or 3 part tuple containing a where_column and a where_value (optional operator)
        :return: WHERE clause statement"
"def listen(self, *, host, port, override=False, forever=False, **kwargs):
        
        if override:
            argv = dict(enumerate(sys.argv))
            host = argv.get(1, host)
            port = int(argv.get(2, port))
        server = self.loop.create_server(
            self.__handler.fork, host, port, **kwargs)
        server = self.loop.run_until_complete(server)
        self.log(,
            .
            format(host=host, port=port))
        if forever:
            try:
                self.loop.run_forever()
            except KeyboardInterrupt:
                pass
        return server","Listen on TCP/IP socket.

        Parameters
        ----------
        host: str
            Host like '127.0.0.1'
        port:
            Port like 80."
"def match_fn(match_values_and_abi, data):
    
    abi_types, all_match_values = zip(*match_values_and_abi)
    decoded_values = decode_abi(abi_types, HexBytes(data))
    for data_value, match_values, abi_type in zip(decoded_values, all_match_values, abi_types):
        if match_values is None:
            continue
        normalized_data = normalize_data_values(abi_type, data_value)
        for value in match_values:
            if not is_encodable(abi_type, value):
                raise ValueError(
                    ""Value {0} is of the wrong abi type. ""
                    ""Expected {1} typed value."".format(value, abi_type))
            if value == normalized_data:
                break
        else:
            return False

    return True","Match function used for filtering non-indexed event arguments.

    Values provided through the match_values_and_abi parameter are
    compared to the abi decoded log data."
"def connect(self, **options):
        
        if self.device is None:
            raise IOError(errno.ENODEV, os.strerror(errno.ENODEV))

        log.debug(""connect{0}"".format(
            tuple([k for k in options if options[k]])))

        terminate = options.get(, lambda: False)
        rdwr_options = options.get()
        llcp_options = options.get()
        card_options = options.get()

        try:
            assert isinstance(rdwr_options, (dict, type(None))), ""rdwr""
            assert isinstance(llcp_options, (dict, type(None))), ""llcp""
            assert isinstance(card_options, (dict, type(None))), ""card""
        except AssertionError as error:
            raise TypeError(""argument  must be a dictionary"" % error)

        if llcp_options is not None:
            llcp_options = dict(llcp_options)
            llcp_options.setdefault(, lambda llc: llc)
            llcp_options.setdefault(, lambda llc: True)
            llcp_options.setdefault(, lambda llc: True)

            llc = nfc.llcp.llc.LogicalLinkController(**llcp_options)
            llc = llcp_options[](llc)
            if isinstance(llc, nfc.llcp.llc.LogicalLinkController):
                llcp_options[] = llc
            else:
                log.debug(""removing llcp_options after on-startup"")
                llcp_options = None

        if rdwr_options is not None:
            def on_discover(target):
                if target.sel_res and target.sel_res[0] & 0x40:
                    return False
                elif target.sensf_res and target.sensf_res[1:3] == b""\x01\xFE"":
                    return False
                else:
                    return True

            rdwr_options = dict(rdwr_options)
            rdwr_options.setdefault(, [, , ])
            rdwr_options.setdefault(, lambda targets: targets)
            rdwr_options.setdefault(, on_discover)
            rdwr_options.setdefault(, lambda tag: True)
            rdwr_options.setdefault(, lambda tag: True)
            rdwr_options.setdefault(, 5)
            rdwr_options.setdefault(, 0.5)
            rdwr_options.setdefault(, True)

            targets = [RemoteTarget(brty) for brty in rdwr_options[]]
            targets = rdwr_options[](targets)
            if targets and all([isinstance(o, RemoteTarget) for o in targets]):
                rdwr_options[] = targets
            else:
                log.debug(""removing rdwr_options after on-startup"")
                rdwr_options = None

        if card_options is not None:
            card_options = dict(card_options)
            card_options.setdefault(, lambda target: None)
            card_options.setdefault(, lambda target: True)
            card_options.setdefault(, lambda tag: True)
            card_options.setdefault(, lambda tag: True)

            target = nfc.clf.LocalTarget()
            target = card_options[](target)
            if isinstance(target, LocalTarget):
                card_options[] = target
            else:
                log.debug(""removing card_options after on-startup"")
                card_options = None

        if not (rdwr_options or llcp_options or card_options):
            log.warning(""no options to connect"")
            return None

        log.debug(""connect options after startup: %s"",
                  .join(filter(bool, [""rdwr"" if rdwr_options else None,
                                          ""llcp"" if llcp_options else None,
                                          ""card"" if card_options else None])))

        try:
            while not terminate():
                if rdwr_options:
                    result = self._rdwr_connect(rdwr_options, terminate)
                    if bool(result) is True:
                        return result
                if llcp_options:
                    result = self._llcp_connect(llcp_options, terminate)
                    if bool(result) is True:
                        return result
                if card_options:
                    result = self._card_connect(card_options, terminate)
                    if bool(result) is True:
                        return result
        except IOError as error:
            log.error(error)
            return False
        except UnsupportedTargetError as error:
            log.info(error)
            return False
        except KeyboardInterrupt:
            log.debug(""terminated by keyboard interrupt"")
            return False","Connect with a Target or Initiator

        The calling thread is blocked until a single activation and
        deactivation has completed or a callback function supplied as
        the keyword argument ``terminate`` returns a true value. The
        example below makes :meth:`~connect()` return after 5 seconds,
        regardless of whether a peer device was connected or not.

        >>> import nfc, time
        >>> clf = nfc.ContactlessFrontend('usb')
        >>> after5s = lambda: time.time() - started > 5
        >>> started = time.time(); clf.connect(llcp={}, terminate=after5s)

        Connect options are given as keyword arguments with dictionary
        values. Possible options are:

        * ``rdwr={key: value, ...}`` - options for reader/writer
        * ``llcp={key: value, ...}`` - options for peer to peer
        * ``card={key: value, ...}`` - options for card emulation

        **Reader/Writer Options**

        'targets' : iterable
           A list of bitrate and technology type strings that will
           produce the :class:`~nfc.clf.RemoteTarget` objects to
           discover. The default is ``('106A', '106B', '212F')``.

        'on-startup' : function(targets)
           This function is called before any attempt to discover a
           remote card. The *targets* argument provides a list of
           :class:`RemoteTarget` objects prepared from the 'targets'
           bitrate and technology type strings. The function must
           return a list of of those :class:`RemoteTarget` objects
           that shall be finally used for discovery, those targets may
           have additional attributes. An empty list or anything else
           that evaluates false will remove the 'rdwr' option
           completely.

        'on-discover' : function(target)
           This function is called when a :class:`RemoteTarget` has
           been discovered. The *target* argument contains the
           technology type specific discovery responses and should be
           evaluated for multi-protocol support. The target will be
           further activated only if this function returns a true
           value. The default function depends on the 'llcp' option,
           if present then the function returns True only if the
           target does not indicate peer to peer protocol support,
           otherwise it returns True for all targets.

        'on-connect' : function(tag)
           This function is called when a remote tag has been
           activated. The *tag* argument is an instance of class
           :class:`nfc.tag.Tag` and can be used for tag reading and
           writing within the callback or in a separate thread. Any
           true return value instructs :meth:`connect` to wait until
           the tag is no longer present and then return True, any
           false return value implies immediate return of the
           :class:`nfc.tag.Tag` object.

        'on-release' : function(tag)
           This function is called when the presence check was run
           (the 'on-connect' function returned a true value) and
           determined that communication with the *tag* has become
           impossible, or when the 'terminate' function returned a
           true value. The *tag* object may be used for cleanup
           actions but not for communication.

        'iterations' : integer
           This determines the number of sense cycles performed
           between calls to the terminate function. Each iteration
           searches once for all specified targets. The default value
           is 5 iterations and between each iteration is a waiting
           time determined by the 'interval' option described below.
           As an effect of math there will be no waiting time if
           iterations is set to 1.

        'interval' : float
           This determines the waiting time between iterations. The
           default value of 0.5 seconds is considered a sensible
           tradeoff between responsiveness in terms of tag discovery
           and power consumption. It should be clear that changing
           this value will impair one or the other. There is no free
           beer.

        'beep-on-connect': boolean
            If the device supports beeping or flashing an LED,
            automatically perform this functionality when a tag is
            successfully detected AND the 'on-connect' function
            returns a true value. Defaults to True.

        .. sourcecode:: python

           import nfc

           def on_startup(targets):
               for target in targets:
                   target.sensf_req = bytearray.fromhex(""0012FC0000"")
               return targets

           def on_connect(tag):
               print(tag)

           rdwr_options = {
               'targets': ['212F', '424F'],
               'on-startup': on_startup,
               'on-connect': on_connect,
           }
           with nfc.ContactlessFrontend('usb') as clf:
               tag = clf.connect(rdwr=rdwr_options)
               if tag.ndef:
                   print(tag.ndef.message.pretty())

        **Peer To Peer Options**

        'on-startup' : function(llc)
           This function is called before any attempt to establish
           peer to peer communication. The *llc* argument provides the
           :class:`~nfc.llcp.llc.LogicalLinkController` that may be
           used to allocate and bind listen sockets for local
           services. The function should return the *llc* object if
           activation shall continue. Any other value removes the
           'llcp' option.

        'on-connect' : function(llc)
           This function is called when peer to peer communication is
           successfully established. The *llc* argument provides the
           now activated :class:`~nfc.llcp.llc.LogicalLinkController`
           ready for allocation of client communication sockets and
           data exchange in separate work threads. The function should
           a true value return more or less immediately, unless it
           wishes to handle the logical link controller run loop by
           itself and anytime later return a false value.

        'on-release' : function(llc)
           This function is called when the symmetry loop was run (the
           'on-connect' function returned a true value) and determined
           that communication with the remote peer has become
           impossible, or when the 'terminate' function returned a
           true value. The *llc* object may be used for cleanup
           actions but not for communication.

        'role' : string
           This attribute determines whether the local device will
           restrict itself to either ``'initiator'`` or ``'target'``
           mode of operation. As Initiator the local device will try
           to discover a remote device. As Target it waits for being
           discovered. The default is to alternate between both roles.

        'miu' : integer
           This attribute sets the maximum information unit size that
           is announced to the remote device during link activation.
           The default and also smallest possible value is 128 bytes.

        'lto' : integer
           This attribute sets the link timeout value (given in
           milliseconds) that is announced to the remote device during
           link activation. It informs the remote device that if the
           local device does not return a protocol data unit before
           the timeout expires, the communication link is broken and
           can not be recovered. The *lto* is an important part of the
           user experience, it ultimately tells when the user should
           no longer expect communication to continue. The default
           value is 500 millisecond.

        'agf' : boolean
           Some early phone implementations did not properly handle
           aggregated protocol data units. This attribute allows to
           disable the use af aggregation at the cost of efficiency.
           Aggregation is disabled with a false value. The default
           is to use aggregation.

        'brs' : integer
           For the local device in Initiator role the bit rate
           selector determines the the bitrate to negotiate with the
           remote Target. The value may be 0, 1, or 2 for 106, 212, or
           424 kbps, respectively. The default is to negotiate 424
           kbps.

        'acm' : boolean
           For the local device in Initiator role this attribute
           determines whether a remote Target may also be activated in
           active communication mode. In active communication mode
           both peer devices mutually generate a radio field when
           sending. The default is to use passive communication mode.

        'rwt' : float
           For the local device in Target role this attribute sets the
           response waiting time announced during link activation. The
           response waiting time is a medium access layer (NFC-DEP)
           value that indicates when the remote Initiator shall
           attempt error recovery after missing a Target response. The
           value is the waiting time index *wt* that determines the
           effective response waiting time by the formula ``rwt =
           4096/13.56E6 * pow(2, wt)``. The value shall not be greater
           than 14. The default value is 8 and yields an effective
           response waiting time of 77.33 ms.

        'lri' : integer
           For the local device in Initiator role this attribute sets
           the length reduction for medium access layer (NFC-DEP)
           information frames. The value may be 0, 1, 2, or 3 for a
           maximum payload size of 64, 128, 192, or 254 bytes,
           respectively. The default value is 3.

        'lrt' : integer
           For the local device in Target role this attribute sets
           the length reduction for medium access layer (NFC-DEP)
           information frames. The value may be 0, 1, 2, or 3 for a
           maximum payload size of 64, 128, 192, or 254 bytes,
           respectively. The default value is 3.

        .. sourcecode:: python

           import nfc
           import nfc.llcp
           import threading

           def server(socket):
               message, address = socket.recvfrom()
               socket.sendto(""It's me!"", address)
               socket.close()

           def client(socket):
               socket.sendto(""Hi there!"", address=32)
               socket.close()

           def on_startup(llc):
               socket = nfc.llcp.Socket(llc, nfc.llcp.LOGICAL_DATA_LINK)
               socket.bind(address=32)
               threading.Thread(target=server, args=(socket,)).start()
               return llc

           def on_connect(llc):
               socket = nfc.llcp.Socket(llc, nfc.llcp.LOGICAL_DATA_LINK)
               threading.Thread(target=client, args=(socket,)).start()
               return True

           llcp_options = {
               'on-startup': on_startup,
               'on-connect': on_connect,
           }
           with nfc.ContactlessFrontend('usb') as clf:
               clf.connect(llcp=llcp_options)
               print(""link terminated"")

        **Card Emulation Options**

        'on-startup' : function(target)
           This function is called to prepare a local target for
           discovery. The input argument is a fresh instance of an
           unspecific :class:`LocalTarget` that can be set to the
           desired bitrate and modulation type and populated with the
           type specific discovery responses (see :meth:`listen` for
           response data that is needed). The fully specified target
           object must then be returned.

        'on-discover' : function(target)
           This function is called when the :class:`LocalTarget` has
           been discovered. The *target* argument contains the
           technology type specific discovery commands. The target
           will be further activated only if this function returns a
           true value. The default function always returns True.

        'on-connect' : function(tag)
           This function is called when the local target was
           discovered and a :class:`nfc.tag.TagEmulation` object
           successfully initialized. The function receives the
           emulated *tag* object which stores the first command
           received after inialization as ``tag.cmd``. The function
           should return a true value if the tag.process_command() and
           tag.send_response() methods shall be called repeatedly
           until either the remote device terminates communication or
           the 'terminate' function returns a true value. The function
           should return a false value if the :meth:`connect` method
           shall return immediately with the emulated *tag* object.

        'on-release' : function(tag)
           This function is called when the Target was released by the
           Initiator or simply moved away, or if the terminate
           callback function has returned a true value. The emulated
           *tag* object may be used for cleanup actions but not for
           communication.

        .. sourcecode:: python

           import nfc

           def on_startup(target):
               idm = bytearray.fromhex(""01010501b00ac30b"")
               pmm = bytearray.fromhex(""03014b024f4993ff"")
               sys = bytearray.fromhex(""1234"")
               target.brty = ""212F""
               target.sensf_res = chr(1) + idm + pmm + sys
               return target

           def on_connect(tag):
               print(""discovered by remote reader"")
               return True

           def on_release(tag):
               print(""remote reader is gone"")
               return True

           card_options = {
               'on-startup': on_startup,
               'on-connect': on_connect,
               'on-release': on_release,
           }
           with nfc.ContactlessFrontend('usb') as clf:
               clf.connect(card=card_options)

        **Return Value**

        The :meth:`connect` method returns :const:`None` if there were
        no options left after the 'on-startup' functions have been
        executed or when the 'terminate' function returned a true
        value. It returns :const:`False` when terminated by any of the
        following exceptions: :exc:`~exceptions.KeyboardInterrupt`,
        :exc:`~exceptions.IOError`, :exc:`UnsupportedTargetError`.

        The :meth:`connect` method returns a :class:`~nfc.tag.Tag`,
        :class:`~nfc.llcp.llc.LogicalLinkController`, or
        :class:`~nfc.tag.TagEmulation` object if the associated
        'on-connect' function returned a false value to indicate that
        it will handle presence check, peer to peer symmetry loop, or
        command/response processing by itself."
"def data_parallelism_from_flags(daisy_chain_variables=True, all_workers=False):
  
  dp_arg_names = inspect.getargspec(data_parallelism).args

  blacklist = [""daisy_chain_variables"", ""all_workers""]

  kwargs = {}
  for arg in dp_arg_names:
    if arg in blacklist:
      continue
    kwargs[arg] = getattr(tf.flags.FLAGS, arg)

  return data_parallelism(
      daisy_chain_variables=daisy_chain_variables,
      all_workers=all_workers,
      **kwargs)","Over which devices do we split each training batch.

  In old-fashioned async mode, we split the batch over all GPUs on the
  current worker.

  In sync mode, we split the batch over all the parameter server GPUs.

  This function returns an expert_utils.Parallelism object, which can be used
  to build the model.  It is configured in a way that any variables created
  by `tf.get_variable` will be assigned to the parameter servers and shared
  between datashards.

  Args:
    daisy_chain_variables: whether to copy variables in a daisy chain on GPUs.
    all_workers: whether the devices are all async workers or just this one.

  Returns:
    a expert_utils.Parallelism."
"def _attach_to_model(self, model):
        
        super(RelatedFieldMixin, self)._attach_to_model(model)

        if model.abstract:
            
        relation = (self._model._name, self.name, self.related_name)
        self.database._relations[self.related_to].append(relation)","When we have a model, save the relation in the database, to later create
        RelatedCollection objects in the related model"
"def from_Cary(filepath, name=None, parent=None, verbose=True):
    
    
    filestr = os.fspath(filepath)
    filepath = pathlib.Path(filepath)

    if "".csv"" not in filepath.suffixes:
        wt_exceptions.WrongFileTypeWarning.warn(filepath, ""csv"")
    if name is None:
        name = ""cary""
    
    lines = []
    ds = np.DataSource(None)
    with ds.open(filestr, ""rt"", encoding=""iso-8859-1"") as f:
        header = f.readline()
        columns = f.readline()
        while True:
            line = f.readline()
            if line == ""\n"" or line == """" or line == ""\r\n"":
                break
            else:
                
                
                line = line.replace("",,"", "",nan,"")
                line = line.replace("",,"", "",nan,"")
                
                if line[0] == "","":
                    line = ""nan"" + line
                clean = line[:-2]  
                lines.append(np.fromstring(clean, sep="",""))
    lines = [line for line in lines if len(line) > 0]
    header = header.split("","")
    columns = columns.split("","")
    arr = np.array(lines).T
    duplicate = len(header) // 2 == len(set(header) - {""""})
    
    datas = Collection(name=name, parent=parent, edit_local=parent is not None)
    units_dict = {""c"": ""deg_C"", ""f"": ""deg_F""}
    for i in range(0, len(header) - 1, 2):
        r = re.compile(r""[ \t\(\)]+"")
        spl = r.split(columns[i])
        ax = spl[0].lower() if len(spl) > 0 else None
        units = spl[1].lower() if len(spl) > 1 else None
        units = units_dict.get(units, units)
        if duplicate:
            name = ""{}_{:03d}"".format(header[i], i // 2)
        else:
            name = header[i]
        dat = datas.create_data(name, kind=""Cary"", source=filestr)
        dat.create_variable(ax, arr[i][~np.isnan(arr[i])], units=units)
        dat.create_channel(
            columns[i + 1].lower(), arr[i + 1][~np.isnan(arr[i + 1])], label=columns[i + 1].lower()
        )
        dat.transform(ax)
    
    if verbose:
        print(""{0} data objects successfully created from Cary file:"".format(len(datas)))
        for i, data in enumerate(datas):
            print(""  {0}: {1}"".format(i, data))
    return datas","Create a collection object from a Cary UV VIS absorbance file.

    We hope to support as many Cary instruments and datasets as possible.
    This function has been tested with data collected on a Cary50 UV/VIS spectrometer.
    If any alternate instruments are found not to work as expected, please
    submit a bug report on our `issue tracker`__.

    __ github.com/wright-group/WrightTools/issues

    .. plot::

        >>> import WrightTools as wt
        >>> from WrightTools import datasets
        >>> p = datasets.Cary.CuPCtS_H2O_vis
        >>> data = wt.collection.from_Cary(p)[0]
        >>> wt.artists.quick1D(data)

    Parameters
    ----------
    filepath : path-like
        Path to Cary output file (.csv).
    parent : WrightTools.Collection
        A collection object in which to place a collection of Data objects.
    verbose : boolean (optional)
        Toggle talkback. Default is True.

    Returns
    -------
    data
        New data object."
"def set_prefix(self, prefix):
        
        warnings.warn(""set_prefix() is deprecated; use the prefix property"",
                      DeprecationWarning, stacklevel=2)
        self.prefix = prefix","Set the prefix for the node (see Leaf class).

        DEPRECATED; use the prefix property directly."
"def ebic_select(self, gamma=0):
        
        if not isinstance(self.precision_, list):
            raise ValueError(""EBIC requires multiple models to select from."")
            return

        if not self.is_fitted_:
            return

        ebic_scores = self.ebic(gamma=gamma)
        min_indices = np.where(np.abs(ebic_scores - ebic_scores.min()) < 1e-10)
        return np.max(min_indices)","Uses Extended Bayesian Information Criteria for model selection.

        Can only be used in path mode (doesn't really make sense otherwise).

        See:
        Extended Bayesian Information Criteria for Gaussian Graphical Models
        R. Foygel and M. Drton
        NIPS 2010

        Parameters
        ----------
        gamma : (float) \in (0, 1)
            Choice of gamma=0 leads to classical BIC
            Positive gamma leads to stronger penalization of large graphs.

        Returns
        -------
        Lambda index with best ebic score.  When multiple ebic scores are the
        same, returns the smallest lambda (largest index) with minimum score."
"def analytics_query(self, query, host, *args, **kwargs):
        
        if not isinstance(query, AnalyticsQuery):
            query = AnalyticsQuery(query, *args, **kwargs)
        else:
            query.update(*args, **kwargs)

        return couchbase.analytics.gen_request(query, host, self)","Execute an Analytics query.

        This method is mainly a wrapper around the :class:`~.AnalyticsQuery`
        and :class:`~.AnalyticsRequest` objects, which contain the inputs
        and outputs of the query.

        Using an explicit :class:`~.AnalyticsQuery`::

            query = AnalyticsQuery(
                ""SELECT VALUE bw FROM breweries bw WHERE bw.name = ?"", ""Kona Brewing"")
            for row in cb.analytics_query(query, ""127.0.0.1""):
                print('Entry: {0}'.format(row))

        Using an implicit :class:`~.AnalyticsQuery`::

            for row in cb.analytics_query(
                ""SELECT VALUE bw FROM breweries bw WHERE bw.name = ?"", ""127.0.0.1"", ""Kona Brewing""):
                print('Entry: {0}'.format(row))

        :param query: The query to execute. This may either be a
            :class:`.AnalyticsQuery` object, or a string (which will be
            implicitly converted to one).
        :param host: The host to send the request to.
        :param args: Positional arguments for :class:`.AnalyticsQuery`.
        :param kwargs: Named arguments for :class:`.AnalyticsQuery`.
        :return: An iterator which yields rows. Each row is a dictionary
            representing a single result"
"def write_back_memory(self, where, expr, size):
        
        if self.write_backs_disabled:
            return
        if type(expr) is bytes:
            self._emu.mem_write(where, expr)
        else:
            if issymbolic(expr):
                data = [Operators.CHR(Operators.EXTRACT(expr, offset, 8)) for offset in range(0, size, 8)]
                concrete_data = []
                for c in data:
                    if issymbolic(c):
                        c = chr(solver.get_value(self._cpu.memory.constraints, c))
                    concrete_data.append(c)
                data = concrete_data
            else:
                data = [Operators.CHR(Operators.EXTRACT(expr, offset, 8)) for offset in range(0, size, 8)]
            logger.debug(f""Writing back {hr_size(size // 8)} to {hex(where)}: {data}"")
            ",Copy memory writes from Manticore back into Unicorn in real-time
"def CheckPathExists(path):
  
  i = 0
  root, ext = os.path.splitext(path)
  while os.path.exists(path):
    i = i + 1
    goodlogging.Log.Info(""UTIL"", ""Path {0} already exists"".format(path))
    path = ""{0}_{1}"".format(root, i) + ext
  return path","Check if path exists, if it does add number to path (incrementing until
  a unique path is found).

  Parameters
  ----------
    path : string
      Path of directory to try.

  Returns
  ----------
    string
      Path of unique directory."
"def create(self, metric_id, value, timestamp=None):
        
        data = ApiParams()
        data[] = value
        data[] = timestamp
        return self._post( % metric_id, data=data)[]","Add a Metric Point to a Metric

        :param int metric_id: Metric ID
        :param int value: Value to plot on the metric graph
        :param str timestamp: Unix timestamp of the point was measured
        :return: Created metric point data (:class:`dict`)

        .. seealso:: https://docs.cachethq.io/reference#post-metric-points"
"def get_element_attribute(elem_to_parse, attrib_name, default_value=u):
    

    element = get_element(elem_to_parse)

    if element is None:
        return default_value

    return element.attrib.get(attrib_name, default_value)",":return: an attribute from the parsed element if it has the attribute,
    otherwise the default value"
"def show_gateway_device(self, gateway_device_id, **_params):
        
        return self.get(self.gateway_device_path % gateway_device_id,
                        params=_params)",Fetch a gateway device.
"def detect_format(text, handlers):
    
    for pattern, handler in handlers.items():
        if pattern.match(text):
            return handler

    
    return None","Figure out which handler to use, based on metadata.
    Returns a handler instance or None.

    ``text`` should be unicode text about to be parsed.

    ``handlers`` is a dictionary where keys are opening delimiters 
    and values are handler instances."
"def jsonrpc_map(self):
        
        result = ""<h1>JSON-RPC map</h1><pre>{0}</pre>"".format(""\n\n"".join([
            ""{0}: {1}"".format(fname, f.__doc__)
            for fname, f in self.dispatcher.items()
        ]))
        return Response(result)","Map of json-rpc available calls.

        :return str:"
"def authenticate(self, transport, account_name, password):
        
        if not isinstance(transport, ZimbraClientTransport):
            raise ZimbraClientException()

        if util.empty(account_name):
            raise AuthException()","Authenticates account, if no password given tries to pre-authenticate.
        @param transport: transport to use for method calls
        @param account_name: account name
        @param password: account password
        @return: AuthToken if authentication succeeded
        @raise AuthException: if authentication fails"
"def on_slice(self, node):    
        
        return slice(self.run(node.lower),
                     self.run(node.upper),
                     self.run(node.step))",Simple slice.
"def _scan_footpaths_to_departure_stop(self, connection_dep_stop, connection_dep_time, arrival_time_target):
        
        for _, neighbor, data in self._walk_network.edges_iter(nbunch=[connection_dep_stop],
                                                               data=True):
            d_walk = data[]
            neighbor_dep_time = connection_dep_time - d_walk / self._walk_speed
            pt = LabelTimeSimple(departure_time=neighbor_dep_time, arrival_time_target=arrival_time_target)
            self._stop_profiles[neighbor].update_pareto_optimal_tuples(pt)",A helper method for scanning the footpaths. Updates self._stop_profiles accordingly
"async def cursor(self, conn=None, *args, **kwargs):
        
        in_transaction = conn is not None
        if not conn:
            conn = await self.acquire()
        cursor = await conn.cursor(*args, **kwargs)
        cursor.release = functools.partial(
            self.release_cursor, cursor,
            in_transaction=in_transaction)
        return cursor","Get a cursor for the specified transaction connection
        or acquire from the pool."
"def __collect_trace_data(self, request, response, error, latency):
        

        data = OrderedDict()
        data[] = latency.elapsed
        data[] = request.environ[]
        data[] = request.url
        data[] = request.headers

        body = request.get_data(as_text=True)
        if body:
            data[] = body

        if response:
            data[] = response.status_code

        if error:
            data[] = str(error)

        return data","Collects the tracing data from the given parameters.
        :param request: The Flask request.
        :param response: The flask response.
        :param error: The error occurred if any.
        :param latency: The time elapsed to process the request.
        :return: The tracing data."
"def parse_scope(self, tup_tree):
        

        self.check_node(tup_tree, , (),
                        (, , , ,
                         , , ), ())

        
        
        
        scopes = NocaseDict()
        for k, v in attrs(tup_tree).items():
            v_ = self.unpack_boolean(v)
            if v_ is None:
                raise CIMXMLParseError(
                    _format(""Element {0!A} has an invalid value {1!A} for its ""
                            ""boolean attribute {2!A}"", name(tup_tree), v, k),
                    conn_id=self.conn_id)
            scopes[k] = v_
        return scopes","Parse a SCOPE element and return a dictionary with an item for each
        specified scope attribute.

        The keys of the dictionary items are the scope names in upper case; the
        values are the Python boolean values True or False.

        Unspecified scope attributes are not represented in the returned
        dictionary; the user is expected to assume their default value of
        False.

        The returned dictionary does not preserve order of the scope
        attributes.

          ::

            <!ELEMENT SCOPE EMPTY>
            <!ATTLIST SCOPE
                CLASS (true | false) ""false""
                ASSOCIATION (true | false) ""false""
                REFERENCE (true | false) ""false""
                PROPERTY (true | false) ""false""
                METHOD (true | false) ""false""
                PARAMETER (true | false) ""false""
                INDICATION (true | false) ""false"""
"def _bam_coverage(name, bam_input, data):
    
    cmd = (""{bam_coverage} -b {bam_input} -o {bw_output} ""
          ""--binSize 20 --effectiveGenomeSize {size} ""
          ""--smoothLength 60 --extendReads 150 --centerReads -p {cores}"")
    size = bam.fasta.total_sequence_length(dd.get_ref_file(data))
    cores = dd.get_num_cores(data)
    try:
        bam_coverage = config_utils.get_program(""bamCoverage"", data)
    except config_utils.CmdNotFound:
        logger.info(""No bamCoverage found, skipping bamCoverage."")
        return None
    resources = config_utils.get_resources(""bamCoverage"", data[""config""])
    if resources:
        options = resources.get(""options"")
        if options:
            cmd += "" %s"" % "" "".join([str(x) for x in options])
    bw_output = os.path.join(os.path.dirname(bam_input), ""%s.bw"" % name)
    if utils.file_exists(bw_output):
        return bw_output
    with file_transaction(bw_output) as out_tx:
        do.run(cmd.format(**locals()), ""Run bamCoverage in %s"" % name)
    return bw_output",Run bamCoverage from deeptools
"def _get_standalone_app_url(self, app_id, spark_master_address, requests_config, tags):
        
        app_page = self._rest_request(
            spark_master_address,
            SPARK_MASTER_APP_PATH,
            SPARK_STANDALONE_SERVICE_CHECK,
            requests_config,
            tags,
            appId=app_id,
        )

        dom = BeautifulSoup(app_page.text, )
        app_detail_ui_links = dom.find_all(, string=)

        if app_detail_ui_links and len(app_detail_ui_links) == 1:
            return app_detail_ui_links[0].attrs[]","Return the application URL from the app info page on the Spark master.
        Due to a bug, we need to parse the HTML manually because we cannot
        fetch JSON data from HTTP interface."
"def reset(self):
        
        
        self._start_line = 0
        self._x = self._y = None
        self._buffer = _DoubleBuffer(self._buffer_height, self.width)
        self._reset()",Reset the internal buffers for the abstract canvas.
"def send_signature_request(self, test_mode=False, files=None, file_urls=None, title=None, subject=None, message=None, signing_redirect_url=None, signers=None, cc_email_addresses=None, form_fields_per_document=None, use_text_tags=False, hide_text_tags=False, metadata=None, ux_version=None, allow_decline=False):
        s signature page

            cc_email_addresses (list, optional):    A list of email addresses that should be CC

        self._check_required_fields({
            ""signers"": signers
        }, [{
            ""files"": files,
            ""file_urls"": file_urls
            }]
        )

        params = {
            : test_mode,
            : files,
            : file_urls,
            : title,
            : subject,
            : message,
            : signing_redirect_url,
            : signers,
            : cc_email_addresses,
            : form_fields_per_document,
            : use_text_tags,
            : hide_text_tags,
            : metadata,
            : allow_decline
        }

        if ux_version is not None:
            params[] = ux_version

        return self._send_signature_request(**params)","Creates and sends a new SignatureRequest with the submitted documents

        Creates and sends a new SignatureRequest with the submitted documents.
        If form_fields_per_document is not specified, a signature page will be
        affixed where all signers will be required to add their signature,
        signifying their agreement to all contained documents.

        Args:

            test_mode (bool, optional):             Whether this is a test, the signature request will not be legally binding if set to True. Defaults to False.

            files (list of str):                    The uploaded file(s) to send for signature

            file_urls (list of str):                URLs of the file for HelloSign to download to send for signature. Use either `files` or `file_urls`

            title (str, optional):                  The title you want to assign to the SignatureRequest

            subject (str, optional):                The subject in the email that will be sent to the signers

            message (str, optional):                The custom message in the email that will be sent to the signers

            signing_redirect_url (str, optional):   The URL you want the signer redirected to after they successfully sign.

            signers (list of dict):                 A list of signers, which each has the following attributes:

                name (str):                         The name of the signer
                email_address (str):                Email address of the signer
                order (str, optional):              The order the signer is required to sign in
                pin (str, optional):                The 4- to 12-character access code that will secure this signer's signature page

            cc_email_addresses (list, optional):    A list of email addresses that should be CC'd

            form_fields_per_document (str):         The fields that should appear on the document, expressed as a serialized JSON data structure which is a list of lists of the form fields. Please refer to the API reference of HelloSign for more details (https://www.hellosign.com/api/reference#SignatureRequest)

            use_text_tags (bool, optional):         Use text tags in the provided file(s) to create form fields

            hide_text_tags (bool, optional):        Hide text tag areas

            metadata (dict, optional):              Metadata to associate with the signature request

            ux_version (int):                       UX version, either 1 (default) or 2.

            allow_decline(bool, optional):         Allows signers to decline to sign a document if set to 1. Defaults to 0.

        Returns:
            A SignatureRequest object"
"def command_line_runner():
    
    filename = sys.argv[-1]
    if not filename.endswith("".rst""):
        print(""ERROR! Please enter a ReStructuredText filename!"")
        sys.exit()
    print(rst_to_json(file_opener(filename)))",I run functions from the command-line!
"def represent_pixel_location(self):
        
        if self.data is None:
            return None

        
        return self._data.reshape(
            self.height + self.y_padding,
            int(self.width * self._num_components_per_pixel + self.x_padding)
        )","Returns a NumPy array that represents the 2D pixel location,
        which is defined by PFNC, of the original image data.

        You may use the returned NumPy array for a calculation to map the
        original image to another format.

        :return: A NumPy array that represents the 2D pixel location."
"def get_default_config(self):
        
        config = super(UDPCollector, self).get_default_config()
        config.update({
            :          ,
            :  +
                             
        })
        return config",Returns the default collector settings
"def write_error(self, status_code, **kwargs):
    
    if ""exc_info"" in kwargs:
      exc_info = kwargs[""exc_info""]
      error = exc_info[1]

      errormessage = ""%s: %s"" % (status_code, error)
      self.render(""error.html"", errormessage=errormessage)
    else:
      errormessage = ""%s"" % (status_code)
      self.render(""error.html"", errormessage=errormessage)",":param status_code:
    :param kwargs:
    :return:"
"def validate(self, input_parameters, context):
        
        errors = {}

        for key, type_handler in self.input_transformations.items():
            if self.raise_on_invalid:
                if key in input_parameters:
                    input_parameters[key] = self.initialize_handler(
                        type_handler,
                        input_parameters[key],
                        context=context
                    )
            else:
                try:
                    if key in input_parameters:
                        input_parameters[key] = self.initialize_handler(
                            type_handler,
                            input_parameters[key],
                            context=context
                        )
                except InvalidTypeData as error:
                    errors[key] = error.reasons or str(error.message)
                except Exception as error:
                    if hasattr(error, ) and error.args:
                        errors[key] = error.args[0]
                    else:
                        errors[key] = str(error)
        for require in self.required:
            if not require in input_parameters:
                errors[require] = ""Required parameter  not supplied"".format(require)
        if not errors and getattr(self, , False):
            errors = self.validate_function(input_parameters)
        return errors",Runs all set type transformers / validators against the provided input parameters and returns any errors
"def ystep(self):
        r

        if self.opt[] or self.opt[]:
            Y1 = self.block_sep1(self.Y)
            if self.opt[]:
                Y1[Y1 < 0.0] = 0.0
            if self.opt[]:
                for n in range(0, self.cri.dimN):
                    Y1[(slice(None),)*n +
                       (slice(1-self.D.shape[n], None),)] = 0.0
            self.block_sep1(self.Y)[:] = Y1","r""""""Minimise Augmented Lagrangian with respect to
        :math:`\mathbf{y}`."
"def update_association(self, association):
        
        bad_goids = set()
        
        for goids in association.values():
            parents = set()
            
            goids.update(parents)
        if bad_goids:
            sys.stdout.write(""{N} GO IDs in assc. are not found in the GO-DAG: {GOs}\n"".format(
                N=len(bad_goids), GOs="" "".join(bad_goids)))",Add the GO parents of a gene's associated GO IDs to the gene's association.
"def upload_batterystats(filename, username, password, bucket_name=BUCKET_NAME, bucket_desc=BUCKET_DESC):
    

    zapi = pyzenobase.ZenobaseAPI(username, password)
    bucket = zapi.create_or_get_bucket(bucket_name, description=bucket_desc)
    bucket_id = bucket[""@id""]

    events = []
    with open(filename, newline="""") as f:
        reader = csv.reader(f)
        header = next(reader)
        for row in reader:
            events.append({header[i]: row[i] for i in range(len(header))})

    print(""Read {} events"".format(len(events)))
    for event in events:
        
        event[""timestamp""] = pyzenobase.fmt_datetime(datetime.strptime(event.pop(""datetime""), ""%Y-%m-%d %H:%M:%S""), timezone=""Europe/Stockholm"")
        event[""tag""] = event.pop(""status"")
        event[""percentage""] = float(event.pop(""level""))
        event[""temperature""] = {""@value"": float(event.pop(""temperature"")), ""unit"": ""C""}
        event.pop(""voltage"")

    print(""Checking that events are valid..."")
    events = [pyzenobase.ZenobaseEvent(event) for event in events]
    print(""Uploading..."")
    zapi.create_events(bucket_id, events)
    zapi.close()
    print(""Done!"")",Works with CSVs generated by the Android app 'Battery Log' (https://play.google.com/store/apps/details?id=kr.hwangti.batterylog)
"def urljoin(*args):
    
    value = ""/"".join(map(lambda x: str(x).strip(), args))
    return ""/{}"".format(value)","Joins given arguments into a url, removing duplicate slashes
    Thanks http://stackoverflow.com/a/11326230/1267398

    >>> urljoin('/lol', '///lol', '/lol//')
    '/lol/lol/lol'"
"def _make_info(shape_list, num_classes):
  
  feature_info = collections.namedtuple(""FeatureInfo"", [""shape"", ""num_classes""])
  cur_shape = list(shape_list[0])
  
  for shape in shape_list:
    if len(shape) != len(cur_shape):
      raise ValueError(""Shapes need to have the same number of dimensions."")
    for i in range(len(shape)):
      if cur_shape[i] is not None:
        if shape[i] != cur_shape[i]:
          cur_shape[i] = None
  return feature_info(cur_shape, num_classes)",Create an info-like tuple for feature given some shapes and vocab size.
"def enumerate(self):
        

        done = False
        while not done:
            hset = self.get()

            if hset != None:
                self.block(hset)
                yield hset
            else:
                done = True","The method can be used as a simple iterator computing and blocking
            the hitting sets on the fly. It essentially calls :func:`get`
            followed by :func:`block`. Each hitting set is reported as a list
            of objects in the original problem domain, i.e. it is mapped back
            from the solutions over Boolean variables computed by the
            underlying oracle.

            :rtype: list(obj)"
"def read_memory_block32(self, addr, size):
        
        data = self.ap.read_memory_block32(addr, size)
        return self.bp_manager.filter_memory_aligned_32(addr, size, data)","read a block of aligned words in memory. Returns
        an array of word values"
"def getMaskArray(self, signature):
        
        if signature in self.masklist:
            mask =  self.masklist[signature]
        else:
            mask = None
        return mask",Returns the appropriate StaticMask array for the image.
"def progressbar(total, pos, msg=""""):
    
    width = get_terminal_size()[0] - 40
    rel_pos = int(float(pos) / total * width)
    bar = .join([""="" * rel_pos, ""."" * (width - rel_pos)])

    
    digits_total = len(str(total))
    fmt_width = ""%0"" + str(digits_total) + ""d""
    fmt = ""\r["" + fmt_width + ""/"" + fmt_width + ""][%s] %s""

    progress_stream.write(fmt % (pos, total, bar, msg))","Given a total and a progress position, output a progress bar
    to stderr. It is important to not output anything else while
    using this, as it relies soley on the behavior of carriage
    return (\\r).

    Can also take an optioal message to add after the
    progressbar. It must not contain newlines.

    The progress bar will look something like this:

    [099/500][=========...............................] ETA: 13:36:59

    Of course, the ETA part should be supplied be the calling
    function."
"def init(fname=None, format=None):
    

    fname = fname or theme.output_file
    format = format or theme.output_format

    if format == None:
        if not isinstance(fname, str):
            format = ""ps""
        elif re.search(""pdf$"", fname):
            format = ""pdf""
        elif re.search(""png$"", fname):
            format = ""png""
        elif re.search(""svg$"", fname):
            format = ""svg""
        else:
            format = ""ps""

    if format == ""ps"":
        can = pscanvas.T(fname)
    elif format == ""png"":
        can = pngcanvas.T(fname)
    elif format == ""x11"":
        can = x11canvas.T(fname)
    elif format == ""svg"":
        can = svgcanvas.T(fname)
    elif format == ""pdf-uncompressed"":
        can = pdfcanvas.T(fname, False)
    else:
        can = pdfcanvas.T(fname, theme.compress_output)
    return can","This is a ""factory"" procedure that creates a new canvas.T object.
    Both parameters, <fname> and
    <format>, are optional. Parameter <fname> specifies either the output
    file name or a file object. Parameter <format>, if specified, defines the
    file's format. Its value must be one of ""ps"", ""pdf"", ""svg"", ""x11"", or
    ""png"".

    When <fname> is omitted or is None, the output is sent to standard
    output. When <format> is omitted, it is guessed from the <fname>'s
    suffix; failing that, ""ps"" is selected."
"def p_duration_conversion(self, p):
        
        logger.debug(, p[1], p[3])
        p[0] = .format(p[1], p[3])",duration : duration IN DURATION_UNIT
"def _ctypes_indices(parameter):
    
    if (parameter.dimension is not None and "":"" in parameter.dimension):
        splice = _ctypes_splice(parameter)
        if ""out"" in parameter.direction:
            
            
            return (""integer, intent(inout) :: {}"".format(splice), False)
        else:
            return (""integer, intent(in) :: {}"".format(splice), False)","Returns code for parameter variable declarations specifying the size of each
    dimension in the specified parameter."
"def _float_precision_changed(self, name, old, new):
        

        if  in new:
            
            fmt = new
            try:
                fmt%3.14159
            except Exception:
                raise ValueError(""Precision must be int or format string, not %r""%new)
        elif new:
            
            try:
                i = int(new)
                assert i >= 0
            except ValueError:
                raise ValueError(""Precision must be int or format string, not %r""%new)
            except AssertionError:
                raise ValueError(""int precision must be non-negative, not %r""%i)

            fmt = %i
            if  in sys.modules:
                
                import numpy
                numpy.set_printoptions(precision=i)
        else:
            
            fmt = 
            if  in sys.modules:
                import numpy
                
                numpy.set_printoptions(precision=8)
        self.float_format = fmt","float_precision changed, set float_format accordingly.

        float_precision can be set by int or str.
        This will set float_format, after interpreting input.
        If numpy has been imported, numpy print precision will also be set.

        integer `n` sets format to '%.nf', otherwise, format set directly.

        An empty string returns to defaults (repr for float, 8 for numpy).

        This parameter can be set via the '%precision' magic."
"def increment(self, name, value):
        
        for counter in self._counters:
            counter.increment(name, value)","Increments counter by given value.

        :param name: a counter name of Increment type.

        :param value: a value to add to the counter."
"def focus(self, focus: Optional[URIPARM]) -> None:
        
        self._focus = normalize_uriparm(focus) if focus else None","Set the focus node(s).  If no focus node is specified, the evaluation will occur for all non-BNode
        graph subjects.  Otherwise it can be a string, a URIRef or a list of string/URIRef combinations

        :param focus: None if focus should be all URIRefs in the graph otherwise a URI or list of URI's"
"def plugins_all(self):
        

        if not self.loaded:
            self.load_modules()

        
        return get_plugins()[self.group]._filter(blacklist=self.blacklist,
                                                 type_filter=self.type_filter)","All resulting versions of all plugins in the group filtered by ``blacklist``

        Returns:
            dict: Nested dictionary of plugins accessible through dot-notation.

        Similar to :py:attr:`plugins`, but lowest level is a regular dictionary of
        all unfiltered plugin versions for the given plugin type and name.

        Parent types are always included.
        Child plugins will only be included if at least one valid, non-blacklisted plugin
        is available."
"def run(self):
    
    run, tag = metrics.run_tag_from_session_and_metric(
        self._request.session_name, self._request.metric_name)
    body, _ = self._scalars_plugin_instance.scalars_impl(
        tag, run, None, scalars_plugin.OutputFormat.JSON)
    return body","Executes the request.

    Returns:
       An array of tuples representing the metric evaluations--each of the form
       (<wall time in secs>, <training step>, <metric value>)."
"def entry(self):
        
        api = Api()
        api.authenticate()
        return api.fetch_video(self.video_id)","Connects to Youtube Api and retrieves the video entry object

        Return:
            gdata.youtube.YouTubeVideoEntry"
"def save(self, *args, **kwargs):
        
        from bakery import tasks
        from django.contrib.contenttypes.models import ContentType
        
        if not kwargs.pop(, True):
            super(AutoPublishingBuildableModel, self).save(*args, **kwargs)
        
        else:
            
            
            try:
                preexisting = self.__class__.objects.get(pk=self.pk)
            except self.__class__.DoesNotExist:
                preexisting = None
            
            if not preexisting:
                
                if not self.get_publication_status() and \
                        preexisting.get_publication_status():
                    action = 
                
                else:
                    action = None
            
            
            
            with transaction.atomic():
                super(AutoPublishingBuildableModel, self).save(*args, **kwargs)
            
            ct = ContentType.objects.get_for_model(self.__class__)
            if action == :
                tasks.publish_object.delay(ct.pk, self.pk)
            elif action == :
                tasks.unpublish_object.delay(ct.pk, self.pk)","A custom save that publishes or unpublishes the object where
        appropriate.

        Save with keyword argument obj.save(publish=False) to skip the process."
"def get_user_data_configuration():
    
    from cloud_inquisitor import get_local_aws_session, app_config

    kms_region = app_config.kms_region
    session = get_local_aws_session()

    if session.get_credentials().method == :
        kms = session.client(, region_name=kms_region)
    else:
        sts = session.client()
        audit_role = sts.assume_role(RoleArn=app_config.aws_api.instance_role_arn, RoleSessionName=)
        kms = boto3.session.Session(
            audit_role[][],
            audit_role[][],
            audit_role[][],
        ).client(, region_name=kms_region)

    user_data_url = app_config.user_data_url
    res = requests.get(user_data_url)

    if res.status_code == 200:
        data = kms.decrypt(CiphertextBlob=b64decode(res.content))
        kms_config = json.loads(zlib.decompress(data[]).decode())

        app_config.database_uri = kms_config[]
    else:
        raise RuntimeError(.format(res.status_code, res.content))","Retrieve and update the application configuration with information from the user-data

    Returns:
        `None`"
"def get_field_value(self, field_key):
        

        def get_value(document, field_key):
            
            if document is None:
                return None

            current_key, new_key_array = trim_field_key(document, field_key)
            key_array_digit = int(new_key_array[-1]) if new_key_array and has_digit(new_key_array) else None
            new_key = make_key(new_key_array)

            if key_array_digit is not None and len(new_key_array) > 0:
                
                if len(new_key_array) == 1:
                    return_data = document._data.get(current_key, [])
                elif isinstance(document, BaseList):
                    return_list = []
                    if len(document) > 0:
                        return_list = [get_value(doc, new_key) for doc in document]
                    return_data = return_list
                else:
                    return_data = get_value(getattr(document, current_key), new_key)

            elif len(new_key_array) > 0:
                return_data = get_value(document._data.get(current_key), new_key)
            else:
                
                try: 
                    return_data = (document._data.get(None, None) if current_key == ""id"" else
                              document._data.get(current_key, None))
                except: 
                    return_data = document._data.get(current_key, None)

            return return_data

        if self.is_initialized:
            return get_value(self.model_instance, field_key)
        else:
            return None","Given field_key will return value held at self.model_instance.  If
        model_instance has not been provided will return None."
"def read_file(self):
        
        with open(self.filename, mode=, encoding=) as text_file:
            self.raw_file = text_file.read()  
        self.file_lines = [x.rstrip() for x in self.raw_file.splitlines()]","Grabs filename and enables it to be read.
        :return: raw_file = unaltered text; file_lines = text split by lines."
"def ji_windows(self, ij_win):  
        
        ji_windows = {}
        transform_src = self._layer_meta[self._res_indices[self._windows_res][0]][""transform""]
        for res in self._res_indices:
            transform_dst = self._layer_meta[self._res_indices[res][0]][""transform""]
            ji_windows[res] = window_from_window(window_src=self.windows[ij_win],
                                                 transform_src=transform_src,
                                                 transform_dst=transform_dst)
        return ji_windows","For a given specific window, i.e. an element of :attr:`windows`, get the windows of all resolutions.

        Arguments:
            ij_win {int} -- The index specifying the window for which to return the resolution-windows."
"def _set_log_bad_packet(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=log_bad_packet.log_bad_packet, is_container=, presence=True, yang_name=""log-bad-packet"", rest_name=""bad-packet"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u: {u: u, u: u}}, namespace=, defining_module=, yang_type=, is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          : ,
          : ""container"",
          : ,
        })

    self.__log_bad_packet = t
    if hasattr(self, ):
      self._set()","Setter method for log_bad_packet, mapped from YANG variable /rbridge_id/ipv6/router/ospf/log/log_bad_packet (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_log_bad_packet is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_log_bad_packet() directly.

    YANG Description: Configure logging for bad packets"
"def sort(self, column_name=None, reverse=False):
        
        if not column_name and self.options.has_key():
            column_name = self.options[].keys()[0]
        if column_name not in self.default_columns:
            raise ValueError(""%s isnsorted_byreverse': reverse}}","Sort rows in this table, preserving a record of how that
        sorting is done in TableFu.options['sorted_by']"
"def base_multinode_parser():
    
    base_parser = ArgumentParser(add_help=False)

    base_parser.add_argument(
        ,
        type=str,
        nargs=,
        help=""The URLs of the validator--usersappendUSERNAME[:PASSWORD]Specify the users to authorize requests, in the same order as the URLs, separate by commas. Passing empty strings between commas is supported.')

    return base_parser","Creates a parser with arguments specific to sending HTTP requests
    to multiple REST APIs.

    Returns:
        {ArgumentParser}: Base parser with default HTTP args"
"def Tphi(self,**kwargs): 
        
        if hasattr(self,):
            return self._Tphi
        (rperi,rap)= self.calcRapRperi(**kwargs)
        if rap == rperi:
            return 2.*m.pi*self._R/self._vT
        TR= self.TR(**kwargs)
        I= self.I(**kwargs)
        Tphi= TR/I*m.pi
        self._Tphi= Tphi
        return self._Tphi","NAME:
           Tphi
        PURPOSE:
           Calculate the azimuthal period
        INPUT:
           +scipy.integrate.quadrature keywords
        OUTPUT:
           T_phi(R,vT,vT)/ro/vc + estimate of the error
        HISTORY:
           2010-12-01 - Written - Bovy (NYU)"
"def create(client, name):
    
    from renku.models.datasets import Author

    with client.with_dataset(name=name) as dataset:
        click.echo(, nl=False)
        author = Author.from_git(client.repo)
        if author not in dataset.authors:
            dataset.authors.append(author)

    click.secho(, fg=)",Create an empty dataset in the current repo.
"def fileattr_from_metadata(md):
    
    
    try:
        mdattr = json.loads(
            md[JSON_KEY_BLOBXFER_METADATA])[_JSON_KEY_FILE_ATTRIBUTES]
    except (KeyError, TypeError):
        return None
    else:
        if blobxfer.util.on_windows():
            global _FILEATTR_WARNED_ON_WINDOWS
            if not _FILEATTR_WARNED_ON_WINDOWS:
                _FILEATTR_WARNED_ON_WINDOWS = True
                logger.warning(
                    
                    )
            fileattr = None
        else:
            try:
                fileattr = PosixFileAttr(
                    mode=mdattr[_JSON_KEY_FILE_ATTRIBUTES_POSIX][
                        _JSON_KEY_FILE_ATTRIBUTES_MODE],
                    uid=mdattr[_JSON_KEY_FILE_ATTRIBUTES_POSIX][
                        _JSON_KEY_FILE_ATTRIBUTES_UID],
                    gid=mdattr[_JSON_KEY_FILE_ATTRIBUTES_POSIX][
                        _JSON_KEY_FILE_ATTRIBUTES_GID],
                )
            except KeyError:
                fileattr = None
        return fileattr","Convert fileattr metadata in json metadata
    :param dict md: metadata dictionary
    :rtype: PosixFileAttr or WindowsFileAttr or None
    :return: fileattr metadata"
"def _set_target_root_count_in_runtracker(self):
    
    
    
    target_count = len(self._target_roots)
    self.run_tracker.pantsd_stats.set_target_root_size(target_count)
    return target_count",Sets the target root count in the run tracker's daemon stats object.
"def application_gateways(self):
        
        api_version = self._get_api_version()
        if api_version == :
            from .v2015_06_15.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2016_09_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2016_12_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2017_03_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2017_06_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2017_08_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2017_09_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2017_10_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2017_11_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2018_01_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2018_02_01.operations import ApplicationGatewaysOperations as OperationClass
        elif api_version == :
            from .v2018_04_01.operations import ApplicationGatewaysOperations as OperationClass
        else:
            raise NotImplementedError(""APIVersion {} is not available"".format(api_version))
        return OperationClass(self._client, self.config, Serializer(self._models_dict(api_version)), Deserializer(self._models_dict(api_version)))","Instance depends on the API version:

           * 2015-06-15: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2015_06_15.operations.ApplicationGatewaysOperations>`
           * 2016-09-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2016_09_01.operations.ApplicationGatewaysOperations>`
           * 2016-12-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2016_12_01.operations.ApplicationGatewaysOperations>`
           * 2017-03-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2017_03_01.operations.ApplicationGatewaysOperations>`
           * 2017-06-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2017_06_01.operations.ApplicationGatewaysOperations>`
           * 2017-08-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2017_08_01.operations.ApplicationGatewaysOperations>`
           * 2017-09-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2017_09_01.operations.ApplicationGatewaysOperations>`
           * 2017-10-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2017_10_01.operations.ApplicationGatewaysOperations>`
           * 2017-11-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2017_11_01.operations.ApplicationGatewaysOperations>`
           * 2018-01-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2018_01_01.operations.ApplicationGatewaysOperations>`
           * 2018-02-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2018_02_01.operations.ApplicationGatewaysOperations>`
           * 2018-04-01: :class:`ApplicationGatewaysOperations<azure.mgmt.network.v2018_04_01.operations.ApplicationGatewaysOperations>`"
"def sum_tbl(tbl, kfield, vfields):
    
    pairs = [(n, tbl.dtype[n]) for n in [kfield] + vfields]
    dt = numpy.dtype(pairs + [(, int)])

    def sum_all(group):
        vals = numpy.zeros(1, dt)[0]
        for rec in group:
            for vfield in vfields:
                vals[vfield] += rec[vfield]
            vals[] += 1
        vals[kfield] = rec[kfield]
        return vals
    rows = groupby(tbl, operator.itemgetter(kfield), sum_all).values()
    array = numpy.zeros(len(rows), dt)
    for i, row in enumerate(rows):
        for j, name in enumerate(dt.names):
            array[i][name] = row[j]
    return array","Aggregate a composite array and compute the totals on a given key.

    >>> dt = numpy.dtype([('name', (bytes, 10)), ('value', int)])
    >>> tbl = numpy.array([('a', 1), ('a', 2), ('b', 3)], dt)
    >>> sum_tbl(tbl, 'name', ['value'])['value']
    array([3, 3])"
"def fit_transform(self, X):
        
        X = as_features(X, stack=True)
        self.kmeans_fit_ = copy(self.kmeans)
        assignments = self.kmeans_fit_.fit_predict(X.stacked_features) 
        return self._group_assignments(X, assignments)","Compute clustering and transform a list of bag features into its
        bag-of-words representation. Like calling fit(X) and then transform(X),
        but more efficient.

        Parameters
        ----------
        X : :class:`skl_groups.features.Features` or list of bag feature arrays
            New data to transform.

        Returns
        -------
        X_new : integer array, shape [len(X), kmeans.n_clusters]
            X transformed into the new space."
"def merge_chromosome_dfs(df_tuple):
    
    

    plus_df, minus_df = df_tuple
    index_cols = ""Chromosome Bin"".split()
    count_column = plus_df.columns[0]

    if plus_df.empty:
        return return_other(minus_df, count_column, index_cols)
    if minus_df.empty:
        return return_other(plus_df, count_column, index_cols)

    
    
    plus_df = plus_df.groupby(index_cols).sum()
    minus_df = minus_df.groupby(index_cols).sum()

    
    df = pd.concat([plus_df, minus_df], axis=1).fillna(0).sum(axis=1)
    df = df.reset_index().sort_values(by=""Bin"")

    df.columns = [""Chromosome"", ""Bin"", count_column]

    df = df.sort_values([""Chromosome"", ""Bin""])
    df[[""Bin"", count_column]] = df[[""Bin"", count_column]].astype(int32)
    df = df[[count_column, ""Chromosome"", ""Bin""]]
    return df.reset_index(drop=True)",Merges data from the two strands into strand-agnostic counts.
"def parse_10qk(self, response):
        
        loader = ReportItemLoader(response=response)
        item = loader.load_item()

        if  in item:
            doc_type = item[]
            if doc_type in (, ):
                return item

        return None",Parse 10-Q or 10-K XML report.
"def close(self):
        
        self._closing = True
        for i in xrange(self.size):
            self.inq.put(_STOP)","shut down the pool's workers

        this method sets the :attr:`closing` attribute, and once all queued
        work has been completed it will set the :attr:`closed` attribute"
"def populate_startup_nodes(self):
        
        for item in self.startup_nodes:
            self.set_node_name(item)

        for n in self.nodes.values():
            if n not in self.startup_nodes:
                self.startup_nodes.append(n)

        
        uniq = {frozenset(node.items()) for node in self.startup_nodes}
        
        self.startup_nodes = [dict(node) for node in uniq]",Do something with all startup nodes and filters out any duplicates
"def gradient(x, a, c):
    
    return jac(x, a).T.dot(g(x, a, c))",J'.G
"def e(self, eid):
    
    ta = datetime.datetime.now()
    rs = self.rest(, self.uri_db + , data={:int(eid)}, parse=True)
    tb =  datetime.datetime.now() - ta
    print cl( % (eid, tb.microseconds/1000.0), )
    return rs",Get an Entity
"def output_story_prefixes(self):
        

        if not self.test_story:
            raise NotImplementedError(
                ""I want to write the prefixes to a file""
                ""called <test_story>_prefixes.txt, but there's no test_story."")

        fn = os.path.join(TGT_DIR, ""%s_prefixes.txt"" % self.test_story)
        with open(fn, ""w"") as f:
            for utter_id in self.test_prefixes:
                print(utter_id.split(""/"")[1], file=f)","Writes the set of prefixes to a file this is useful for pretty
        printing in results.latex_output."
"def CreateLink(target_path, link_path, override=True):
    
    _AssertIsLocal(target_path)
    _AssertIsLocal(link_path)

    if override and IsLink(link_path):
        DeleteLink(link_path)

    
    dirname = os.path.dirname(link_path)
    if dirname:
        CreateDirectory(dirname)

    if sys.platform != :
        return os.symlink(target_path, link_path)  
    else:
        
        

        import jaraco.windows.filesystem
        return jaraco.windows.filesystem.symlink(target_path, link_path)

        from ._easyfs_win32 import CreateSymbolicLink
        try:
            dw_flags = 0
            if target_path and os.path.isdir(target_path):
                dw_flags = 1
            return CreateSymbolicLink(target_path, link_path, dw_flags)
        except Exception as e:
            reraise(e,  % locals())","Create a symbolic link at `link_path` pointing to `target_path`.

    :param unicode target_path:
        Link target

    :param unicode link_path:
        Fullpath to link name

    :param bool override:
        If True and `link_path` already exists as a link, that link is overridden."
"def parse(cls, root):
        
        if root.tag != utils.lxmlns(""mets"") + ""mdWrap"":
            raise exceptions.ParseError(
                ""MDWrap can only parse mdWrap elements with METS namespace.""
            )
        mdtype = root.get(""MDTYPE"")
        if not mdtype:
            raise exceptions.ParseError(""mdWrap must have a MDTYPE"")
        othermdtype = root.get(""OTHERMDTYPE"")
        document = root.xpath(""mets:xmlData/*"", namespaces=utils.NAMESPACES)
        if len(document) == 0:
            raise exceptions.ParseError(
                ""All mdWrap/xmlData elements must have at least one child; this""
                "" one has none""
            )
        elif len(document) == 1:
            document = document[0]

        
        document = copy.deepcopy(document)

        return cls(document, mdtype, othermdtype)","Create a new MDWrap by parsing root.

        :param root: Element or ElementTree to be parsed into a MDWrap.
        :raises exceptions.ParseError: If mdWrap does not contain MDTYPE
        :raises exceptions.ParseError: If xmlData contains no children"
"def _create_core_dns_instance(self, instance):
        
        endpoint = instance.get()
        if endpoint is None:
            raise ConfigurationError(""Unable to find prometheus endpoint in config file."")

        metrics = [DEFAULT_METRICS, GO_METRICS]
        metrics.extend(instance.get(, []))

        instance.update({: endpoint, : , : metrics})

        return instance",Set up coredns instance so it can be used in OpenMetricsBaseCheck
"def clipRegionToScreen(self):
        
        if not self.isRegionValid():
            return None
        screens = PlatformManager.getScreenDetails()
        total_x, total_y, total_w, total_h = Screen(-1).getBounds()
        containing_screen = None
        for screen in screens:
            s_x, s_y, s_w, s_h = screen[""rect""]
            if self.x >= s_x and self.x+self.w <= s_x+s_w and self.y >= s_y and self.y+self.h <= s_y+s_h:
                
                return self
            elif self.x+self.w <= s_x or s_x+s_w <= self.x or self.y+self.h <= s_y or s_y+s_h <= self.y:
                
                continue
            elif self.x == total_x and self.y == total_y and self.w == total_w and self.h == total_h:
                
                return self
            else:
                
                x = max(self.x, s_x)
                y = max(self.y, s_y)
                w = min(self.w, s_w)
                h = min(self.h, s_h)
                return Region(x, y, w, h)
        return None","Returns the part of the region that is visible on a screen

        If the region equals to all visible screens, returns Screen(-1).
        If the region is visible on multiple screens, returns the screen with the smallest ID.
        Returns None if the region is outside the screen."
"def parse(readDataInstance):
        
        configDir = ImageLoadConfigDirectory64()

        configDir.size.value = readDataInstance.readDword()
        configDir.timeDateStamp.value = readDataInstance.readDword()
        configDir.majorVersion.value = readDataInstance.readWord()
        configDir.minorVersion.value = readDataInstance.readWord()
        configDir.globalFlagsClear.value = readDataInstance.readDword()
        configDir.globalFlagsSet.value = readDataInstance.readDword()
        configDir.criticalSectionDefaultTimeout.value = readDataInstance.readDword()
        configDir.deCommitFreeBlockThreshold.value = readDataInstance.readQword()
        configDir.deCommitTotalFreeThreshold.value = readDataInstance.readQword()
        configDir.lockPrefixTable.value = readDataInstance.readQword()
        configDir.maximumAllocationSize.value = readDataInstance.readQword()
        configDir.virtualMemoryThreshold.value = readDataInstance.readQword()
        configDir.processAffinityMask.value = readDataInstance.readQword()
        configDir.processHeapFlags.value = readDataInstance.readDword()
        configDir.cdsVersion.value = readDataInstance.readWord()
        configDir.reserved1.value = readDataInstance.readWord()
        configDir.editList.value = readDataInstance.readQword()
        configDir.securityCookie.value = readDataInstance.readQword()
        configDir.SEHandlerTable.value = readDataInstance.readQword()
        configDir.SEHandlerCount.value = readDataInstance.readQword()

        
        configDir.GuardCFCheckFunctionPointer.value = readDataInstance.readQword() 
        configDir.Reserved2.value = readDataInstance.readQword()
        configDir.GuardCFFunctionTable.value = readDataInstance.readQword() 
        configDir.GuardCFFunctionCount.value = readDataInstance.readQword()
        configDir.GuardFlags.value = readDataInstance.readQword()
        return configDir","Returns a new L{ImageLoadConfigDirectory64} object.
        
        @type readDataInstance: L{ReadData}
        @param readDataInstance: A L{ReadData} object containing data to create a new L{ImageLoadConfigDirectory64} object.
        
        @rtype: L{ImageLoadConfigDirectory64}
        @return: A new L{ImageLoadConfigDirectory64} object."
"def get_expire_brok(self, host_name, service_name=):
        
        data = self.serialize()
        data[] = host_name
        if service_name != :
            data[] = service_name

        return Brok({: , : data})","Get an expire downtime brok

        :param host_name: host concerned by the downtime
        :type host_name
        :param service_name: service concerned by the downtime
        :type service_name
        :return: brok with wanted data
        :rtype: alignak.brok.Brok"
"def get_filter(self, **filter_kwargs):
        

        q_objects = super(ListView, self).get_filter(**filter_kwargs)
        form = self.get_filter_form()
        if form:
            q_objects.extend(form.get_filter())

        return q_objects","Combines the Q objects returned by a valid
        filter form with any other arguments and
        returns a list of Q objects that can be passed
        to a queryset."
"def rotation(self):
        
        rs = dict(PORTRAIT=0, LANDSCAPE=1, UIA_DEVICE_ORIENTATION_LANDSCAPERIGHT=3)
        return rs.get(self.session.orientation, 0)","Rotation of device
        Returns:
            int (0-3)"
"def movePage(self, pno, to = -1):
        
        pl = list(range(len(self)))
        if pno < 0 or pno > pl[-1]:
            raise ValueError("" page number out of range"")
        if to < -1 or to > pl[-1]:
            raise ValueError("" page number out of range"")
        pl.remove(pno)
        if to == -1:
            pl.append(pno)
        else:
            pl.insert(to-1, pno)
        return self.select(pl)",Move a page to before some other page of the document. Specify 'to = -1' to move after last page.
"def import_from_nhmmer_table(hmmout_path):
        
        
        
        
        res=HMMSearchResult()
        res.fields = [
                       SequenceSearchResult.QUERY_ID_FIELD,
                       SequenceSearchResult.HMM_NAME_FIELD,
                       SequenceSearchResult.ALIGNMENT_LENGTH_FIELD,
                       SequenceSearchResult.QUERY_FROM_FIELD,
                       SequenceSearchResult.QUERY_TO_FIELD,
                       SequenceSearchResult.HIT_FROM_FIELD,
                       SequenceSearchResult.HIT_TO_FIELD,
                       SequenceSearchResult.ALIGNMENT_BIT_SCORE,
                       SequenceSearchResult.ALIGNMENT_DIRECTION,
                       ]
        
        for row in [x.rstrip().split() for x in open(hmmout_path) if not x.startswith()]:
            alifrom    = int(row[6])
            alito      = int(row[7])
            aln_length = (alito-alifrom if alito-alifrom>0 else alifrom-alito)
            res.results.append([row[0],
                                row[2],
                                aln_length,
                                int(row[4]),
                                int(row[5]),
                                alifrom,
                                alito,
                                row[13],
                                alito > alifrom
                                ])
        return res",Generate new results object from the output of nhmmer search
"def filter(self, texts):
        u
        return (text for text in texts if self.regex.match(text) is not None)","u""""""
        Returns a generator yielding the elements of `texts` matching this pattern.

        :type texts: typing.Iterable[text_type]
        :param texts: An iterable collection of texts to match
        :rtype: typing.Iterable[text_type]
        :return: A generator of filtered elements."
"def log(self, message: str):
        
        dset_log_id =  % self.iid

        if dset_log_id not in self.parent.data.keys():
            dset = self.parent.data.create_dataset(
                dset_log_id, shape=(1,),
                dtype=np.dtype([
                    (, ),
                    (, )
                ])
            )
        else:
            dset = self.parent.data[dset_log_id]

        timestamp = np.array(
            datetime.now().strftime(""%s"")
        ).astype().view()

        dset[] = timestamp.view()
        dset[] = message
        self.parent.data.flush()","@deprecated

        :param message:
        :return:"
"def direct_messages(self, since_id=None, max_id=None, count=None,
                        include_entities=None, skip_status=None):
        
        params = {}
        set_str_param(params, , since_id)
        set_str_param(params, , max_id)
        set_int_param(params, , count)
        set_bool_param(params, , include_entities)
        set_bool_param(params, , skip_status)
        return self._get_api(, params)","Gets the 20 most recent direct messages received by the authenticating
        user.

        https://dev.twitter.com/docs/api/1.1/get/direct_messages

        :param str since_id:
            Returns results with an ID greater than (that is, more recent than)
            the specified ID. There are limits to the number of Tweets which
            can be accessed through the API. If the limit of Tweets has occured
            since the since_id, the since_id will be forced to the oldest ID
            available.

        :params str max_id:
            Returns results with an ID less than (that is, older than) or equal
            to the specified ID.

        :param int count:
            Specifies the number of direct messages to try and retrieve, up to
            a maximum of ``200``. The value of count is best thought of as a
            limit to the number of Tweets to return because suspended or
            deleted content is removed after the count has been applied.

        :param bool include_entities:
            The entities node will not be included when set to ``False``.

        :param bool skip_status:
            When set to ``True``, statuses will not be included in the returned
            user objects.

        :returns:
            A list of direct message dicts."
"def to_dict(self):
        
        out_dict = dict(self)
        for k, v in out_dict.items():
            if v is self:
                out_dict[k] = out_dict
            elif hasattr(v, ):
                out_dict[k] = v.to_dict()
            elif hasattr(v, ):
                out_dict[k] = v.to_list()
        return out_dict","Turn the Box and sub Boxes back into a native
        python dictionary.

        :return: python dictionary of this Box"
"def SSTORE(self, offset, value):
        
        storage_address = self.address
        self._publish(, storage_address, offset, value)
        
        
        
        

        if istainted(self.pc):
            for taint in get_taints(self.pc):
                value = taint_with(value, taint)
        self.world.set_storage_data(storage_address, offset, value)
        self._publish(, storage_address, offset, value)",Save word to storage
"def logProbability(self, distn):
    
    x = numpy.asarray(distn)
    n = x.sum()
    return (logFactorial(n) - numpy.sum([logFactorial(k) for k in x]) +
      numpy.sum(x * numpy.log(self.dist.pmf)))",Form of distribution must be an array of counts in order of self.keys.
"def must_exist(*components):
    
    _path = path(*components)
    if not exists(_path):
        raise File404(_path)
    return _path","Ensure path exists.

    Arguments:
        *components (str[]): Path components.

    Returns:
        str: File path.

    Raises:
        File404: If path does not exist."
"def parseJuiceHeaders(lines):
    
    b = JuiceBox()
    bodylen = 0
    key = None
    for L in lines:
        if L[0] == :
            
            assert key is not None
            b[key] += +L[1:]
            continue
        parts = L.split(, 1)
        if len(parts) != 2:
            raise MalformedJuiceBox(""Wrong number of parts: %r"" % (L,))
        key, value = parts
        key = normalizeKey(key)
        b[key] = value
    return int(b.pop(LENGTH, 0)), b","Create a JuiceBox from a list of header lines.

    @param lines: a list of lines."
"def cget(self, key):
        
        if key is ""canvaswidth"":
            return self._canvaswidth
        elif key is ""canvasheight"":
            return self._canvasheight
        elif key is ""function_new"":
            return self._function_new
        elif key is ""callback_add"":
            return self._callback_add
        elif key is ""callback_del"":
            return self._callback_del
        elif key is ""callback_move"":
            return self._callback_move
        else:
            ttk.Frame.cget(self, key)","Query widget option.

        :param key: option name
        :type key: str
        :return: value of the option

        To get the list of options for this widget, call the method :meth:`~ItemsCanvas.keys`."
"def CheckClientLabels(client_id,
                      labels_whitelist=None,
                      labels_owners_whitelist=None,
                      token=None):
  

  labels_whitelist = labels_whitelist or []
  labels_owners_whitelist = labels_owners_whitelist or []

  if data_store.RelationalDBEnabled():
    labels = data_store.REL_DB.ReadClientLabels(str(client_id))
  else:
    with aff4.FACTORY.Open(
        client_id.ToClientURN(), aff4_type=aff4_grr.VFSGRRClient,
        token=token) as fd:
      labels = fd.GetLabels()

  for label in labels:
    if (label.name in labels_whitelist and
        label.owner in labels_owners_whitelist):
      return

  raise access_control.UnauthorizedAccess(
      ""Client %s doesn't have necessary labels."" % utils.SmartStr(client_id))",Checks a given client against labels/owners whitelists.
"def AfterTransitionEventHandler(instance, event):
    
    if call_workflow_event(instance, event, after=True):
        return

    
    
    if not event.transition:
        return
    
        after_event = getattr(instance, key, False)
    if not after_event:
        return
    after_event()","This event is executed after each transition and delegates further
    actions to 'workflow.<portal_type>.events.after_<transition_id> function
    if exists for the instance passed in.
    :param instance: the instance that has been transitioned
    :type instance: ATContentType
    :param event: event that holds the transition performed
    :type event: IObjectEvent"
"def week(self):
        
        doc = self.get_doc()
        raw = doc().attr[]
        match = re.match(
            r.format(self.season()), raw
        )
        if match:
            return int(match.group(1))
        else:
            return 21","Returns the week in which this game took place. 18 is WC round, 19
        is Div round, 20 is CC round, 21 is SB.
        :returns: Integer from 1 to 21."
"def write(self, image, options, thumbnail):
        
        format_ = options[]
        quality = options[]
        image_info = options.get(, {})
        
        progressive = options.get(, settings.THUMBNAIL_PROGRESSIVE)
        raw_data = self._get_raw_data(
            image, format_, quality,
            image_info=image_info,
            progressive=progressive
        )
        thumbnail.write(raw_data)",Wrapper for ``_write``
"def request_method(self, method: str,
                       **method_kwargs: Union[str, int]) -> dict:
        
        response = self.session.send_method_request(method, method_kwargs)
        self.check_for_errors(method, method_kwargs, response)
        return response","Process method request and return json with results

        :param method: str: specifies the method, example: ""users.get""
        :param method_kwargs: dict: method parameters,
        example: ""users_id=1"", ""fields='city, contacts'"""
"def extract_secs(self, tx, tx_in_idx):
        
        sc = tx.SolutionChecker(tx)
        tx_context = sc.tx_context_for_idx(tx_in_idx)
        
        solution_stack = []
        for puzzle_script, solution_stack, flags, sighash_f in sc.puzzle_and_solution_iterator(tx_context):
            for opcode, data, pc, new_pc in self._script_tools.get_opcodes(puzzle_script):
                if data and is_sec(data):
                    yield data
            for data in solution_stack:
                if is_sec(data):
                    yield data","For a given script solution, iterate yield its sec blobs"
"def publish_amqp_message_by_unit(self, sentry_unit, message,
                                     queue=""test"", ssl=False,
                                     username=""testuser1"",
                                     password=""changeme"",
                                     port=None):
        
        self.log.debug(.format(queue,
                                                                    message))
        connection = self.connect_amqp_by_unit(sentry_unit, ssl=ssl,
                                               port=port,
                                               username=username,
                                               password=password)

        
        
        
        self.log.debug()
        channel = connection.channel()
        self.log.debug()
        channel.queue_declare(queue=queue, auto_delete=False, durable=True)
        self.log.debug()
        channel.basic_publish(exchange=, routing_key=queue, body=message)
        self.log.debug()
        channel.close()
        self.log.debug()
        connection.close()","Publish an amqp message to a rmq juju unit.

        :param sentry_unit: sentry unit pointer
        :param message: amqp message string
        :param queue: message queue, default to test
        :param username: amqp user name, default to testuser1
        :param password: amqp user password
        :param ssl: boolean, default to False
        :param port: amqp port, use defaults if None
        :returns: None.  Raises exception if publish failed."
"def _update_inernal(self, varp_list):
        
        
        self._b_prob_all = 1.-param_to_array(varp_list[0].gamma_group)
        [np.multiply(self._b_prob_all, 1.-vp.gamma_group, self._b_prob_all) for vp in varp_list[1:]]",Make an update of the internal status by gathering the variational posteriors for all the individual models.
"def fix_subparsers(subparsers):
    
    from dvc.utils.compat import is_py3

    if is_py3:  
        subparsers.required = True
        subparsers.dest = ""cmd""","Workaround for bug in Python 3. See more info at:
        https://bugs.python.org/issue16308
        https://github.com/iterative/dvc/issues/769

        Args:
            subparsers: subparsers to fix."
"def _generateMetricSpecs(options):
  
  inferenceType = options[]
  inferenceArgs = options[]
  predictionSteps = inferenceArgs[]
  metricWindow = options[]
  if metricWindow is None:
    metricWindow = int(Configuration.get(""nupic.opf.metricWindow""))

  metricSpecStrings = []
  optimizeMetricLabel = """"

  
  
  metricSpecStrings.extend(_generateExtraMetricSpecs(options))

  

  optimizeMetricSpec = None
  
  
  
  
  if options[]:
    assert len(predictionSteps) == 1
    predictionSteps = []

  
  
  if inferenceType in (InferenceType.TemporalNextStep,
                       InferenceType.TemporalAnomaly,
                       InferenceType.TemporalMultiStep,
                       InferenceType.NontemporalMultiStep,
                       InferenceType.NontemporalClassification,
                       ):

    predictedFieldName, predictedFieldType = _getPredictedField(options)
    isCategory = _isCategory(predictedFieldType)
    metricNames = (,) if isCategory else (, )
    trivialErrorMetric =  if isCategory else 
    oneGramErrorMetric =  if isCategory else 
    movingAverageBaselineName =  if isCategory else 

    
    for metricName in metricNames:
      metricSpec, metricLabel = \
        _generateMetricSpecString(field=predictedFieldName,
                 inferenceElement=InferenceElement.multiStepBestPredictions,
                 metric=,
                 params={: metricName,
                               :metricWindow,
                               : predictionSteps},
                 returnLabel=True)
      metricSpecStrings.append(metricSpec)

    
    if options[""customErrorMetric""] is not None :
      metricParams = dict(options[""customErrorMetric""])
      metricParams[] = 
      metricParams[] = predictionSteps
      
      if not ""errorWindow"" in metricParams:
        metricParams[""errorWindow""] = metricWindow
      metricSpec, metricLabel =_generateMetricSpecString(field=predictedFieldName,
                   inferenceElement=InferenceElement.multiStepPredictions,
                   metric=""multiStep"",
                   params=metricParams,
                   returnLabel=True)
      metricSpecStrings.append(metricSpec)

    
    
    optimizeMetricSpec = metricSpec
    metricLabel = metricLabel.replace(, )
    metricLabel = metricLabel.replace(, )
    optimizeMetricLabel = metricLabel

    if options[""customErrorMetric""] is not None :
      optimizeMetricLabel = "".*custom_error_metric.*""

    
    if options[""runBaselines""] \
          and inferenceType != InferenceType.NontemporalClassification:
      for steps in predictionSteps:
        metricSpecStrings.append(
          _generateMetricSpecString(field=predictedFieldName,
                                    inferenceElement=InferenceElement.prediction,
                                    metric=""trivial"",
                                    params={:metricWindow,
                                                  ""errorMetric"":trivialErrorMetric,
                                                  : steps})
          )

        
        
        
        
        
        
        
        
        
        
        
        
        if isCategory:
          metricSpecStrings.append(
            _generateMetricSpecString(field=predictedFieldName,
                                      inferenceElement=InferenceElement.prediction,
                                      metric=movingAverageBaselineName,
                                      params={:metricWindow
                                                    ,""errorMetric"":""avg_err"",
                                                    ""mode_window"":200,
                                                    ""steps"": steps})
            )
        else :
          metricSpecStrings.append(
            _generateMetricSpecString(field=predictedFieldName,
                                      inferenceElement=InferenceElement.prediction,
                                      metric=movingAverageBaselineName,
                                      params={:metricWindow
                                                    ,""errorMetric"":""altMAPE"",
                                                    ""mean_window"":200,
                                                    ""steps"": steps})
            )




  
  
  elif inferenceType in (InferenceType.TemporalClassification):

    metricName = 
    trivialErrorMetric = 
    oneGramErrorMetric = 
    movingAverageBaselineName = 

    optimizeMetricSpec, optimizeMetricLabel = \
      _generateMetricSpecString(inferenceElement=InferenceElement.classification,
                               metric=metricName,
                               params={:metricWindow},
                               returnLabel=True)

    metricSpecStrings.append(optimizeMetricSpec)

    if options[""runBaselines""]:
      
      if inferenceType == InferenceType.TemporalClassification:
        metricSpecStrings.append(
          _generateMetricSpecString(inferenceElement=InferenceElement.classification,
                                    metric=""trivial"",
                                    params={:metricWindow,
                                                  ""errorMetric"":trivialErrorMetric})
          )
        metricSpecStrings.append(
          _generateMetricSpecString(inferenceElement=InferenceElement.classification,
                                    metric=""two_gram"",
                                    params={:metricWindow,
                                                  ""errorMetric"":oneGramErrorMetric})
          )
        metricSpecStrings.append(
          _generateMetricSpecString(inferenceElement=InferenceElement.classification,
                                    metric=movingAverageBaselineName,
                                    params={:metricWindow
                                                  ,""errorMetric"":""avg_err"",
                                                  ""mode_window"":200})
          )


    
    if not options[""customErrorMetric""] == None :
      
      if not ""errorWindow"" in options[""customErrorMetric""]:
        options[""customErrorMetric""][""errorWindow""] = metricWindow
      optimizeMetricSpec = _generateMetricSpecString(
                                inferenceElement=InferenceElement.classification,
                                metric=""custom"",
                                params=options[""customErrorMetric""])
      optimizeMetricLabel = "".*custom_error_metric.*""

      metricSpecStrings.append(optimizeMetricSpec)


  
  
  
  if options[]:
    for i in range(len(metricSpecStrings)):
      metricSpecStrings[i] = metricSpecStrings[i].replace(
          """", ""predictionSteps"")
    optimizeMetricLabel = optimizeMetricLabel.replace(
        """", "".*"")
  return metricSpecStrings, optimizeMetricLabel","Generates the Metrics for a given InferenceType

  Parameters:
  -------------------------------------------------------------------------
  options: ExpGenerator options
  retval: (metricsList, optimizeMetricLabel)
            metricsList: list of metric string names
            optimizeMetricLabel: Name of the metric which to optimize over"
"def json_as_html(self):
        

        
        from cspreports import utils

        formatted_json = utils.format_report(self.json)
        return mark_safe(""<pre>\n%s</pre>"" % escape(formatted_json))",Print out self.json in a nice way.
"def size(self):
        
        if self._instrs is []:
            return None

        return sum([instr.size for instr in self._instrs])",Get basic block size.
"def tobinary(series, path, prefix=, overwrite=False, credentials=None):
    
    from six import BytesIO
    from thunder.utils import check_path
    from thunder.writers import get_parallel_writer

    if not overwrite:
        check_path(path, credentials=credentials)
        overwrite = True

    def tobuffer(kv):
        firstkey = None
        buf = BytesIO()
        for k, v in kv:
            if firstkey is None:
                firstkey = k
            buf.write(v.tostring())
        val = buf.getvalue()
        buf.close()
        if firstkey is None:
            return iter([])
        else:
            label = prefix +  + getlabel(firstkey) + "".bin""
            return iter([(label, val)])

    writer = get_parallel_writer(path)(path, overwrite=overwrite, credentials=credentials)

    if series.mode == :
        binary = series.values.tordd().sortByKey().mapPartitions(tobuffer)
        binary.foreach(writer.write)

    else:
        basedims = [series.shape[d] for d in series.baseaxes]

        def split(k):
            ind = unravel_index(k, basedims)
            return ind, series.values[ind]

        buf = tobuffer([split(i) for i in range(prod(basedims))])
        [writer.write(b) for b in buf]

    shape = series.shape
    dtype = series.dtype

    write_config(path, shape=shape, dtype=dtype, overwrite=overwrite, credentials=credentials)","Writes out data to binary format.

    Parameters
    ----------
    series : Series
        The data to write

    path : string path or URI to directory to be created
        Output files will be written underneath path.
        Directory will be created as a result of this call.

    prefix : str, optional, default = 'series'
        String prefix for files.

    overwrite : bool
        If true, path and all its contents will be deleted and
        recreated as partof this call."
"def remove(self, id):
        

        p = Pool.get(int(id))
        p.remove()
        redirect(url(controller = , action = ))",Remove pool.
"def _get_ids_from_hostname(self, hostname):
        
        results = self.list_hardware(hostname=hostname, mask=""id"")
        return [result[] for result in results]",Returns list of matching hardware IDs for a given hostname.
"def convolve(self, array):
        
        if self.shape[0] % 2 == 0 or self.shape[1] % 2 == 0:
            raise exc.KernelException(""PSF Kernel must be odd"")

        return scipy.signal.convolve2d(array, self, mode=)","Convolve an array with this PSF

        Parameters
        ----------
        image : ndarray
            An array representing the image the PSF is convolved with.

        Returns
        -------
        convolved_image : ndarray
            An array representing the image after convolution.

        Raises
        ------
        KernelException if either PSF psf dimension is odd"
"def get_node(obj, args, kwargs):
    

    if args is None and kwargs is None:
        return (obj,)

    if kwargs is None:
        kwargs = {}
    return obj, _bind_args(obj, args, kwargs)",Create a node from arguments and return it
"def remove_object(self, obj):
        
        self._objects.remove(obj)
        self._pairs.difference_update((obj, p) for p in self._properties)",Remove an object from the definition.
"def value(self) -> Decimal:
        
        assert isinstance(self.price, Decimal)
        
        return self.quantity * self.price","Value of the holdings in exchange currency.
        Value = Quantity * Price"
"def has_permission_to_view(page, user):
    
    if page.permissions.count() == 0:
        return True
    for perm in page.permissions.all():
        perm_label =  % (perm.content_type.app_label, perm.codename)
        if user.has_perm(perm_label):
            return True
    return False","Check whether the user has permission to view the page. If the user has
    any of the page's permissions, they have permission. If the page has no set
    permissions, they have permission."
"def geocode(
            self,
            query,
            exactly_one=True,
            timeout=DEFAULT_SENTINEL,
            proximity=None,
            country=None,
            bbox=None,
    ):
        
        params = {}

        params[] = self.api_key
        query = self.format_string % query
        if bbox:
            params[] = self._format_bounding_box(
                bbox, ""%(lon1)s,%(lat1)s,%(lon2)s,%(lat2)s"")

        if not country:
            country = []
        if isinstance(country, string_compare):
            country = [country]
        if country:
            params[] = "","".join(country)

        if proximity:
            p = Point(proximity)
            params[] = ""%s,%s"" % (p.longitude, p.latitude)

        quoted_query = quote(query.encode())
        url = ""?"".join((self.api % dict(query=quoted_query),
                        urlencode(params)))
        logger.debug(""%s.geocode: %s"", self.__class__.__name__, url)

        return self._parse_json(
            self._call_geocoder(url, timeout=timeout)
        )","Return a location point by address

        :param str query: The address or query you wish to geocode.

        :param bool exactly_one: Return one result or a list of results, if
            available.

        :param int timeout: Time, in seconds, to wait for the geocoding service
            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`
            exception. Set this only if you wish to override, on this call
            only, the value set during the geocoder's initialization.

        :param proximity: A coordinate to bias local results based on a provided
            location.
        :type proximity: :class:`geopy.point.Point`, list or tuple of ``(latitude,
            longitude)``, or string as ``""%(latitude)s, %(longitude)s""``.

        :param country: Country to filter result in form of
            ISO 3166-1 alpha-2 country code (e.g. ``FR``).
            Might be a Python list of strings.

            .. versionchanged:: 1.19.0
                Previously only a single string could be specified.
                Now a Python list of individual countries is supported.

        :type country: str or list

        :param bbox: The bounding box of the viewport within which
            to bias geocode results more prominently.
            Example: ``[Point(22, 180), Point(-22, -180)]``.
        :type bbox: list or tuple of 2 items of :class:`geopy.point.Point` or
            ``(latitude, longitude)`` or ``""%(latitude)s, %(longitude)s""``.

        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if
            ``exactly_one=False``."
"def get_public_datasets_and_tokens(self):
        
        datasets = {}
        tokens = self.get_public_tokens()
        for t in tokens:
            dataset = self.get_token_dataset(t)
            if dataset in datasets:
                datasets[dataset].append(t)
            else:
                datasets[dataset] = [t]
        return datasets","NOTE: VERY SLOW!
        Get a dictionary relating key:dataset to value:[tokens] that rely
        on that dataset.

        Arguments:
            None

        Returns:
            dict: relating key:dataset to value:[tokens]"
"def run_agent(device_type):
    
    p = argparse.ArgumentParser()
    p.add_argument(, default=os.environ.get())
    p.add_argument(, , default=0, action=)
    p.add_argument(, default=False, action=,
                   help=)

    p.add_argument(, type=str, default=,
                   help=)
    p.add_argument(, type=str, default=,
                   help=)
    p.add_argument(, type=float, default=float(),
                   help=)

    args, _ = p.parse_known_args()

    assert args.homedir

    log_file = os.path.join(args.homedir, )
    util.setup_logging(verbosity=args.verbose, filename=log_file)

    log.debug(, sys.argv)
    log.debug(, os.environ)
    log.debug(, os.getpid(), os.getppid())
    try:
        env = {: args.homedir, : os.environ[]}
        pubkey_bytes = keyring.export_public_keys(env=env)
        device_type.ui = device.ui.UI(device_type=device_type,
                                      config=vars(args))
        device_type.ui.cached_passphrase_ack = util.ExpiringCache(
            seconds=float(args.cache_expiry_seconds))
        handler = agent.Handler(device=device_type(),
                                pubkey_bytes=pubkey_bytes)

        sock_server = _server_from_assuan_fd(os.environ)
        if sock_server is None:
            sock_server = _server_from_sock_path(env)

        with sock_server as sock:
            for conn in agent.yield_connections(sock):
                with contextlib.closing(conn):
                    try:
                        handler.handle(conn)
                    except agent.AgentStop:
                        log.info()
                        return
                    except IOError as e:
                        log.info(, e)
                        return
                    except Exception as e:  
                        log.exception(, e)

    except Exception as e:  
        log.exception(, e)",Run a simple GPG-agent server.
"def ckw03(handle, begtim, endtim, inst, ref, avflag, segid, nrec, sclkdp, quats,
          avvs, nints, starts):
    
    handle = ctypes.c_int(handle)
    begtim = ctypes.c_double(begtim)
    endtim = ctypes.c_double(endtim)
    inst = ctypes.c_int(inst)
    ref = stypes.stringToCharP(ref)
    avflag = ctypes.c_int(avflag)
    segid = stypes.stringToCharP(segid)
    sclkdp = stypes.toDoubleVector(sclkdp)
    quats = stypes.toDoubleMatrix(quats)
    avvs = stypes.toDoubleMatrix(avvs)
    nrec = ctypes.c_int(nrec)
    starts = stypes.toDoubleVector(starts)
    nints = ctypes.c_int(nints)
    libspice.ckw03_c(handle, begtim, endtim, inst, ref, avflag, segid, nrec,
                     sclkdp, quats, avvs, nints, starts)","Add a type 3 segment to a C-kernel.

    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/ckw03_c.html

    :param handle: Handle of an open CK file.
    :type handle: int
    :param begtim: The beginning encoded SCLK of the segment.
    :type begtim: float
    :param endtim: The ending encoded SCLK of the segment.
    :type endtim: float
    :param inst: The NAIF instrument ID code.
    :type inst: int
    :param ref: The reference frame of the segment.
    :type ref: str
    :param avflag: True if the segment will contain angular velocity.
    :type avflag: bool
    :param segid: Segment identifier.
    :type segid: str
    :param nrec: Number of pointing records.
    :type nrec: int
    :param sclkdp: Encoded SCLK times.
    :type sclkdp: Array of floats
    :param quats: Quaternions representing instrument pointing.
    :type quats: Nx4-Element Array of floats
    :param avvs: Angular velocity vectors.
    :type avvs: Nx3-Element Array of floats
    :param nints: Number of intervals.
    :type nints: int
    :param starts: Encoded SCLK interval start times.
    :type starts: Array of floats"
"def get_many(self, type: Type[T], query: Mapping[str, Any], streaming: bool = False) -> Iterable[T]:
        
        LOGGER.info(""Getting SourceHandlers for \""{type}\"""".format(type=type.__name__))
        try:
            handlers = self._get_types[type]
        except KeyError:
            try:
                LOGGER.info(""Building new SourceHandlers for \""{type}\"""".format(type=type.__name__))
                handlers = self._get_handlers(type)
            except NoConversionError:
                handlers = None
            self._get_types[type] = handlers

        if handlers is None:
            raise NoConversionError(""No source can provide \""{type}\"""".format(type=type.__name__))

        LOGGER.info(""Creating new PipelineContext"")
        context = self._new_context()

        LOGGER.info(""Querying SourceHandlers for \""{type}\"""".format(type=type.__name__))
        for handler in handlers:
            try:
                return handler.get_many(query, context, streaming)
            except NotFoundError:
                pass

        raise NotFoundError(""No source returned a query result!"")","Gets a query from the data pipeline, which contains a request for multiple objects.

        1) Extracts the query the sequence of data sources.
        2) Inserts the results into the data sinks (if appropriate).
        3) Transforms the results into the requested type if it wasn't already.
        4) Inserts the transformed result into any data sinks.

        Args:
            query: The query being requested (contains a request for multiple objects).
            context: The context for the extraction (mutable).
            streaming: Specifies whether the results should be returned as a generator (default False).

        Returns:
            The requested objects or a generator of the objects if streaming is True."
"def genl_register_family(ops):
    
    if not ops.o_name or (ops.o_cmds and ops.o_ncmds <= 0):
        return -NLE_INVAL

    if ops.o_id and lookup_family(ops.o_id):
        return -NLE_EXIST

    if lookup_family_by_name(ops.o_name):
        return -NLE_EXIST

    nl_list_add_tail(ops.o_list, genl_ops_list)

    return 0","Register Generic Netlink family and associated commands.

    https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/mngt.c#L164

    Registers the specified Generic Netlink family definition together with all associated commands. After registration,
    received Generic Netlink messages can be passed to genl_handle_msg() which will validate the messages, look for a
    matching command and call the respective callback function automatically.

    Positional arguments:
    ops -- Generic Netlink family definition (genl_ops class instance).

    Returns:
    0 on success or a negative error code."
"def update(self, model_obj):
        
        identifier = model_obj[self.entity_cls.meta_.id_field.field_name]
        with self.conn[]:
            
            if identifier not in self.conn[][self.schema_name]:
                raise ObjectNotFoundError(
                    f
                    f)

            self.conn[][self.schema_name][identifier] = model_obj
        return model_obj",Update the entity record in the dictionary
"def _get_offsets(self, aref, dim=0):
        
        if isinstance(aref, c_ast.ID):
            return None

        
        assert type(aref.name) in [c_ast.ArrayRef, c_ast.ID], \
            ""array references must only be used with variables or other array references""
        assert type(aref.subscript) in [c_ast.ID, c_ast.Constant, c_ast.BinaryOp], \
            

        
        idxs = [self.conv_ast_to_sym(aref.subscript)]

        
        if type(aref.name) is c_ast.ArrayRef:
            idxs += self._get_offsets(aref.name, dim=dim+1)

        
        if dim == 0:
            idxs.reverse()

        return tuple(idxs)","Return a tuple of offsets of an ArrayRef object in all dimensions.

        The index order is right to left (c-code order).
        e.g. c[i+1][j-2] -> (-2, +1)

        If aref is actually a c_ast.ID, None will be returned."
"def smooth(data, fw):
    
    if fw == 0:
        fdata = data
    else:
        fdata = lfilter(np.ones(fw)/fw, 1, data)
    return fdata",Smooth data with a moving average.
"def encode_field(self, field, value):
        
        for encoder in _GetFieldCodecs(field, ):
            result = encoder(field, value)
            value = result.value
            if result.complete:
                return value
        if isinstance(field, messages.EnumField):
            if field.repeated:
                remapped_value = [GetCustomJsonEnumMapping(
                    field.type, python_name=e.name) or e.name for e in value]
            else:
                remapped_value = GetCustomJsonEnumMapping(
                    field.type, python_name=value.name)
            if remapped_value:
                return remapped_value
        if (isinstance(field, messages.MessageField) and
                not isinstance(field, message_types.DateTimeField)):
            value = json.loads(self.encode_message(value))
        return super(_ProtoJsonApiTools, self).encode_field(field, value)","Encode the given value as JSON.

        Args:
          field: a messages.Field for the field we're encoding.
          value: a value for field.

        Returns:
          A python value suitable for json.dumps."
"def issue_command(self, cmd, *args):
    
    self._writeline(cmd)
    self._writeline(str(len(args)))
    for arg in args:
      arg = str(arg)
      self._writeline(str(len(arg)))
      self._sock.sendall(arg.encode(""utf-8""))

    return self._read_response()",Sends and receives a message to/from the server
"def parse(self, format_string):
        
        txt, state = , 0
        colorstack = [(None, None)]
        itokens = self.tokenize(format_string)

        for token, escaped in itokens:
            if token == self._START_TOKEN and not escaped:
                if txt:
                    yield txt, colorstack[-1]
                    txt = 

                state += 1
                colors = self.extract_syntax(colorise.compat.next(itokens)[0])
                colorstack.append(tuple(b or a
                                        for a, b in zip(colorstack[-1],
                                                        colors)))
            elif token == self._FMT_TOKEN and not escaped:
                
                
                

                if state % 2 != 0:
                    state += 1
                else:
                    txt += token
            elif token == self._STOP_TOKEN and not escaped:
                if state < 2:
                    raise ColorSyntaxError(""Missing  or ""
                                           .format(self._STOP_TOKEN,
                                                   self._FMT_TOKEN))

                if txt:
                    yield txt, colorstack[-1]
                    txt = 

                state -= 2
                colorstack.pop()
            else:
                txt += token

        if state != 0:
            raise ColorSyntaxError(""Invalid color format"")

        if txt:
            yield txt, colorstack[-1]",Parse color syntax from a formatted string.
"def make_input(self, with_header=False):
        
        s = str(self.input)
        if with_header: s = str(self) + ""\n"" + s
        return s",Construct the input file of the calculation.
"def _fw_update(self, drvr_name, data):
        
        fw = data.get()
        tenant_id = fw.get()
        if self.fwid_attr[tenant_id].is_fw_complete() or \
           self.fwid_attr[tenant_id].is_fw_drvr_create_needed():
            prev_info_complete = True
        else:
            prev_info_complete = False

        tenant_obj = self.fwid_attr[tenant_id]
        if  in fw and len(fw.get()) != 0:
            rtr_id = fw.get()[0]
            name = dfa_dbm.DfaDBMixin.get_project_name(self, tenant_id)
            rtr_name = .join([fw_constants.TENANT_EDGE_RTR, name])

            fw_rtr_name = self.os_helper.get_rtr_name(rtr_id)
            fw_type = None
            if fw_rtr_name == rtr_name:
                fw_type = fw_constants.FW_TENANT_EDGE
            tenant_obj.update_fw_params(rtr_id, fw_type)

        if not prev_info_complete:
            self._check_create_fw(tenant_id, drvr_name)","Update routine for the Firewall.

        Check if FW is already cfgd using the below function
        if self.fwid_attr[tenant_id].is_fw_complete() or
        is_fw_drvr_create_needed():
        The above two functions will take care of whether FW is already
        cfgd or about to be cfgd in case of error.
        If yes, this may be a change in policies attached to FW.
        If no, do a check, create after storing the parameters like
        rtr_id."
"def use_plenary_repository_view(self):
        
        self._repository_view = PLENARY
        
        for session in self._get_provider_sessions():
            try:
                session.use_plenary_repository_view()
            except AttributeError:
                pass",Pass through to provider AssetRepositorySession.use_plenary_repository_view
"def LogUpdate(self, data):
        
        for hypo in self.Values():
            like = self.LogLikelihood(data, hypo)
            self.Incr(hypo, like)","Updates a suite of hypotheses based on new data.

        Modifies the suite directly; if you want to keep the original, make
        a copy.

        Note: unlike Update, LogUpdate does not normalize.

        Args:
            data: any representation of the data"
"def WaitUntilValid(self, timeout=None):
    

    return utils.Poll(
        generator=self.Get,
        condition=lambda f: f.data.is_valid,
        timeout=timeout)","Wait until the approval is valid (i.e. - approved).

    Args:
      timeout: timeout in seconds. None means default timeout (1 hour).
               0 means no timeout (wait forever).
    Returns:
      Operation object with refreshed target_file.
    Raises:
      PollTimeoutError: if timeout is reached."
"def emit(self, object, *args):
		
		for cb in self.map.get(object, []):
			cb(*args)",Emit the signal.
"def folderitem(self, obj, item, index):
        
        
        obj = api.get_object(obj)
        uid = api.get_uid(obj)
        url = api.get_url(obj)
        title = api.get_title(obj)

        
        if self.show_categories_enabled():
            category = obj.getCategoryTitle()
            if category not in self.categories:
                self.categories.append(category)
            item[""category""] = category

        item[""replace""][""Title""] = get_link(url, value=title)
        item[""selected""] = False
        item[""selected""] = uid in self.selected_services_uids

        
        methods = obj.getMethods()
        if methods:
            links = map(
                lambda m: get_link(
                    m.absolute_url(), value=m.Title(), css_class=""link""),
                methods)
            item[""replace""][""Methods""] = "", "".join(links)
        else:
            item[""methods""] = """"

        calculation = obj.getCalculation()
        if calculation:
            title = calculation.Title()
            url = calculation.absolute_url()
            item[""Calculation""] = title
            item[""replace""][""Calculation""] = get_link(url, value=title)
        else:
            item[""Calculation""] = """"

        
        after_icons = """"
        if obj.getAccredited():
            after_icons += get_image(
                ""accredited.png"", title=_(""Accredited""))
        if obj.getAttachmentOption() == ""r"":
            after_icons += get_image(
                ""attach_reqd.png"", title=_(""Attachment required""))
        if obj.getAttachmentOption() == ""n"":
            after_icons += get_image(
                ""attach_no.png"", title=_(""Attachment not permitted""))
        if after_icons:
            item[""after""][""Title""] = after_icons

        return item","Service triggered each time an item is iterated in folderitems.

        The use of this service prevents the extra-loops in child objects.

        :obj: the instance of the class to be foldered
        :item: dict containing the properties of the object to be used by
            the template
        :index: current index of the item"
"def on_stop(self):
        
        LOGGER.debug(""zeromq.Subscriber.on_stop"")
        self.running = False
        while self.is_started:
            time.sleep(0.1)
        self.zmqsocket.close()
        self.zmqcontext.destroy()",stop subscriber
"def _private_notes2marc(self, key, value):
    
    def _is_from_hal(value):
        return value.get() == 

    if not _is_from_hal(value):
        return {
            : value.get(),
            : value.get(),
        }

    self.setdefault(, []).append({: value.get()})","Populate the ``595`` MARC key.

    Also populates the `595_H` MARC key through side effects."
"def create_end_of_rib_update():
    
    mpunreach_attr = BGPPathAttributeMpUnreachNLRI(RF_IPv4_VPN.afi,
                                                   RF_IPv4_VPN.safi,
                                                   [])
    eor = BGPUpdate(path_attributes=[mpunreach_attr])
    return eor",Construct end-of-rib (EOR) Update instance.
"def login(email, password):
    
    user = _login(API.login, email, password)
    user.password = password
    return user","Login to Todoist.

    :param email: A Todoist user's email address.
    :type email: str
    :param password: A Todoist user's password.
    :type password: str
    :return: The Todoist user.
    :rtype: :class:`pytodoist.todoist.User`

    >>> from pytodoist import todoist
    >>> user = todoist.login('john.doe@gmail.com', 'password')
    >>> print(user.full_name)
    John Doe"
"def _calc_recip(self):
        
        numsites = self._s.num_sites
        prefactor = 2 * pi / self._vol
        erecip = np.zeros((numsites, numsites), dtype=np.float)
        forces = np.zeros((numsites, 3), dtype=np.float)
        coords = self._coords
        rcp_latt = self._s.lattice.reciprocal_lattice
        recip_nn = rcp_latt.get_points_in_sphere([[0, 0, 0]], [0, 0, 0],
                                                 self._gmax)

        frac_coords = [fcoords for (fcoords, dist, i, img) in recip_nn if dist != 0]

        gs = rcp_latt.get_cartesian_coords(frac_coords)
        g2s = np.sum(gs ** 2, 1)
        expvals = np.exp(-g2s / (4 * self._eta))
        grs = np.sum(gs[:, None] * coords[None, :], 2)

        oxistates = np.array(self._oxi_states)

        
        qiqj = oxistates[None, :] * oxistates[:, None]

        
        sreals = np.sum(oxistates[None, :] * np.cos(grs), 1)
        simags = np.sum(oxistates[None, :] * np.sin(grs), 1)

        for g, g2, gr, expval, sreal, simag in zip(gs, g2s, grs, expvals,
                                                   sreals, simags):

            
            m = (gr[None, :] + pi / 4) - gr[:, None]
            np.sin(m, m)
            m *= expval / g2

            erecip += m

            if self._compute_forces:
                pref = 2 * expval / g2 * oxistates
                factor = prefactor * pref * (
                    sreal * np.sin(gr) - simag * np.cos(gr))

                forces += factor[:, None] * g[None, :]

        forces *= EwaldSummation.CONV_FACT
        erecip *= prefactor * EwaldSummation.CONV_FACT * qiqj * 2 ** 0.5
        return erecip, forces","Perform the reciprocal space summation. Calculates the quantity
        E_recip = 1/(2PiV) sum_{G < Gmax} exp(-(G.G/4/eta))/(G.G) S(G)S(-G)
        where
        S(G) = sum_{k=1,N} q_k exp(-i G.r_k)
        S(G)S(-G) = |S(G)|**2

        This method is heavily vectorized to utilize numpy's C backend for
        speed."
"def create_double(self, value: float) -> Double:
        
        self.append((6, value))
        self.append(None)
        return self.get(self.raw_count - 2)","Creates a new :class:`ConstantDouble`, adding it to the pool and
        returning it.

        :param value: The value of the new Double."
"def status(config, group, accounts=(), region=None):
    
    config = validate.callback(config)
    destination = config.get()
    client = boto3.Session().client()

    for account in config.get(, ()):
        if accounts and account[] not in accounts:
            continue

        session = get_session(account[], region)
        account_id = session.client().get_caller_identity()[]
        prefix = destination.get(, ).rstrip() +  % account_id
        prefix = ""%s/flow-log"" % prefix

        role = account.pop()
        if isinstance(role, six.string_types):
            account[] = role.split()[4]
        else:
            account[] = role[-1].split()[4]

        account.pop()

        try:
            tag_set = client.get_object_tagging(
                Bucket=destination[], Key=prefix).get(, [])
        except ClientError:
            account[] = 
            continue
        tags = {t[]: t[] for t in tag_set}

        if  not in tags:
            account[] = 
        else:
            last_export = parse(tags[])
            account[] = last_export.strftime()

    accounts = [a for a in config.get() if a in accounts or not accounts]
    accounts.sort(key=operator.itemgetter(), reverse=True)
    print(tabulate(accounts, headers=))",report current export state status
"def readfmt(self, fmt):
        
        size = struct.calcsize(fmt)
        blob = self.read(size)
        obj, = struct.unpack(fmt, blob)
        return obj","Read a specified object, using a struct format string."
"def force_utf8_in_ascii_mode_hack():
    t break anything important.ANSI_X3.4-1968wutf8wutf8', buffering=1)","In systems without a UTF8 locale configured, Python will default to
    ASCII mode for stdout and stderr. This causes our fancy display to fail
    with encoding errors. In particular, you run into this if you try to run
    peru inside of Docker. This is a hack to force emitting UTF8 in that case.
    Hopefully it doesn't break anything important."
"def add_sample(self, **data):
        
        missing_dimensions = set(data).difference(self.dimensions)

        if missing_dimensions:
            raise KeyError(
                           % .join(missing_dimensions))

        for dim in self.dimensions:
            getattr(self, dim).append(data.get(dim))",Add a sample to this series.
"def get_field_link(context, field, obj=None):
    
    view = context[]
    return view.lookup_field_link(context, field, obj)","Determine what the field link should be for the given field, object pair"
"def fit(self, inputs, outputs, epochs=1):
        
        self.model.fit(inputs, outputs, epochs=epochs)","Simple wrapper of model.fit.

        :param inputs: Inputs.
        :param outputs: List of forward and backward outputs.
        :param epochs: Number of epoch.

        :return: None"
"def write(self, value):
        
        if self.mode is UNAVAILABLE:
            raise IOError(""{0} can not be used through Firmata"".format(self))
        if self.mode is INPUT:
            raise IOError(""{0} is set up as an INPUT and can therefore not be written to""
                          .format(self))
        if value is not self.value:
            self.value = value
            if self.mode is OUTPUT:
                if self.port:
                    self.port.write()
                else:
                    msg = bytearray([DIGITAL_MESSAGE, self.pin_number, value])
                    self.board.sp.write(msg)
            elif self.mode is PWM:
                value = int(round(value * 255))
                msg = bytearray([ANALOG_MESSAGE + self.pin_number, value % 128, value >> 7])
                self.board.sp.write(msg)
            elif self.mode is SERVO:
                value = int(value)
                msg = bytearray([ANALOG_MESSAGE + self.pin_number, value % 128, value >> 7])
                self.board.sp.write(msg)","Output a voltage from the pin

        :arg value: Uses value as a boolean if the pin is in output mode, or
            expects a float from 0 to 1 if the pin is in PWM mode. If the pin
            is in SERVO the value should be in degrees."
"def plot_predict(self,h=5,past_values=20,intervals=True,**kwargs):      
        
        import matplotlib.pyplot as plt
        import seaborn as sns

        figsize = kwargs.get(,(10,7))

        if self.latent_variables.estimated is False:
            raise Exception(""No latent variables estimated!"")
        else:
            
            scale, shape, skewness = self._get_scale_and_shape(self.latent_variables.get_z_values(transformed=True))
            previous_value = self.data[-1]  
            forecasted_values = np.ones(h)*self.states[-1]  
            date_index = self.shift_dates(h)
            simulations = 10000
            sim_vector = np.zeros([simulations,h])
            t_params = self.transform_z()

            for n in range(0,simulations):  
                rnd_q = np.random.normal(0,np.sqrt(self.latent_variables.get_z_values(transformed=True)[0]),h) 
                exp = forecasted_values.copy()

                for t in range(0,h):
                    if t == 0:
                        exp[t] = forecasted_values[t] + rnd_q[t]
                    else:
                        exp[t] = exp[t-1] + rnd_q[t]

                sim_vector[n] = self.family.draw_variable(loc=self.link(exp),shape=shape,scale=scale,skewness=skewness,nsims=exp.shape[0])

            sim_vector = np.transpose(sim_vector)

            forecasted_values = self.link(forecasted_values)

            if self.model_name2 == :
                forecasted_values = forecasted_values + ((t_params[-3] - (1.0/t_params[-3]))*t_params[-2]*gas.SkewtScore.tv_variate_exp(t_params[-1]))

            plt.figure(figsize=figsize) 

            if intervals == True:
                plt.fill_between(date_index[-h-1:], np.insert([np.percentile(i,5) for i in sim_vector],0,previous_value), 
                    np.insert([np.percentile(i,95) for i in sim_vector],0,previous_value), alpha=0.2,label=""95 C.I."")   

            plot_values = np.append(self.data[-past_values:],forecasted_values)
            plot_index = date_index[-h-past_values:]

            plt.plot(plot_index,plot_values,label=self.data_name)
            plt.title(""Forecast for "" + self.data_name)
            plt.xlabel(""Time"")
            plt.ylabel(self.data_name)
            plt.show()","Makes forecast with the estimated model

        Parameters
        ----------
        h : int (default : 5)
            How many steps ahead would you like to forecast?

        past_values : int (default : 20)
            How many past observations to show on the forecast graph?

        intervals : Boolean
            Would you like to show 95% prediction intervals for the forecast?

        Returns
        ----------
        - Plot of the forecast"
"def networkMultiMode(self, *tags, recordType = True, nodeCount = True, edgeWeight = True, stemmer = None, edgeAttribute = None):
        
        if len(tags) == 1:
            if not isinstance(tags[0], str):
                try:
                    tags = list(tags[0])
                except TypeError:
                    raise TagError("" is not a string it cannot be a tag."".format(tags[0]))
        for t in (i for i in tags if not isinstance(i, str)):
            raise TagError(""{} is not a string it cannot be a tag."".format(t))
        stemCheck = False
        if stemmer is not None:
            if isinstance(stemmer, collections.abc.Callable):
                stemCheck = True
            else:
                raise TagError(""stemmer must be Callable, e.g. a function or class with a __call__ method."")
        count = 0
        progArgs = (0, ""Starting to make a "" + str(len(tags)) + ""-mode network of: "" + .join(tags))
        if metaknowledge.VERBOSE_MODE:
            progKwargs = { : False}
        else:
            progKwargs = { : True}
        with _ProgressBar(*progArgs, **progKwargs) as PBar:
            if edgeAttribute is not None:
                grph = nx.MultiGraph()
            else:
                grph = nx.Graph()
            for R in self:
                if PBar:
                    count += 1
                    PBar.updateVal(count / len(self), ""Analyzing: "" + str(R))
                contents = []
                for t in tags:
                    tmpVal = R.get(t)
                    if stemCheck:
                        if tmpVal:
                            if isinstance(tmpVal, list):
                                contents.append((t, [stemmer(str(v)) for v in tmpVal]))
                            else:
                                contents.append((t, [stemmer(str(tmpVal))]))
                    else:
                        if tmpVal:
                            if isinstance(tmpVal, list):
                                contents.append((t, [str(v) for v in tmpVal]))
                            else:
                                contents.append((t, [str(tmpVal)]))
                for i, vlst1 in enumerate(contents):
                    for node1 in vlst1[1]:
                        for vlst2 in contents[i + 1:]:
                            for node2 in vlst2[1]:
                                if edgeAttribute:
                                    for edgeVal in edgeVals:
                                        if grph.has_edge(node1, node2, key = edgeVal):
                                            if edgeWeight:
                                                for i, a in grph.edges[node1, node2].items():
                                                    if a[] == edgeVal:
                                                        grph[node1][node2][i][] += 1
                                                        break
                                        else:
                                            if edgeWeight:
                                                attrDict = { : edgeVal,  : 1}
                                            else:
                                                attrDict = { : edgeVal}
                                            grph.add_edge(node1, node2, **attrDict)
                                elif edgeWeight:
                                    try:
                                        grph.edges[node1, node2][] += 1
                                    except KeyError:
                                        grph.add_edge(node1, node2, weight = 1)
                                else:
                                    if not grph.has_edge(node1, node2):
                                        grph.add_edge(node1, node2)
                        if nodeCount:
                            try:
                                grph.node[node1][] += 1
                            except KeyError:
                                try:
                                    grph.node[node1][] = 1
                                    if recordType:
                                        grph.node[node1][] = vlst1[0]
                                except KeyError:
                                    if recordType:
                                        grph.add_node(node1, type = vlst1[0])
                                    else:
                                        grph.add_node(node1)
                        else:
                            if not grph.has_node(node1):
                                if recordType:
                                    grph.add_node(node1, type = vlst1[0])
                                else:
                                    grph.add_node(node1)
                            elif recordType:
                                try:
                                    grph.node[node1][] += vlst1[0]
                                except KeyError:
                                    grph.node[node1][] = vlst1[0]
            if PBar:
                PBar.finish(""Done making a {}-mode network of: {}"".format(len(tags), .join(tags)))
        return grph","Creates a network of the objects found by all tags in _tags_, each node is marked by which tag spawned it making the resultant graph n-partite.

        A **networkMultiMode()** looks are each item in the collection and extracts its values for the tags given by _tags_. Then for all objects returned an edge is created between them, regardless of their type. Each node will have an attribute call `'type'` that gives the tag that created it or both if both created it, e.g. if `'LA'` were in _tags_ node `'English'` would have the type attribute be `'LA'`.

        For example if _tags_ was set to `['CR', 'UT', 'LA']`, a three mode network would be created, composed of a co-citation network from the `'CR'` tag. Then each citation would also have edges to all the languages of Records that cited it and to the WOS number of the those Records.

        The number of times each object occurs is count if _nodeCount_ is `True` and the edges count the number of co-occurrences if _edgeWeight_ is `True`. Both are`True` by default.

        # Parameters

        _tags_ : `str`, `str`, `str`, ... or `list [str]`

        > Any number of tags, or a list of tags

        _nodeCount_ : `optional [bool]`

        > Default `True`, if `True` each node will have an attribute called `'count'` that contains an int giving the number of time the object occurred.

        _edgeWeight_ : `optional [bool]`

        > Default `True`, if `True` each edge will have an attribute called `'weight'` that contains an int giving the number of time the two objects co-occurrenced.

        _stemmer_ : `optional [func]`

        > Default `None`, If _stemmer_ is a callable object, basically a function or possibly a class, it will be called for the ID of every node in the graph, note that all IDs are strings.

        > For example: the function `f = lambda x: x[0]` if given as the stemmer will cause all IDs to be the first character of their unstemmed IDs. e.g. the title `'Goos-Hanchen and Imbert-Fedorov shifts for leaky guided modes'` will create the node `'G'`.

        # Returns

        `networkx Graph`

        > A networkx Graph with the objects of the tags _tags_ as nodes and their co-occurrences as edges"
"def QA_fetch_stock_xdxr(code, format=, collections=DATABASE.stock_xdxr):
    
    code = QA_util_code_tolist(code)
    data = pd.DataFrame([item for item in collections.find(
        {:  {: code}}, batch_size=10000)]).drop([], axis=1)
    data[] = pd.to_datetime(data[])
    return data.set_index(, drop=False)",/
"def git_day():
    
    vec = [, , , , , ]
    day = cmd(*(vec + [, ])).split()[0]
    commits = cmd(*(vec + [, day + ])).strip()
    n = len(commits.split())
    day = day.replace(, )
    if n > 1:
        day +=  % n
    
    
    branch = get_git_branch()
    if branch != :
        day +=  + s(branch)
    return day","Constructs a version string of the form:

           day[.<commit-number-in-day>][+<branch-name-if-not-master>]

       Master is understood to be always buildable and thus untagged
       versions are treated as patch levels. Branches not master are treated
       as PEP-440 ""local version identifiers""."
"def DbGetDeviceAlias(self, argin):
        
        self._log.debug(""In DbGetDeviceAlias()"")
        ret, dev_name, dfm = check_device_name(argin)
        if not ret:
            th_exc(DB_IncorrectDeviceName,
                  ""device name ("" + argin + "") syntax error (should be [tango:][//instance/]domain/family/member)"",
                  ""DataBase::DbGetDeviceAlias()"")

        return self.db.get_device_alias(dev_name)","Return alias for device name if found.

        :param argin: The device name
        :type: tango.DevString
        :return: The alias found
        :rtype: tango.DevString"
"def fifo(rst, clk, full, we, din, empty, re, dout, afull=None, aempty=None, afull_th=None, aempty_th=None, ovf=None, udf=None, count=None, count_max=None, depth=None, width=None):
    

    if (width == None):
        width = 0
        if din is not None:
            width = len(din)
    if (depth == None):
        depth = 2

    full_flg        = Signal(bool(1))
    empty_flg       = Signal(bool(1))
    we_safe         = Signal(bool(0))
    re_safe         = Signal(bool(0))

    rd_ptr          = Signal(intbv(0, min=0, max=depth))
    rd_ptr_new      = Signal(intbv(0, min=0, max=depth))
    wr_ptr          = Signal(intbv(0, min=0, max=depth))
    wr_ptr_new      = Signal(intbv(0, min=0, max=depth))

    @always_comb
    def safe_read_write():
        full.next       = full_flg
        empty.next      = empty_flg
        we_safe.next    = we and not full_flg
        re_safe.next    = re and not empty_flg


    
    
    
    @always_comb
    def ptrs_new():
        rd_ptr_new.next     = ((rd_ptr + 1) % depth)
        wr_ptr_new.next     = ((wr_ptr + 1) % depth)

    @always(clk.posedge)
    def state_main():
        if (rst):
            wr_ptr.next     = 0
            rd_ptr.next     = 0
            full_flg.next   = 0
            empty_flg.next  = 1
        else:
            
            if (we_safe): wr_ptr.next = wr_ptr_new
            
            if (re_safe): rd_ptr.next = rd_ptr_new
            
            if (we_safe):
                empty_flg.next  = 0
            elif (re_safe and (rd_ptr_new == wr_ptr)):
                empty_flg.next  = 1
            
            if (re_safe):
                full_flg.next   = 0
            elif (we_safe and (wr_ptr_new == rd_ptr)):
                full_flg.next   = 1


    
    
    
    
    if (count != None) or (count_max != None) or (afull != None) or (aempty != None):
        count_r = Signal(intbv(0, min=0, max=depth+1))
        count_new  = Signal(intbv(0, min=-1, max=depth+2))

        if (count != None):
            assert count.max > depth
            @always_comb
            def count_out():
                count.next = count_r

        @always_comb
        def count_comb():
            if   (we_safe and not re_safe):
                    count_new.next = count_r + 1
            elif (not we_safe and re_safe):
                    count_new.next = count_r - 1
            else:
                    count_new.next = count_r

        @always(clk.posedge)
        def count_proc():
            if (rst):
                count_r.next = 0
            else:
                count_r.next = count_new

    
    if (count_max != None):
        assert count_max.max > depth
        count_max_r = Signal(intbv(0, min=0,max=count_max.max))
        @always(clk.posedge)
        def count_max_proc():
            if (rst):
                count_max_r.next = 0
            else:
                if (count_max_r < count_new):
                    count_max_r.next = count_new

        @always_comb
        def count_max_out():
            count_max.next = count_max_r

    
    
    
    
    if (afull != None):
        if (afull_th == None):
            afull_th = depth//2
        @always(clk.posedge)
        def afull_proc():
            if (rst):
                afull.next = 0
            else:
                afull.next = (count_new >= depth-afull_th)

    
    if (aempty != None):
        if (aempty_th == None):
            aempty_th = depth//2
        @always(clk.posedge)
        def aempty_proc():
            if (rst):
                aempty.next = 1
            else:
                aempty.next = (count_new <=  aempty_th)


    
    
    
    
    if (ovf != None):
        @always(clk.posedge)
        def ovf_proc():
            if (rst):
                ovf.next = 0
            else:
                if (we and full_flg ):
                    ovf.next = 1

    
    if (udf != None):
        @always(clk.posedge)
        def udf_proc():
            if (rst):
                udf.next = 0
            else:
                if (re and empty_flg):
                    udf.next = 1

    if width>0:
        
        
        
        mem_we      = Signal(bool(0))
        mem_addrw   = Signal(intbv(0, min=0, max=depth))
        mem_addrr   = Signal(intbv(0, min=0, max=depth))
        mem_di      = Signal(intbv(0)[width:0])
        mem_do      = Signal(intbv(0)[width:0])

        
        mem = ram_sdp_ar(   clk     = clk,
                            we      = mem_we,
                            addrw   = mem_addrw,
                            addrr   = mem_addrr,
                            di      = mem_di,
                            do      = mem_do )

        @always_comb
        def mem_connect():
            mem_we.next         = we_safe
            mem_addrw.next      = wr_ptr
            mem_addrr.next      = rd_ptr
            mem_di.next         = din
            dout.next           = mem_do

    return instances()","Synchronous FIFO

        Input  interface: full,  we, din
        Output interface: empty, re, dout
            It s possible to set din and dout to None. Then the fifo width will be 0 and the fifo will contain no storage.

        Extra interface:
        afull     (o) - almost full flag, asserted when the number of empty cells <= afull_th
        aempty    (o) - almost empty flag, asserted when the number of full cells <= aempty_th

        afull_th  (i) - almost full threshold, in terms of fifo cells; signal or constant; Optional, default depth/2
        aempty_th (i) - almost empty threshold, in terms of fifo cells; signal or constant; Optional, default depth/2

        count     (o) - number of occupied fifo cells

        count_max (o) - max number of occupied fifo cells reached since the last reset
        ovf       (o) - overflow flag, set at the first write in a full fifo, cleared at reset
        udf       (o) - underflow flag, set at the first read from an empty fifo, cleared at reset

        Parameters:
        depth         - fifo depth, must be >= 1; if not set or set to `None` default value 2 is used
        width         - data width in bits, must be >= 0; if not set or set to `None` the `din` width is used"
"def check(projects):
    
    log = logging.getLogger()
    log.info(.format(len(projects)))
    print()
    blockers = dependencies.blockers(projects)

    print()
    for line in message(blockers):
        print(line)

    print()
    for line in pprint_blockers(blockers):
        print(, line)

    return len(blockers) == 0",Check the specified projects for Python 3 compatibility.
"def get_transactions(self, **kwargs):
        
        kw_map = {
            : ,
            : ,
            : ,
            : }

        if not self.TRANSACTIONS_FORM:
            try:
                self.get_url(self.TRANSACTIONS_URL)
            except AuthRequiredException:
                self._auth()
                self.get_url(self.TRANSACTIONS_URL)
            self.br.select_form(""accountHistoryForm"")
            self.br.form.method = 
            self.br.form.action = self.TRANSACTIONS_EXPORT_URL
            self.TRANSACTIONS_FORM = self.br.form
            _form = deepcopy(self.TRANSACTIONS_FORM)
        else:
            _form = deepcopy(self.TRANSACTIONS_FORM)

        
        _form.set_all_readonly(False)

        for key, field_name in kw_map.items():
            if key in kwargs:
                
                if key.endswith():
                    _form[field_name] = kwargs.get(key).strftime()
                else:
                    _form[field_name] = kwargs.get(key)

        try:
            r = self.post_url(self.TRANSACTIONS_EXPORT_URL, form=_form)
            return self._parse_transactions(r)
        except AuthRequiredException:
            self._auth()
            r = self.post_url(self.TRANSACTIONS_EXPORT_URL, form=_form)
            return self._parse_transactions(r)","This method optionally takes the following extra
        keyword arguments:
        to_date: a datetime object representing the date the filter should end with
        from_date: a datetime object representing the date the filter should start from
        txn_ref: the transaction reference of a particular transaction
        from_account_id: the account id for the account to filter transactions by (you will
        need to get this information from `get_account_details` method)
        If you specify txn_ref, then it's not necessary to specify to_date and from_date."
"def update(self, state, tnow):
        
        self.state = state
        self.update_time = tnow",update the threat state
"def creators(self):
        
        
        result = []
        creators = self._safe_get_element()
        if creators is not None:
            for creator in creators:
                role = creator.attrib[] if \
                     in creator.attrib else None
                result.append((creator.text, role))
        return result","Creators.

        Creators are not the authors. These are usually editors, translators,
        narrators, etc.

        :return:
            Returns a list of creators where each is a tuple containing:

                1. The creators name (string).
                2. The creators role (string)."
"def cancel_firewall(self, firewall_id, dedicated=False):
        

        fwl_billing = self._get_fwl_billing_item(firewall_id, dedicated)
        billing_item_service = self.client[]
        return billing_item_service.cancelService(id=fwl_billing[])","Cancels the specified firewall.

        :param int firewall_id: Firewall ID to be cancelled.
        :param bool dedicated: If true, the firewall instance is dedicated,
                               otherwise, the firewall instance is shared."
"def format(self, *args, **kwargs):
        
        return self.__class__(super(ColorStr, self).format(*args, **kwargs), keep_tags=True)","Return a formatted version, using substitutions from args and kwargs.

        The substitutions are identified by braces ('{' and '}')."
"def _validate_cmds(self):
        

        cmd_list = list(self.rule_map.keys())

        for bp in self.blueprints:
            cmd_list.extend(bp.rule_map.keys())

        duplicate_cmds = (Counter(cmd_list) - Counter(set(cmd_list))).keys()

        assert not duplicate_cmds,  % duplicate_cmds"," cmd 
        :return:"
"def encode_async_options(async):
    
    options = copy.deepcopy(async._options)

    options[] = reference_to_path(async.__class__)

    

    return options",Encode Async options for JSON encoding.
"def identifiers(config):
    
    ids = []
    if (config.klass_name == ):
        for generator in os.listdir(config.generator_dir):
            if (generator == ):
                continue
            (gid, ext) = os.path.splitext(generator)
            if (ext ==  and
                    os.path.isfile(os.path.join(config.generator_dir, generator))):
                ids.append(gid)
    else:
        for image_file in os.listdir(config.image_dir):
            (iid, ext) = os.path.splitext(image_file)
            if (ext in [, , ] and
                    os.path.isfile(os.path.join(config.image_dir, image_file))):
                ids.append(iid)
    return ids","Show list of identifiers for this prefix.

    Handles both the case of local file based identifiers and
    also image generators.

    Arguments:
        config - configuration object in which:
            config.klass_name - 'gen' if a generator function
            config.generator_dir - directory for generator code
            config.image_dir - directory for images

    Returns:
        ids - a list of ids"
"def check_worker_status():
    
    if  not in request.args:
        resp = {""status"": ""bad request""}
        return jsonify(**resp)
    else:
        worker_id = request.args[]
        assignment_id = request.args[]
        allow_repeats = CONFIG.getboolean(, )
        if allow_repeats: 
            try:
                part = Participant.query.\
                    filter(Participant.workerid == worker_id).\
                    filter(Participant.assignmentid == assignment_id).one()
                status = part.status
            except exc.SQLAlchemyError:
                status = NOT_ACCEPTED
        else: 
            try:
                matches = Participant.query.\
                    filter(Participant.workerid == worker_id).all()
                numrecs = len(matches)
                if numrecs==0: 
                    status = NOT_ACCEPTED
                else:
                    status = max([record.status for record in matches])
            except exc.SQLAlchemyError:
                status = NOT_ACCEPTED
        resp = {""status"" : status}
        return jsonify(**resp)",Check worker status route
"def save(self, filename, garbage=0, clean=0, deflate=0, incremental=0, ascii=0, expand=0, linear=0, pretty=0, decrypt=1):
        

        if self.isClosed or self.isEncrypted:
            raise ValueError(""operation illegal for closed / encrypted doc"")
        if type(filename) == str:
            pass
        elif type(filename) == unicode:
            filename = filename.encode()
        else:
            raise TypeError(""filename must be a string"")
        if filename == self.name and not incremental:
            raise ValueError(""save to original must be incremental"")
        if self.pageCount < 1:
            raise ValueError(""cannot save with zero pages"")
        if incremental:
            if self.name != filename or self.stream:
                raise ValueError(""incremental needs original file"")


        return _fitz.Document_save(self, filename, garbage, clean, deflate, incremental, ascii, expand, linear, pretty, decrypt)","save(self, filename, garbage=0, clean=0, deflate=0, incremental=0, ascii=0, expand=0, linear=0, pretty=0, decrypt=1) -> PyObject *"
"def get_locations(self):
        
        if not self.zipcode_mapping:
            self.download(overwrite=False)

            zipcode_mapping = {}
            with UnicodeReader(self.file_path, delimiter=, encoding=) as csv_reader:
                
                next(csv_reader)
                for line in csv_reader:
                    zipcode_mapping[int(line[1])] = Location(
                        official_name=line[0],
                        canton=line[5],
                        municipality=line[3]
                    )
            self.zipcode_mapping = zipcode_mapping

        return self.zipcode_mapping","Return the zipcodes mapping as a list of ``{zipcode: location}`` dicts.
        The zipcodes file will be downloaded if necessary."
"def _get_device_grain(name, proxy=None):
    
    device = _retrieve_device_cache(proxy=proxy)
    return device.get(name.upper())",Retrieves device-specific grains.
"def expand_all(self):
        

        def aux(item):
            self.item(item, open=True)
            children = self.get_children(item)
            for c in children:
                aux(c)

        children = self.get_children("""")
        for c in children:
            aux(c)",Expand all items.
"def uncheck_all(self, name):
        
        for option in self.form.find_all(""input"", {""name"": name}):
            if ""checked"" in option.attrs:
                del option.attrs[""checked""]","Remove the *checked*-attribute of all input elements with
        a *name*-attribute given by ``name``."
"def get_deposit(self, deposit_id, **params):
        
        return self.api_client.get_deposit(self.id, deposit_id, **params)",https://developers.coinbase.com/api/v2#show-a-deposit
"def _get_actual_addrs(action, state):
        

        if action.actual_addrs is None:
            
                addr_list = {0x60000000}  
        else:
            addr_list = set(action.actual_addrs)

        return addr_list","For memory actions, get a list of addresses it operates on.

        :param SimAction action: The action object to work with.
        :return:                 A list of addresses that are accessed with that action.
        :rtype:                  list"
"def move_pos(self, line=1, column=1):
        
        return self.chained(move.pos(line=line, column=column))","Move the cursor to a new position.
            Default: line 1, column 1"
"def get_x_y_var(model):
    
    
    if hasattr(model, ) and model.has_uncertain_inputs():
        X = model.X.mean.values
        X_variance = model.X.variance.values
    else:
        try:
            X = model.X.values
        except AttributeError:
            X = model.X
        X_variance = None
    try:
        Y = model.Y.values
    except AttributeError:
        Y = model.Y

    if isinstance(model, WarpedGP) and not model.predict_in_warped_space:
        Y = model.Y_normalized
    
    if sparse.issparse(Y): Y = Y.todense().view(np.ndarray)
    return X, X_variance, Y","Either the the data from a model as
    X the inputs,
    X_variance the variance of the inputs ([default: None])
    and Y the outputs

    If (X, X_variance, Y) is given, this just returns.

    :returns: (X, X_variance, Y)"
"def get_summary(self):
        
        return {
            ""count"": self.count(),
            ""pass"": self.success_count(),
            ""fail"": self.failure_count(),
            ""skip"": self.skip_count(),
            ""inconclusive"": self.inconclusive_count(),
            ""retries"": self.retry_count(),
            ""duration"": self.total_duration()
        }","Get a summary of this ResultLists contents as dictionary.

        :return: dictionary"
"def clean_dict(d):
    

    if not isinstance(d, (dict, list)):
        return d
    if isinstance(d, list):
        return [v for v in (clean_dict(v) for v in d) if v]
    return OrderedDict(
        [(k, v) for k, v in ((k, clean_dict(v)) for k, v in list(d.items())) if v]
    )",Remove all empty fields in a nested dict
"def get_db_prep_save(self, value, connection, prepared=False):
        
        log.debug(""get_db_prep_save: {}, {}"", value, type(value))
        if not value:
            return 
            
            
            
        return python_localized_datetime_to_human_iso(value)","Convert Python value to database value for SAVING.
        We save with full timezone information."
"def _generate_legacy_type_checks(types=()):
    
    types = dict(types)

    def gen_type_check(pytypes):
        pytypes = _utils.flatten(pytypes)

        def type_check(checker, instance):
            if isinstance(instance, bool):
                if bool not in pytypes:
                    return False
            return isinstance(instance, pytypes)

        return type_check

    definitions = {}
    for typename, pytypes in iteritems(types):
        definitions[typename] = gen_type_check(pytypes)

    return definitions","Generate newer-style type checks out of JSON-type-name-to-type mappings.

    Arguments:

        types (dict):

            A mapping of type names to their Python types

    Returns:

        A dictionary of definitions to pass to `TypeChecker`"
"def _ELtowRRapRperi(self,E,L):
        
        if self._beta == 0.:
            xE= sc.exp(E-.5)
        else: 
            xE= (2.*E/(1.+1./self._beta))**(1./2./self._beta)
        rperi,rap= self._aA.calcRapRperi(xE,0.,L/xE,0.,0.)
        
        aA= actionAngleAxi(xE,0.,L/xE,
                           pot=PowerSphericalPotential(normalize=1.,
                                                       alpha=2.-2.*self._beta).toPlanar())
        TR= aA.TR()
        return (2.*math.pi/TR,rap,rperi)","NAME:
           _ELtowRRapRperi
        PURPOSE:
           calculate the radial frequency based on E,L, also return rap and 
           rperi
        INPUT:
           E - energy
           L - angular momentum
        OUTPUT:
           wR(E.L)
        HISTORY:
           2010-07-11 - Written - Bovy (NYU)"
"def mk_tmpl(self, path, tmpl, ctx, mode=None):
        

        path = os.path.abspath(path)

        if os.path.isfile(path):
            logger.warning(""File %s already exists, not creating it."", tmpl)

        with open(path, ) as fd:
            fd.write(
                tmpl.format(**ctx)
            )
            if mode:
                os.chmod(path, mode)",Create a file from a template if it doesn't already exist.
"def add_kb(self, issuer, kb):
        
        try:
            self.issuer_keys[issuer].append(kb)
        except KeyError:
            self.issuer_keys[issuer] = [kb]","Add a key bundle and bind it to an identifier
        
        :param issuer: Owner of the keys in the key bundle
        :param kb: A :py:class:`oidcmsg.key_bundle.KeyBundle` instance"
"def imfrombytes(content, flag=):
    
    img_np = np.frombuffer(content, np.uint8)
    flag = imread_flags[flag] if is_str(flag) else flag
    img = cv2.imdecode(img_np, flag)
    return img","Read an image from bytes.

    Args:
        content (bytes): Image bytes got from files or other streams.
        flag (str): Same as :func:`imread`.

    Returns:
        ndarray: Loaded image array."
"def _doAction(self, action):
		

		if self._db == :
			raise FMError, 

		result = 

		try:
			request = [
				uu({: self._db })
			]

			if self._layout != :
				request.append(uu({: self._layout }))

			if action ==  and self._lop != :
				request.append(uu({: self._lop }))

			if action in [, ]:

				if self._skipRecords != 0:
					request.append(uu({ : self._skipRecords }))

				if self._maxRecords != 0:
					request.append(uu({ : self._maxRecords }))

				for i in range(0, len(self._sortParams)):
					sort = self._sortParams[i]
					request.append(uu({ +str(i+1): sort[0] }))

					if sort[1] != :
						request.append(uu({ +str(i+1): sort[1] }))

			for dbParam in self._dbParams:

				if dbParam[0] == :
					request.append(uu({ : dbParam[1] }))

				elif dbParam[0] == :
					request.append(uu({ : dbParam[1] }))

				elif hasattr(dbParam[1], ):
					d = dbParam[1]
					if (not hasattr(d, )):
						request.append(uu({ dbParam[0]: d.strftime() }))
					else:
						request.append(uu({ dbParam[0]: d.strftime() }))
					del(d)
				else:
					request.append(uu({ dbParam[0]: dbParam[1] }))
			request.append(action)

			if self._extra_script:
				request += self._extra_script
				self._extra_script = None

			result = self._doRequest(request)

			try:
				result = FMResultset.FMResultset(result)
			except FMFieldError, value:
				realfields = FMServer(self._buildUrl(), self._db, self._layout).doView()

				l = []
				for k, v in self._dbParams:
					if k[-3:] !=  and k[0] != :
						l.append(("""" % k.replace(,)).encode())
				raise FMError, ""Field(s) %s not found on layout "" % (.join(l), self._layout)

			if action == :
				result = result.fieldNames

		finally:
			self._dbParams = []
			self._sortParams = []
			self._skipRecords = 0
			self._maxRecords = 0
			self._lop = 

		return result",This function will perform a FileMaker action.
"def author_info(name, contact=None, public_key=None):
    
    return Storage(name=name, contact=contact, public_key=public_key)","Easy object-oriented representation of contributor info.

    :param str name: The contributors name.
    :param str contact: The contributors email address or contact
                        information, if given.
    :param str public_key: The contributors public keyid, if given."
"def ReadUserNotifications(self,
                            username,
                            state=None,
                            timerange=None,
                            cursor=None):
    

    query = (""SELECT UNIX_TIMESTAMP(timestamp), ""
             ""       notification_state, notification ""
             ""FROM user_notification ""
             ""WHERE username_hash = %s "")
    args = [mysql_utils.Hash(username)]

    if state is not None:
      query += ""AND notification_state = %s ""
      args.append(int(state))

    if timerange is not None:
      time_from, time_to = timerange  

      if time_from is not None:
        query += ""AND timestamp >= FROM_UNIXTIME(%s) ""
        args.append(mysql_utils.RDFDatetimeToTimestamp(time_from))

      if time_to is not None:
        query += ""AND timestamp <= FROM_UNIXTIME(%s) ""
        args.append(mysql_utils.RDFDatetimeToTimestamp(time_to))

    query += ""ORDER BY timestamp DESC ""

    ret = []
    cursor.execute(query, args)

    for timestamp, state, notification_ser in cursor.fetchall():
      n = rdf_objects.UserNotification.FromSerializedString(notification_ser)
      n.timestamp = mysql_utils.TimestampToRDFDatetime(timestamp)
      n.state = state
      ret.append(n)

    return ret",Reads notifications scheduled for a user within a given timerange.
"def info(gandi, resource):
    
    output_keys = [, , , , , ]

    datacenters = gandi.datacenter.list()

    ip = gandi.ip.info(resource)
    iface = gandi.iface.info(ip[])
    vms = None
    if iface.get():
        vm = gandi.iaas.info(iface[])
        vms = {vm[]: vm}

    output_ip(gandi, ip, datacenters, vms, {iface[]: iface},
              output_keys)

    return ip","Display information about an ip.

    Resource can be an ip or id."
"def agents(self):
        
        if self._agent_manager is None:
            self._agent_manager = AgentManager(session=self._session)
        return self._agent_manager","Property for accessing :class:`AgentManager` instance, which is used to manage agents.

        :rtype: yagocd.resources.agent.AgentManager"
"def lpd_to_noaa(D, wds_url, lpd_url, version, path=""""):
    
    logger_noaa.info(""enter process_lpd"")
    d = D
    try:
        dsn = get_dsn(D)
        
        
        _convert_obj = LPD_NOAA(D, dsn, wds_url, lpd_url, version, path)
        _convert_obj.main()
        
        d = _convert_obj.get_master()
        noaas = _convert_obj.get_noaa_texts()
        __write_noaas(noaas, path)
        
        d = __rm_wdc_url(d)

    except Exception as e:
        logger_noaa.error(""lpd_to_noaa: {}"".format(e))
        print(""Error: lpd_to_noaa: {}"".format(e))

    
    return d","Convert a LiPD format to NOAA format

    :param dict D: Metadata
    :return dict D: Metadata"
"def read_json_from_file(filename):
    
    logger_jsons.info(""enter read_json_from_file"")
    d = OrderedDict()
    try:
        
        d = demjson.decode_file(filename, decode_float=float)
        logger_jsons.info(""successful read from json file"")
    except FileNotFoundError:
        
        try:
            d = demjson.decode_file(os.path.splitext(filename)[0] + , decode_float=float)
        except FileNotFoundError as e:
            
            print(""Error: jsonld file not found: {}"".format(filename))
            logger_jsons.debug(""read_json_from_file: FileNotFound: {}, {}"".format(filename, e))
        except Exception:
            print(""Error: unable to read jsonld file"")

    if d:
        d = rm_empty_fields(d)
    logger_jsons.info(""exit read_json_from_file"")
    return d","Import the JSON data from target file.
    :param str filename: Target File
    :return dict: JSON data"
"def disassociate_hosting_device_with_config_agent(
            self, client, config_agent_id, hosting_device_id):
        
        return client.delete((ConfigAgentHandlingHostingDevice.resource_path +
                              CFG_AGENT_HOSTING_DEVICES + ""/%s"") % (
            config_agent_id, hosting_device_id))",Disassociates a hosting_device with a config agent.
"def add_status_code(code):
    

    def class_decorator(cls):
        cls.status_code = code
        _sanic_exceptions[code] = cls
        return cls

    return class_decorator",Decorator used for adding exceptions to :class:`SanicException`.
"def user_project_from_remote(remote):
        

        
        
        
        regex1 = br"".*(?:[:/])(?P<user>(-|\w|\.)*)/"" \
                 br""(?P<project>(-|\w|\.)*)(\.git).*""
        match = re.match(regex1, remote)
        if match:
            return match.group(""user""), match.group(""project"")

        
        
        
        regex2 = r"".*/((?:-|\w|\.)*)/((?:-|\w|\.)*).*""
        match = re.match(regex2, remote)
        if match:
            return match.group(""user""), match.group(""project"")

        return None, None","Try to find user and project name from git remote output

        @param [String] output of git remote command
        @return [Array] user and project"
"def sections(self):
    
    if not self._is_parsed:
      self._Parse()
      self._is_parsed = True

    return self._sections",list[VolumeExtent]: sections.
"def reply_count(self, url, mode=5, after=0):
        

        sql = [,
               ,
               ,
               ,
               ,
               ]

        return dict(self.db.execute(sql, [url, mode, mode, after]).fetchall())",Return comment count for main thread and all reply threads for one url.
"def init_database(connection=None, dbname=None):
    

    connection = connection or connect()
    dbname = dbname or bigchaindb.config[][]

    create_database(connection, dbname)
    create_tables(connection, dbname)","Initialize the configured backend for use with BigchainDB.

    Creates a database with :attr:`dbname` with any required tables
    and supporting indexes.

    Args:
        connection (:class:`~bigchaindb.backend.connection.Connection`): an
            existing connection to use to initialize the database.
            Creates one if not given.
        dbname (str): the name of the database to create.
            Defaults to the database name given in the BigchainDB
            configuration."
"def _find_cont_fitfunc_regions(fluxes, ivars, contmask, deg, ranges, ffunc,
                               n_proc=1):
    
    nstars = fluxes.shape[0]
    npixels = fluxes.shape[1]
    cont = np.zeros(fluxes.shape)
    for chunk in ranges:
        start = chunk[0]
        stop = chunk[1]
        if ffunc==""chebyshev"":
            output = _find_cont_fitfunc(fluxes[:,start:stop],
                                        ivars[:,start:stop],
                                        contmask[start:stop],
                                        deg=deg, ffunc=""chebyshev"",
                                        n_proc=n_proc)
        elif ffunc==""sinusoid"":
            output = _find_cont_fitfunc(fluxes[:,start:stop],
                                        ivars[:,start:stop],
                                        contmask[start:stop],
                                        deg=deg, ffunc=""sinusoid"",
                                        n_proc=n_proc)
        cont[:, start:stop] = output

    return cont","Run fit_cont, dealing with spectrum in regions or chunks

    This is useful if a spectrum has gaps.

    Parameters
    ----------
    fluxes: ndarray of shape (nstars, npixels)
        training set or test set pixel intensities

    ivars: numpy ndarray of shape (nstars, npixels)
        inverse variances, parallel to fluxes

    contmask: numpy ndarray of length (npixels)
        boolean pixel mask, True indicates that pixel is continuum 

    deg: int
        degree of fitting function

    ffunc: str
        type of fitting function, chebyshev or sinusoid

    Returns
    -------
    cont: numpy ndarray of shape (nstars, npixels)
        the continuum, parallel to fluxes"
"def _series_sis(self):
        
        pages = self.pages._getlist(validate=False)
        page = pages[0]
        lenpages = len(pages)
        md = self.sis_metadata
        if  in md and  in md:
            shape = md[] + page.shape
            axes = md[] + page.axes
        elif lenpages == 1:
            shape = page.shape
            axes = page.axes
        else:
            shape = (lenpages,) + page.shape
            axes =  + page.axes
        self.is_uniform = True
        return [TiffPageSeries(pages, shape, page.dtype, axes, kind=)]",Return image series in Olympus SIS file.
"def new_category(blog_id, username, password, category_struct):
    
    authenticate(username, password, )
    category_dict = {: category_struct[],
                     : category_struct[],
                     : category_struct[]}
    if int(category_struct[]):
        category_dict[] = Category.objects.get(
            pk=category_struct[])
    category = Category.objects.create(**category_dict)

    return category.pk","wp.newCategory(blog_id, username, password, category)
    => category_id"
"def bigtable_viewers(self):
        
        result = set()
        for member in self._bindings.get(BIGTABLE_VIEWER_ROLE, ()):
            result.add(member)
        return frozenset(result)","Access to bigtable.viewer role memebers

        For example:

        .. literalinclude:: snippets.py
            :start-after: [START bigtable_viewers_policy]
            :end-before: [END bigtable_viewers_policy]"
"def _bowtie_args_from_config(data):
    
    config = data[]
    qual_format = config[""algorithm""].get(""quality_format"", """")
    if qual_format.lower() == ""illumina"":
        qual_flags = [""--phred64-quals""]
    else:
        qual_flags = []
    multi_mappers = config[""algorithm""].get(""multiple_mappers"", True)
    multi_flags = [""-M"", 1] if multi_mappers else [""-m"", 1]
    multi_flags = [] if data[""analysis""].lower().startswith(""smallrna-seq"") else multi_flags
    cores = config.get(""resources"", {}).get(""bowtie"", {}).get(""cores"", None)
    num_cores = config[""algorithm""].get(""num_cores"", 1)
    core_flags = [""-p"", str(num_cores)] if num_cores > 1 else []
    return core_flags + qual_flags + multi_flags",Configurable high level options for bowtie.
"def stop_tracing_process(self, pid):
        
        for thread in self.system.get_process(pid).iter_threads():
            self.__stop_tracing(thread)","Stop tracing mode for all threads in the given process.

        @type  pid: int
        @param pid: Global ID of process to stop tracing."
"def _read_df(f,nrows,names,converters,defaults=None):
        
        seek_point = f.tell()
        line = f.readline()
        raw = line.strip().split()
        if raw[0].lower() == ""external"":
            filename = raw[1]
            assert os.path.exists(filename),""Pst._read_df() error: external file  not found"".format(filename)
            df = pd.read_csv(filename,index_col=False,comment=)
            df.columns = df.columns.str.lower()
            for name in names:
                assert name in df.columns,""Pst._read_df() error: name"" +\
                "" not in external file  columns"".format(name,filename)
                if name in converters:
                    df.loc[:,name] = df.loc[:,name].apply(converters[name])
            if defaults is not None:
                for name in names:
                    df.loc[:, name] = df.loc[:, name].fillna(defaults[name])
        else:
            if nrows is None:
                raise Exception(""Pst._read_df() error: non-external sections require nrows"")
            f.seek(seek_point)
            df = pd.read_csv(f, header=None,names=names,
                             nrows=nrows,delim_whitespace=True,
                             converters=converters, index_col=False,
                             comment=)

            
            if df.shape[1] > len(names):
                df = df.iloc[:,len(names)]
                df.columns = names
            isnull = pd.isnull(df)
            if defaults is not None:
                for name in names:
                    df.loc[:,name] = df.loc[:,name].fillna(defaults[name])

            elif np.any(pd.isnull(df).values.flatten()):
                raise Exception(""NANs found"")
            f.seek(seek_point)
            extras = []
            for i in range(nrows):
                line = f.readline()
                extra = np.NaN
                if  in line:
                    raw = line.strip().split()
                    extra = .join(raw[1:])
                extras.append(extra)

            df.loc[:,""extra""] = extras

        return df","a private method to read part of an open file into a pandas.DataFrame.

        Parameters
        ----------
        f : file object
        nrows : int
            number of rows to read
        names : list
            names to set the columns of the dataframe with
        converters : dict
            dictionary of lambda functions to convert strings
            to numerical format
        defaults : dict
            dictionary of default values to assign columns.
            Default is None

        Returns
        -------
        pandas.DataFrame : pandas.DataFrame"
"def update(self, friendly_name=values.unset, api_version=values.unset,
               voice_url=values.unset, voice_method=values.unset,
               voice_fallback_url=values.unset, voice_fallback_method=values.unset,
               status_callback=values.unset, status_callback_method=values.unset,
               voice_caller_id_lookup=values.unset, sms_url=values.unset,
               sms_method=values.unset, sms_fallback_url=values.unset,
               sms_fallback_method=values.unset, sms_status_callback=values.unset,
               message_status_callback=values.unset):
        
        return self._proxy.update(
            friendly_name=friendly_name,
            api_version=api_version,
            voice_url=voice_url,
            voice_method=voice_method,
            voice_fallback_url=voice_fallback_url,
            voice_fallback_method=voice_fallback_method,
            status_callback=status_callback,
            status_callback_method=status_callback_method,
            voice_caller_id_lookup=voice_caller_id_lookup,
            sms_url=sms_url,
            sms_method=sms_method,
            sms_fallback_url=sms_fallback_url,
            sms_fallback_method=sms_fallback_method,
            sms_status_callback=sms_status_callback,
            message_status_callback=message_status_callback,
        )","Update the ApplicationInstance

        :param unicode friendly_name: A string to describe the resource
        :param unicode api_version: The API version to use to start a new TwiML session
        :param unicode voice_url: The URL to call when the phone number receives a call
        :param unicode voice_method: The HTTP method to use with the voice_url
        :param unicode voice_fallback_url: The URL to call when a TwiML error occurs
        :param unicode voice_fallback_method: The HTTP method to use with voice_fallback_url
        :param unicode status_callback: The URL to send status information to your application
        :param unicode status_callback_method: The HTTP method to use to call status_callback
        :param bool voice_caller_id_lookup: Whether to lookup the caller's name
        :param unicode sms_url: The URL to call when the phone number receives an incoming SMS message
        :param unicode sms_method: The HTTP method to use with sms_url
        :param unicode sms_fallback_url: The URL to call when an error occurs while retrieving or executing the TwiML
        :param unicode sms_fallback_method: The HTTP method to use with sms_fallback_url
        :param unicode sms_status_callback: The URL to send status information to your application
        :param unicode message_status_callback: The URL to send message status information to your application

        :returns: Updated ApplicationInstance
        :rtype: twilio.rest.api.v2010.account.application.ApplicationInstance"
"def confirm_tell(self, data, success):
        
        logger.info(""confirm_tell(success=%s) [lid=\""%s\"",pid=\""%s\""]"", success, data[P_ENTITY_LID], data[P_LID])
        evt = self._request_point_confirm_tell(R_CONTROL, data[P_ENTITY_LID], data[P_LID], success, data[])
        self._wait_and_except_if_failed(evt)","Confirm that you've done as you were told.  Call this from your control callback to confirm action.
        Used when you are advertising a control and you want to tell the remote requestor that you have
        done what they asked you to.

        `Example:` this is a minimal example to show the idea.  Note - no Exception handling and ugly use of globals

            #!python
            client = None

            def controlreq_cb(args):
                global client   # the client object you connected with

                # perform your action with the data they sent
                success = do_control_action(args['data'])

                if args['confirm']:  # you've been asked to confirm
                    client.confirm_tell(args, success)
                # else, if you do not confirm_tell() this causes a timeout at the requestor's end.

            client = IOT.Client(config='test.ini')
            thing = client.create_thing('test321')
            control = thing.create_control('control', controlreq_cb)

        Raises [IOTException](./Exceptions.m.html#IoticAgent.IOT.Exceptions.IOTException)
        containing the error if the infrastructure detects a problem

        Raises [LinkException](../Core/AmqpLink.m.html#IoticAgent.Core.AmqpLink.LinkException)
        if there is a communications problem between you and the infrastructure

        `data` (mandatory) (dictionary)  The `""args""` dictionary that your callback was called with

        `success` (mandatory) (boolean)  Whether or not the action you have been asked to do has been
        sucessful.

        More details on the contents of the `data` dictionary for controls see:
        [create_control()](./Thing.m.html#IoticAgent.IOT.Thing.Thing.create_control)"
"def transform(self, Z):
        
        if isinstance(Z, DictRDD):
            X = Z[:, ]
        else:
            X = Z

        Zs = [_transform_one(trans, name, X, self.transformer_weights)
              for name, trans in self.transformer_list]
        X_rdd = reduce(lambda x, y: x.zip(y._rdd), Zs)
        X_rdd = X_rdd.map(flatten)
        mapper = np.hstack
        for item in X_rdd.first():
            if sp.issparse(item):
                mapper = sp.hstack
        X_rdd = X_rdd.map(lambda x: mapper(x))

        if isinstance(Z, DictRDD):
            return DictRDD([X_rdd, Z[:, ]],
                           columns=Z.columns,
                           dtype=Z.dtype,
                           bsize=Z.bsize)
        else:
            return X_rdd","TODO: rewrite docstring
        Transform X separately by each transformer, concatenate results.
        Parameters
        ----------
        X : array-like or sparse matrix, shape (n_samples, n_features)
            Input data to be transformed.
        Returns
        -------
        X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)
            hstack of results of transformers. sum_n_components is the
            sum of n_components (output dimension) over transformers."
"def argval(self):
        
        if self.arg is None or any(x is None for x in self.arg):
            return None

        for x in self.arg:
            if not isinstance(x, int):
                raise InvalidArgError(self.arg)

        return self.arg","Returns the value of the arg (if any) or None.
        If the arg. is not an integer, an error be triggered."
"def extract_attribute_grid(self, model_grid, potential=False, future=False):
        

        if potential:
            var_name = model_grid.variable + ""-potential""
            timesteps = np.arange(self.start_time - 1, self.end_time)
        elif future:
            var_name = model_grid.variable + ""-future""
            timesteps = np.arange(self.start_time + 1, self.end_time + 2)
        else:
            var_name = model_grid.variable
            timesteps = np.arange(self.start_time, self.end_time + 1)
        self.attributes[var_name] = []
        for ti, t in enumerate(timesteps):
            self.attributes[var_name].append(
                model_grid.data[t - model_grid.start_hour, self.i[ti], self.j[ti]])","Extracts the data from a ModelOutput or ModelGrid object within the bounding box region of the STObject.
        
        Args:
            model_grid: A ModelGrid or ModelOutput Object
            potential: Extracts from the time before instead of the same time as the object"
"def _trim_xpath(self, xpath, prop):
        

        xroot = self._get_xroot_for(prop)

        if xroot is None and isinstance(xpath, string_types):
            xtags = xpath.split(XPATH_DELIM)

            if xtags[-1] in _iso_tag_primitives:
                xroot = XPATH_DELIM.join(xtags[:-1])

        return xroot",Removes primitive type tags from an XPATH
"def resolve(self, var, context):
        
        if var is None:
            return var
        if var[0] in (, ""'"") and var[-1] == var[0]:
            return var[1:-1]
        else:
            return template.Variable(var).resolve(context)",Resolves a variable out of context if it's not in quotes
"def label_from_attrs(da, extra=):
    

    if da.attrs.get():
        name = da.attrs[]
    elif da.attrs.get():
        name = da.attrs[]
    elif da.name is not None:
        name = da.name
    else:
        name = 

    if da.attrs.get():
        units = .format(da.attrs[])
    else:
        units = 

    return .join(textwrap.wrap(name + extra + units, 30))","Makes informative labels if variable metadata (attrs) follows
        CF conventions."
"def files(self, *args, **kwargs):
        

        return [p for p in self.listdir(*args, **kwargs) if p.isfile()]","D.files() -> List of the files in this directory.

        The elements of the list are Path objects.
        This does not walk into subdirectories (see :meth:`walkfiles`).

        Accepts parameters to :meth:`listdir`."
"def get_action(self, create=False):
        
        action = self._widget_action
        if action is None and create:
            action = self._widget_action = QWidgetAction(None)
            action.setDefaultWidget(self.widget)
        return action","Get the shared widget action for this widget.

        This API is used to support widgets in tool bars and menus.

        Parameters
        ----------
        create : bool, optional
            Whether to create the action if it doesn't already exist.
            The default is False.

        Returns
        -------
        result : QWidgetAction or None
            The cached widget action or None, depending on arguments."
"def convertDict2Attrs(self, *args, **kwargs):
        
        for n,u in enumerate(self.attrs):
            try:
                params = self.params
            except AttributeError as aerr:
                params = {}
            kwargs.update(params)
            try:
                role = self.mamburoleclass(urlfunc=None, entid=None, *args, **kwargs)
            except AttributeError as ae:
                self.mamburoleclass = MambuRole
                role = self.mamburoleclass(urlfunc=None, entid=None, *args, **kwargs)
            role.init(u, *args, **kwargs)
            self.attrs[n] = role","The trick for iterable Mambu Objects comes here:

        You iterate over each element of the responded List from Mambu,
        and create a Mambu Role (or your own itemclass) object for each
        one, initializing them one at a time, and changing the attrs
        attribute (which just holds a list of plain dictionaries) with a
        MambuUser (or your own itemclass) just created."
"def align_epi_anat(anatomy,epi_dsets,skull_strip_anat=True):
    

    if isinstance(epi_dsets,basestring):
        epi_dsets = [epi_dsets]

    if len(epi_dsets)==0:
        nl.notify( % anatomy,level=nl.level.warning)
        return

    if all(os.path.exists(nl.suffix(x,)) for x in epi_dsets):
        return

    anatomy_use = anatomy

    if skull_strip_anat:
        nl.skull_strip(anatomy,)
        anatomy_use = nl.suffix(anatomy,)

    inputs = [anatomy_use] + epi_dsets
    dset_products = lambda dset: [nl.suffix(dset,), nl.prefix(dset)+, nl.prefix(dset)+]
    products = nl.flatten([dset_products(dset) for dset in epi_dsets])
    with nl.run_in_tmp(inputs,products):
        if nl.is_nifti(anatomy_use):
            anatomy_use = nl.afni_copy(anatomy_use)
        epi_dsets_use = []
        for dset in epi_dsets:
            if nl.is_nifti(dset):
                epi_dsets_use.append(nl.afni_copy(dset))
            else:
                epi_dsets_use.append(dset)
        cmd = [""align_epi_anat.py"", ""-epi2anat"", ""-anat_has_skull"", ""no"", ""-epi_strip"", ""3dAutomask"",""-anat"", anatomy_use, ""-epi_base"", ""5"", ""-epi"", epi_dsets_use[0]]
        if len(epi_dsets_use)>1:
            cmd += [] + epi_dsets_use[1:]
            out = nl.run(cmd)

        for dset in epi_dsets:
            if nl.is_nifti(dset):
                dset_nifti = nl.nifti_copy(nl.prefix(dset)+)
                if dset_nifti and os.path.exists(dset_nifti) and dset_nifti.endswith() and dset.endswith():
                    nl.run([,dset_nifti])","aligns epis to anatomy using ``align_epi_anat.py`` script

    :epi_dsets:       can be either a string or list of strings of the epi child datasets
    :skull_strip_anat:     if ``True``, ``anatomy`` will be skull-stripped using the default method

    The default output suffix is ""_al"""
"def find_executable(name: str, flags=os.X_OK) -> List[str]:
    r
    result = []
    extensions = [x for x in os.environ.get(""PATHEXT"", """").split(os.pathsep) if x]
    path = os.environ.get(""PATH"", None)
    if path is None:
        return []
    for path in os.environ.get(""PATH"", """").split(os.pathsep):
        path = os.path.join(path, name)
        if os.access(path, flags):
            result.append(path)
        for extension in extensions:
            path_extension = path + extension
            if os.access(path_extension, flags):
                result.append(path_extension)
    return result","r""""""Finds executable `name`.

    Similar to Unix ``which`` command.

    Returns list of zero or more full paths to `name`."
"def fatal(*args, **kwargs):
    
    
    exitcode = None
    if  in kwargs:
        exitcode = kwargs.pop()
    if  in kwargs:
        cause = kwargs.pop()
        if not isinstance(cause, Result):
            raise TypeError(
                ""invalid cause of fatal error: expected %r, got %r"" % (
                    Result, type(cause)))
        exitcode = exitcode or cause.return_code

    logging.error(*args, **kwargs)
    raise Exit(exitcode or -1)","Log an error message and exit.

    Following arguments are keyword-only.

    :param exitcode: Optional exit code to use
    :param cause: Optional Invoke's Result object, i.e.
                  result of a subprocess invocation"
"def getlist(self, key):
        
        try:
            vals = _dict_getitem(self, key.lower())
        except KeyError:
            return []
        else:
            if isinstance(vals, tuple):
                return [vals[1]]
            else:
                return vals[1:]","Returns a list of all the values for the named field. Returns an
        empty list if the key doesn't exist."
"def untranslated_policy(self, default):
        
        return self.generator.settings.get(self.info.get(, None),
                                           default)",Get the policy for untranslated content
"def return_dat(self, chan, begsam, endsam):
        
        if begsam < 0:
            begpad = -1 * begsam
            begsam = 0
        else:
            begpad = 0

        if endsam > self.n_smp:
            endpad = endsam - self.n_smp
            endsam = self.n_smp
        else:
            endpad = 0

        first_sam = DATA_PRECISION * self.n_chan * begsam
        toread_sam = DATA_PRECISION * self.n_chan * (endsam - begsam)

        with open(join(self.filename, EEG_FILE), ) as f:
            f.seek(first_sam)
            x = f.read(toread_sam)

        dat = _read_dat(x)
        dat = reshape(dat, (self.n_chan, -1), )
        dat = self.convertion(dat[chan, :])
        dat = pad(dat, ((0, 0), (begpad, endpad)),
                  mode=, constant_values=NaN)

        return dat","Return the data as 2D numpy.ndarray.

        Parameters
        ----------
        chan : int or list
            index (indices) of the channels to read
        begsam : int
            index of the first sample
        endsam : int
            index of the last sample

        Returns
        -------
        numpy.ndarray
            A 2d matrix, with dimension chan X samples"
"def dag_drawer(dag, scale=0.7, filename=None, style=):
    
    try:
        import nxpd
        import pydot  
    except ImportError:
        raise ImportError(""dag_drawer requires nxpd, pydot, and Graphviz. ""
                          ""Run , and install graphviz"")

    G = dag.to_networkx()
    G.graph[] = 100 * scale

    if style == :
        pass
    elif style == :
        for node in G.nodes:
            n = G.nodes[node]
            n[] = node.name
            if node.type == :
                n[] = 
                n[] = 
                n[] = 
            if node.type == :
                n[] = 
                n[] = 
                n[] = 
            if node.type == :
                n[] = 
                n[] = 
                n[] = 
        for e in G.edges(data=True):
            e[2][] = e[2][]
    else:
        raise VisualizationError(""Unrecognized style for the dag_drawer."")

    if filename:
        show = False
    elif ( in sys.modules) and ( not in sys.modules):
        show = 
    else:
        show = True

    return nxpd.draw(G, filename=filename, show=show)","Plot the directed acyclic graph (dag) to represent operation dependencies
    in a quantum circuit.

    Note this function leverages
    `pydot <https://github.com/erocarrera/pydot>`_ (via
    `nxpd <https://github.com/chebee7i/nxpd`_) to generate the graph, which
    means that having `Graphviz <https://www.graphviz.org/>`_ installed on your
    system is required for this to work.

    Args:
        dag (DAGCircuit): The dag to draw.
        scale (float): scaling factor
        filename (str): file path to save image to (format inferred from name)
        style (str): 'plain': B&W graph
                     'color' (default): color input/output/op nodes

    Returns:
        Ipython.display.Image: if in Jupyter notebook and not saving to file,
            otherwise None.

    Raises:
        VisualizationError: when style is not recognized.
        ImportError: when nxpd or pydot not installed."
"def search_auth(self, list_or_file_path, source):
        
        _auth = source.args[]
        if isinstance(_auth, str):
            if  in _auth:
                _auth = _auth.split()
            elif _auth.endswith() and (
                    _auth.startswith() or  in _auth):  
                _auth = self.get_auth(list_or_file_path, _auth)
                _auth = self.split_auth(_auth)

        if isinstance(_auth, list):
            for i in range(len(_auth)):
                if _auth[i].endswith() and (
                        _auth[i].startswith() or  in _auth[i]):  
                    _auth[i] = self.get_auth(list_or_file_path, _auth[i])
                    if  in _auth[i]:
                        _auth = self.split_auth(_auth[i])

        source.args[] = _auth","Looking for auth in env, cmdline, str
        :param list_or_file_path:
        :param source:"
"def deleteData(self, offset: int, count: int) -> None:
        
        self._delete_data(offset, count)",Delete data by offset to count letters.
"def tasks(self):
        
        tasks_response = self.get_request()
        return [Task(self, tjson[]) for tjson in tasks_response]",Generates a list of all Tasks.
"def delete(self, exchange, if_unused=False, nowait=True, ticket=None,
               cb=None):
        
        nowait = nowait and self.allow_nowait() and not cb

        args = Writer()
        args.write_short(ticket or self.default_ticket).\
            write_shortstr(exchange).\
            write_bits(if_unused, nowait)
        self.send_frame(MethodFrame(self.channel_id, 40, 20, args))

        if not nowait:
            self._delete_cb.append(cb)
            self.channel.add_synchronous_cb(self._recv_delete_ok)",Delete an exchange.
"def zrevrangebyscore(self, key, max=float(), min=float(),
                         *, exclude=None, withscores=False,
                         offset=None, count=None, encoding=_NOTSET):
        
        if not isinstance(min, (int, float)):
            raise TypeError(""min argument must be int or float"")
        if not isinstance(max, (int, float)):
            raise TypeError(""max argument must be int or float"")

        if (offset is not None and count is None) or \
                (count is not None and offset is None):
            raise TypeError(""offset and count must both be specified"")
        if offset is not None and not isinstance(offset, int):
            raise TypeError(""offset argument must be int"")
        if count is not None and not isinstance(count, int):
            raise TypeError(""count argument must be int"")

        min, max = _encode_min_max(exclude, min, max)

        args = []
        if withscores:
            args = [b]
        if offset is not None and count is not None:
            args.extend([b, offset, count])
        fut = self.execute(b, key, max, min, *args,
                           encoding=encoding)
        if withscores:
            return wait_convert(fut, pairs_int_or_float)
        return fut","Return a range of members in a sorted set, by score,
        with scores ordered from high to low.

        :raises TypeError: if min or max is not float or int
        :raises TypeError: if both offset and count are not specified
        :raises TypeError: if offset is not int
        :raises TypeError: if count is not int"
"def to_primitive(self, value, context=None):
        

        if context and context.get():
            epoch = dt(1970, 1, 1)
            value = (value - epoch).total_seconds()
            return int(value)
        elif context and context.get():
            return value
        else:
            return super(Type, self).to_primitive(value, context)","Schematics serializer override

        If epoch_date is true then convert the `datetime.datetime`
        object into an epoch `int`."
"def alternative_short_name(self, name=None, entry_name=None, limit=None, as_df=False):
        
        q = self.session.query(models.AlternativeShortName)

        model_queries_config = (
            (name, models.AlternativeShortName.name),
        )
        q = self.get_model_queries(q, model_queries_config)

        q = self.get_one_to_many_queries(q, ((entry_name, models.Entry.name),))

        return self._limit_and_df(q, limit, as_df)","Method to query :class:`.models.AlternativeShortlName` objects in database

        :param name: alternative short name(s)
        :type name: str or tuple(str) or None

        :param entry_name: name(s) in :class:`.models.Entry`
        :type entry_name: str or tuple(str) or None

        :param limit:
            - if `isinstance(limit,int)==True` -> limit
            - if `isinstance(limit,tuple)==True` -> format:= tuple(page_number, results_per_page)
            - if limit == None -> all results
        :type limit: int or tuple(int) or None

        :param bool as_df: if `True` results are returned as :class:`pandas.DataFrame`

        :return:
            - if `as_df == False` -> list(:class:`.models.AlternativeShortName`)
            - if `as_df == True`  -> :class:`pandas.DataFrame`
        :rtype: list(:class:`.models.AlternativeShortName`) or :class:`pandas.DataFrame`"
"def add_zone_condition(self, droppable_id, zone_id, match=True):
        
        self.my_osid_object_form._my_map[].append(
            {: droppable_id, : zone_id, : match})
        self.my_osid_object_form._my_map[].sort(key=lambda k: k[])",stub
"def setup_logging(default_json_path=None, default_level=None, env_key=,
                  custom_log_dir=None):
    

    if not default_json_path:
        default_json_path = os.path.join(
            os.path.dirname(os.path.realpath(__file__)), ""logging.json"")
    path = default_json_path
    value = os.getenv(env_key, None)
    if value:
        path = value

    if os.path.exists(path):
        with open(path, ) as f:
            config = json.load(f)

        log_dir = os.path.abspath(prms.Paths[""filelogdir""])

        if custom_log_dir:
            log_dir = custom_log_dir

        if not os.path.isdir(log_dir):
            warning_txt = (""\nCould not set custom log-dir - ""
                           ""non-existing directory""
                           f""\nDir: {log_dir}""
                           ""\nUsing current directory instead: ""
                           f""{os.getcwd()}"")
            logging.warning(warning_txt)
            log_dir = os.getcwd()

        for file_handler in [""error_file_handler"", ""info_file_handler"",
                             ""debug_file_handler""]:
            try:
                file_name = config[""handlers""][file_handler][""filename""]
                logging.debug(""Setting file handlers for logging."")
                logging.debug(f""Filename: {file_name}"")
                logging.debug(f""Full path: {os.path.join(log_dir,file_name)}"")
                
                
                config[""handlers""][file_handler][
                    ""filename""] = os.path.join(log_dir,
                                               file_name)
            except Exception as e:
                warnings.warn(""\nCould not set custom log-dir"" + str(e))

        if default_level:
            w_txt = ""\nCould not set custom default level for logger""
            if default_level not in [
                ""INFO"", ""DEBUG"", logging.INFO, logging.DEBUG
            ]:
                _txt = ""\nonly  and  is supported""
                _txt += "" as default_level""
                warnings.warn(w_txt + _txt)

            else:
                try:
                    config[""handlers""][""console""][""level""] = default_level
                    if default_level in [""DEBUG"", logging.DEBUG]:
                        config[""handlers""][""console""][""formatter""] = ""stamped""

                except Exception as e:
                    warnings.warn(w_txt + ""\n"" + str(e))

        logging.config.dictConfig(config)
    else:
        if not default_level:
            default_level = logging.INFO
        logging.basicConfig(level=default_level)",Setup logging configuration
"def transform(self, X):
        
        self._check_fitted()
        X = as_features(X, stack=True)
        assignments = self.kmeans_fit_.predict(X.stacked_features)
        return self._group_assignments(X, assignments)","Transform a list of bag features into its bag-of-words representation.

        Parameters
        ----------
        X : :class:`skl_groups.features.Features` or list of bag feature arrays
            New data to transform.

        Returns
        -------
        X_new : integer array, shape [len(X), kmeans.n_clusters]
            X transformed into the new space."
"def securityEventDF(symbol=None, token=, version=):
    
    x = securityEvent(symbol, token, version)
    data = []
    for key in x:
        d = x[key]
        d[] = key
        data.append(d)
    df = pd.DataFrame(data)
    _toDatetime(df)
    return df","The Security event message is used to indicate events that apply to a security. A Security event message will be sent whenever such event occurs

    https://iexcloud.io/docs/api/#deep-security-event

    Args:
        symbol (string); Ticker to request
        token (string); Access token
        version (string); API version

    Returns:
        DataFrame: result"
"def identify(self, req, resp, resource, uri_kwargs):
        
        header = req.get_header(, False)
        auth = header.split() if header else None

        if auth is None or auth[0].lower() != :
            return None

        if len(auth) != 2:
            raise HTTPBadRequest(
                ""Invalid Authorization header"",
                ""The Authorization header for Token auth should be in form:\n""
                ""Authorization: Token <token_value>""
            )

        return auth[1]",Identify user using Authenticate header with Token auth.
"def generateRaDec(self):
        
        self.prefix = self.PAR_PREFIX

        if not isinstance(self.wcs,pywcs.WCS):
            print(
                textutil.textbox(
                    
                    
                    ),
                file=sys.stderr
            )
            raise ValueError

        if self.xypos is None or len(self.xypos[0]) == 0:
            self.xypos = None
            warnstr = textutil.textbox(
                
                
            )

            for line in warnstr.split():
                log.warning(line)

            print(warnstr)
            return

        if self.radec is None:
            print(.format(len(self.xypos[0])))
            if self.wcs is not None:
                ra, dec = self.wcs.all_pix2world(self.xypos[0], self.xypos[1], self.origin)
                self.radec = [ra, dec] + copy.deepcopy(self.xypos[2:])
            else:
                
                
                self.radec = copy.deepcopy(self.xypos)",Convert XY positions into sky coordinates using STWCS methods.
"def get_collections(kwdb, libtype=""*""):
    
    collections = kwdb.get_collections(libtype=libtype)
    for result in collections:
        url = flask.url_for("".doc_for_library"", collection_id=result[""collection_id""])
        result[""url""] = url

    return collections","Get list of collections from kwdb, then add urls necessary for hyperlinks"
"def set_params(self, **params):
        
        if not params:
            
            return self

        for key, value in params.items():
            if hasattr(self, key):
                setattr(self, key, value)
            else:
                self.kwargs[key] = value

        return self","Set the parameters of this estimator.
        Modification of the sklearn method to allow unknown kwargs. This allows using
        the full range of xgboost parameters that are not defined as member variables
        in sklearn grid search.
        Returns
        -------
        self"
"def save_file(self, srcfile):
        
        filehash = digest_file(srcfile)
        if not os.path.exists(self.object_path(filehash)):
            
            
            tmppath = self.temporary_object_path(filehash)
            copyfile(srcfile, tmppath)
            self._move_to_store(tmppath, filehash)

        return filehash",Save a (raw) file to the store.
"def formMarkup(self, realm, return_to=None, immediate=False,
            form_tag_attrs=None):
        
        message = self.getMessage(realm, return_to, immediate)
        return message.toFormMarkup(self.endpoint.server_url,
                    form_tag_attrs)","Get html for a form to submit this request to the IDP.

        @param form_tag_attrs: Dictionary of attributes to be added to
            the form tag. 'accept-charset' and 'enctype' have defaults
            that can be overridden. If a value is supplied for
            'action' or 'method', it will be replaced.
        @type form_tag_attrs: {unicode: unicode}"
"def csv_to_numpy(string_like, dtype=None):  
    
    stream = StringIO(string_like)
    return np.genfromtxt(stream, dtype=dtype, delimiter=)","Convert a CSV object to a numpy array.

    Args:
        string_like (str): CSV string.
        dtype (dtype, optional):  Data type of the resulting array. If None, the dtypes will be determined by the
                                        contents of each column, individually. This argument can only be used to
                                        'upcast' the array.  For downcasting, use the .astype(t) method.
    Returns:
        (np.array): numpy array"
"def generate(env):
    
    gnulink.generate(env)

    env[]   = SCons.Util.CLVar()

    env[] = shlib_action
    env[] = ldmod_action
    env.Append(SHLIBEMITTER = [shlib_emitter])
    env.Append(LDMODULEEMITTER = [ldmod_emitter])

    env[]         = 
    env[]         = 

    env[]        = 
    env[]        = 

    
    env[]      = 
    env[]   = 

    

    
    env[] = {
                  : _versioned_lib_suffix,
                  : _versioned_lib_suffix,
                 : _versioned_lib_suffix,
                    : link._versioned_shlib_name,
                    : link._versioned_ldmod_name,
              : lambda *args: _versioned_implib_name(*args, libtype=),
              : lambda *args: _versioned_implib_name(*args, libtype=),
          : lambda *args: _versioned_implib_symlinks(*args, libtype=),
          : lambda *args: _versioned_implib_symlinks(*args, libtype=),
    }

    
    try: del env[]
    except KeyError: pass
    try: del env[]
    except KeyError: pass",Add Builders and construction variables for cyglink to an Environment.
"def removeblanklines(astr):
    
    lines = astr.splitlines()
    lines = [line for line in lines if line.strip() != """"]
    return ""\n"".join(lines)",remove the blank lines in astr
"def user_package_filter(config, message, fasnick=None, *args, **kw):
    

    fasnick = kw.get(, fasnick)
    if fasnick:
        msg_packages = fmn.rules.utils.msg2packages(message, **config)
        if not msg_packages:
            
            
            return False
        usr_packages = fmn.rules.utils.get_packages_of_user(
            config, fasnick, all_acls)
        return usr_packages.intersection(msg_packages)

    return False","A particular user's packages (any acl)

    This rule includes messages that relate to packages where the
    specified user has *either* commit ACLs or the watchcommits flag."
"def plot_perf_stats(returns, factor_returns, ax=None):
    

    if ax is None:
        ax = plt.gca()

    bootstrap_values = timeseries.perf_stats_bootstrap(returns,
                                                       factor_returns,
                                                       return_stats=False)
    bootstrap_values = bootstrap_values.drop(, axis=)

    sns.boxplot(data=bootstrap_values, orient=, ax=ax)

    return ax","Create box plot of some performance metrics of the strategy.
    The width of the box whiskers is determined by a bootstrap.

    Parameters
    ----------
    returns : pd.Series
        Daily returns of the strategy, noncumulative.
         - See full explanation in tears.create_full_tear_sheet.
    factor_returns : pd.Series
        Daily noncumulative returns of the benchmark factor to which betas are
        computed. Usually a benchmark such as market returns.
         - This is in the same style as returns.
    ax : matplotlib.Axes, optional
        Axes upon which to plot.

    Returns
    -------
    ax : matplotlib.Axes
        The axes that were plotted on."
"def is_one_of(obj, types):
    
    for type_ in types:
        if isinstance(obj, type_):
            return True
    return False",Return true iff obj is an instance of one of the types.
"def dump_conf(self, config_pathname=None):
        

        if not config_pathname:
            config_pathname = self._get_option().value

        opener = functools.partial(open, config_pathname, )
        config_file_type = os.path.splitext(config_pathname)[1][1:]

        skip_keys = [
            k for (k, v)
            in six.iteritems(self.option_definitions)
            if isinstance(v, Option) and v.exclude_from_dump_conf
        ]

        self.write_conf(config_file_type, opener, skip_keys=skip_keys)","write a config file to the pathname specified in the parameter.  The
        file extention determines the type of file written and must match a
        registered type.

        parameters:
            config_pathname - the full path and filename of the target config
                               file."
"def message(self, executor_id, slave_id, message):
        
        logging.info(.format(
                     message, executor_id, slave_id))
        return self.driver.sendFrameworkMessage(encode(executor_id),
                                                encode(slave_id),
                                                message)","Sends a message from the framework to one of its executors.

        These messages are best effort; do not expect a framework message to be
        retransmitted in any reliable fashion."
"def get_vartype(data):
    
    if data.name is not None and data.name in _MEMO:
        return _MEMO[data.name]

    vartype = None
    try:
        distinct_count = get_groupby_statistic(data)[1]
        leng = len(data)

        if distinct_count <= 1:
            vartype = S_TYPE_CONST
        elif pd.api.types.is_bool_dtype(data) or (distinct_count == 2 and pd.api.types.is_numeric_dtype(data)):
            vartype = TYPE_BOOL
        elif pd.api.types.is_numeric_dtype(data):
            vartype = TYPE_NUM
        elif pd.api.types.is_datetime64_dtype(data):
            vartype = TYPE_DATE
        elif distinct_count == leng:
            vartype = S_TYPE_UNIQUE
        else:
            vartype = TYPE_CAT
    except:
        vartype = S_TYPE_UNSUPPORTED

    if data.name is not None:
        _MEMO[data.name] = vartype

    return vartype","Infer the type of a variable (technically a Series).

    The types supported are split in standard types and special types.

    Standard types:
        * Categorical (`TYPE_CAT`): the default type if no other one can be determined
        * Numerical (`TYPE_NUM`): if it contains numbers
        * Boolean (`TYPE_BOOL`): at this time only detected if it contains boolean values, see todo
        * Date (`TYPE_DATE`): if it contains datetime

    Special types:
        * Constant (`S_TYPE_CONST`): if all values in the variable are equal
        * Unique (`S_TYPE_UNIQUE`): if all values in the variable are different
        * Unsupported (`S_TYPE_UNSUPPORTED`): if the variable is unsupported

     The result is cached by column name in a global variable to avoid recomputing.

    Parameters
    ----------
    data : Series
        The data type of the Series.

    Returns
    -------
    str
        The data type of the Series.

    Notes
    ----
        * Should improve verification when a categorical or numeric field has 3 values, it could be a categorical field
        or just a boolean with NaN values
        * #72: Numeric with low Distinct count should be treated as ""Categorical"""
"def format_file_params(files):
        
        files_payload = {}
        if files:
            for idx, filename in enumerate(files):
                files_payload[""file["" + str(idx) + ""]""] = open(filename, )
        return files_payload",Utility method for formatting file parameters for transmission
"def get_network_instances(self, name=""""):
        

        
        
        command = ""show vrf detail""
        vrf_table_raw = self._get_command_table(command, ""TABLE_vrf"", ""ROW_vrf"")

        
        
        command = ""show vrf interface""
        intf_table_raw = self._get_command_table(command, ""TABLE_if"", ""ROW_if"")

        
        vrf_intfs = defaultdict(list)
        for intf in intf_table_raw:
            vrf_intfs[intf[""vrf_name""]].append(py23_compat.text_type(intf[""if_name""]))

        vrfs = {}
        for vrf in vrf_table_raw:
            vrf_name = py23_compat.text_type(vrf.get(""vrf_name""))
            vrfs[vrf_name] = {}
            vrfs[vrf_name][""name""] = vrf_name

            
            if vrf_name == ""default"":
                vrfs[vrf_name][""type""] = ""DEFAULT_INSTANCE""
            else:
                vrfs[vrf_name][""type""] = ""L3VRF""

            vrfs[vrf_name][""state""] = {
                ""route_distinguisher"": py23_compat.text_type(vrf.get(""rd""))
            }

            
            
            vrfs[vrf_name][""interfaces""] = {}
            vrfs[vrf_name][""interfaces""][""interface""] = dict.fromkeys(
                vrf_intfs[vrf_name], {}
            )

        
        
        if name:
            if name in vrfs.keys():
                return {py23_compat.text_type(name): vrfs[name]}
            else:
                return {}
        
        else:
            return vrfs",get_network_instances implementation for NX-OS
"def get_queryset(self, request):
        
        if request.GET.get(ShowHistoryFilter.parameter_name) == :
            queryset = self.model.objects.with_active_flag()
        else:
            
            queryset = self.model.objects.current_set()
        ordering = self.get_ordering(request)
        if ordering:
            return queryset.order_by(*ordering)
        return queryset","Annote the queryset with an 'is_active' property that's true iff that row is the most
        recently added row for that particular set of KEY_FIELDS values.
        Filter the queryset to show only is_active rows by default."
"def register_env(name, env_creator):
    

    if not isinstance(env_creator, FunctionType):
        raise TypeError(""Second argument must be a function."", env_creator)
    _global_registry.register(ENV_CREATOR, name, env_creator)","Register a custom environment for use with RLlib.

    Args:
        name (str): Name to register.
        env_creator (obj): Function that creates an env."
"def remove_forwarding_rules(self, forwarding_rules):
        
        rules_dict = [rule.__dict__ for rule in forwarding_rules]

        return self.get_data(
            ""load_balancers/%s/forwarding_rules/"" % self.id,
            type=DELETE,
            params={""forwarding_rules"": rules_dict}
        )","Removes existing forwarding rules from a LoadBalancer.

        Args:
            forwarding_rules (obj:`list`): A list of `ForwrdingRules` objects"
"def excel_to_bytes(wb: Workbook) -> bytes:
    
    memfile = io.BytesIO()
    wb.save(memfile)
    return memfile.getvalue()","Obtain a binary version of an :class:`openpyxl.Workbook` representation of
    an Excel file."
"def _on_report(_loop, adapter, conn_id, report):
    

    conn_string = None
    if conn_id is not None:
        conn_string = adapter._get_property(conn_id, )

    if isinstance(report, BroadcastReport):
        adapter.notify_event_nowait(conn_string, , report)
    elif conn_string is not None:
        adapter.notify_event_nowait(conn_string, , report)
    else:
        adapter._logger.debug(""Dropping report with unknown conn_id=%s"", conn_id)",Callback when a report is received.
"def create_profiles(self, prefix, weeks, ip_user=False):
        
        
        

        
        record_counter = {}
        for year, week in weeks:
            file = self.storage.get(prefix, year, week)
            self.count_records(record_counter, file)

        
        print(""Records read all: {}"".format(self.stat))

        
        records_valid = self.filter_counter(record_counter)

        
        profiles = defaultdict(list)
        for year, week in weeks:
            file = self.storage.get(prefix, year, week)
            self._create_user_profiles(profiles, file, records_valid, ip_user,
                                       year, week)

        return profiles",Create the user profiles for the given weeks.
"def index_modules(idx=None, path=None):
    
    suppress_output()
    modules = defaultdict(list)
    pkglist = pkgutil.walk_packages(onerror=lambda x: True)
    print(pkglist)
    if path:
        pkglist = pkgutil.walk_packages(path, onerror=lambda x: True)
    for modl, name, ispkg in pkglist:
        try:
            path = os.path.join(modl.path, name.split()[-1])
        except AttributeError:
            
            continue

        if os.path.isdir(path):
            path = os.path.join(path, )
        path += 

        objs = []

        if os.path.exists(path):
            try:
                objs = read_objs_from_path(path)
            except:
                continue
        elif not re.search(MODULE_BLACKLIST, name):
            try:
                mod = __import__(name)
                objs = [k for k in dir(mod) if not k.startswith()]
            except:
                continue
        else:
            continue

        for obj in objs:
            if name not in modules[obj]:
                modules[obj].append(name)
    suppress_output(True)
    return merge_dicts(idx, dict(modules))",Indexes objs from all modules
"def get_for(self, query_type, value):
        
        from yacms.twitter.models import Query
        lookup = {""type"": query_type, ""value"": value}
        query, created = Query.objects.get_or_create(**lookup)
        if created:
            query.run()
        elif not query.interested:
            query.interested = True
            query.save()
        return query.tweets.all()","Create a query and run it for the given arg if it doesn't exist, and
        return the tweets for the query."
"def imread(img, color=None, dtype=None):
    noUintfloat
    COLOR2CV = {: cv2.IMREAD_GRAYSCALE,
                : cv2.IMREAD_COLOR,
                None: cv2.IMREAD_ANYCOLOR
                }
    c = COLOR2CV[color]
    if callable(img):
        img = img()
    elif isinstance(img, string_types):
        
        
        
        
        
        
        
        if dtype in (None, ""noUint"") or np.dtype(dtype) != np.uint8:
            c |= cv2.IMREAD_ANYDEPTH
        img2 = cv2.imread(img, c)
        if img2 is None:
            raise IOError(""image  is not existing"" % img)
        img = img2

    elif color ==  and img.ndim == 3:  
        
        img = toGray(img)
    
    if dtype is not None:
        if isinstance(img, np.ndarray):
            img = _changeArrayDType(img, dtype, cutHigh=False)

    return img","dtype = 'noUint', uint8, float, 'float', ..."
"def walk_skip_hidden(top, onerror=None, followlinks=False):
    

    for root, dirs, files in os.walk(
            top, topdown=True, onerror=onerror,
            followlinks=followlinks):
        
        
        dirs[:] = [d for d in dirs if not is_path_hidden(d)]
        files[:] = [f for f in files if not is_path_hidden(f)]
        yield root, dirs, files","A wrapper for `os.walk` that skips hidden files and directories.

    This function does not have the parameter `topdown` from
    `os.walk`: the directories must always be recursed top-down when
    using this function.

    See also
    --------
    os.walk : For a description of the parameters"
"def _parse_uri(uri):
        

        tokens = urlparse(uri)

        if tokens.netloc != :
            logger.error(""Invalid URI: %s"", uri)
            raise ValueError(""MediaFire URI format error: ""
                             ""host should be empty - mf:///path"")

        if tokens.scheme !=  and tokens.scheme != URI_SCHEME:
            raise ValueError(""MediaFire URI format error: ""
                             ""must start with  or "")

        return posixpath.normpath(tokens.path)",Parse and validate MediaFire URI.
"def _recoverfile_from_lockfile(self, lockfile):
        
        
        
        if not (os.path.isabs(lockfile) or lockfile.startswith(self.outfolder)):
            lockfile = self._make_lock_path(lockfile)
        return lockfile.replace(LOCK_PREFIX, ""recover."" + LOCK_PREFIX)","Create path to recovery file with given name as base.
        
        :param str lockfile: Name of file on which to base this path, 
            perhaps already prefixed with the designation of a lock file.
        :return str: Path to recovery file."
"def check_repo_teams(repo, allow_teams, deny_teams, team_names=None):
    
    assert isinstance(repo, github.Repository.Repository), type(repo)

    
    if not team_names:
        try:
            team_names = [t.name for t in repo.get_teams()]
        except github.RateLimitExceededException:
            raise
        except github.GithubException as e:
            msg = 
            raise CaughtRepositoryError(repo, e, msg) from None

    if not any(x in team_names for x in allow_teams)\
       or any(x in team_names for x in deny_teams):
        raise RepositoryTeamMembershipError(
            repo,
            team_names,
            allow_teams=allow_teams,
            deny_teams=deny_teams
        )","Check if repo teams match allow/deny lists

    Parameters
    ----------
    repo: github.Repository.Repository
        repo to check for membership

    allow_teams: list(str)
        list of team names that repo MUST belong to at least one of.

    deny_teams: list(str)
        list of team that repo MUST NOT be a member of.

    team_names: list(str)
        list of the team name which the repo is a member of (optional).
        Providing this list saves retrieving the list of teams from the github
        API.

    Raises
    ------
    RepositoryTeamMembershipError
        Upon permission error"
"def subtasks(self, task, params={}, **options): 
        
        path = ""/tasks/%s/subtasks"" % (task)
        return self.client.get_collection(path, params, **options)","Returns a compact representation of all of the subtasks of a task.

        Parameters
        ----------
        task : {Id} The task to get the subtasks of.
        [params] : {Object} Parameters for the request"
"def close(self, reason=None):
        
        with self._closing:
            if self._closed:
                return

            self._websocket.close()

            self._consumer.join()
            self._consumer = None

            self._websocket = None
            self._closed = True
            for cb in self._close_callbacks:
                cb(self, reason)","Stop consuming messages and perform an orderly shutdown.

        If ``reason`` is None, then this is considered a regular close."
"def close_socket(sock):
    
    if sock:
        try:
            sock.shutdown(socket.SHUT_RDWR)
        except Exception:
            pass
        try:
            sock.close()
        except Exception:
            pass",Shutdown and close the socket.
"def agp(args):
    
    p = OptionParser(agp.__doc__)
    opts, args = p.parse_args(args)

    if len(args) != 2:
        sys.exit(not p.print_help())

    evidencefile, contigs = args
    ef = EvidenceFile(evidencefile, contigs)

    agpfile = evidencefile.replace("".evidence"", "".agp"")
    ef.write_agp(agpfile)","%prog agp evidencefile contigs.fasta

    Convert SSPACE scaffold structure to AGP format."
"def _find_convertable_object(self, data):
        
        data = list(data)
        convertable_object_idxs = [
            idx
            for idx, obj
            in enumerate(data)
            if obj.get() in self.pod_types.keys()
        ]
        if len(convertable_object_idxs) < 1:
            raise Exception(""Kubernetes config didn, '.join(self.pod_types.keys())
            ))
        return list(data)[convertable_object_idxs[0]]",Get the first instance of a `self.pod_types`
"def vm_detach_nic(name, kwargs=None, call=None):
    
    if call != :
        raise SaltCloudSystemExit(
            
        )

    if kwargs is None:
        kwargs = {}

    nic_id = kwargs.get(, None)
    if nic_id is None:
        raise SaltCloudSystemExit(
            nic_id\
        )

    server, user, password = _get_xml_rpc()
    auth = .join([user, password])
    vm_id = int(get_vm_id(kwargs={: name}))
    response = server.one.vm.detachnic(auth, vm_id, int(nic_id))

    data = {
        : ,
        : response[0],
        : response[1],
        : response[2],
    }

    return data","Detaches a disk from a virtual machine.

    .. versionadded:: 2016.3.0

    name
        The name of the VM from which to detach the network interface.

    nic_id
        The ID of the nic to detach.

    CLI Example:

    .. code-block:: bash

        salt-cloud -a vm_detach_nic my-vm nic_id=1"
"def exec_cmd(self, cmdstr):
        
        parts = cmdstr.split()
        if len(parts):
            cmd, args = parts[0], parts[1:]
            self._dispatch(cmd, args)
        else:
            pass",Parse line from CLI read loop and execute provided command
"def clean_column_names(df: DataFrame) -> DataFrame:
    
    f = df.copy()
    f.columns = [col.strip() for col in f.columns]
    return f","Strip the whitespace from all column names in the given DataFrame
    and return the result."
"def _symbol_trades(self, symbols):
        

        @skip_http_error((404, 504))
        def fetch(symbol):
            return self._api.polygon.last_trade(symbol)

        return parallelize(fetch)(symbols)","Query last_trade in parallel for multiple symbols and
        return in dict.

        symbols: list[str]

        return: dict[str -> polygon.Trade]"
"def archive(self, request, id):
        
        gist = self.send(request, id).json()

        with tarfile.open(.format(id), mode=) as archive:
            for name, data in gist[].items():
                with tempfile.NamedTemporaryFile() as fp:
                    fp.write(data[])
                    fp.flush()
                    archive.add(fp.name, arcname=name)","Create an archive of a gist

        The files in the gist are downloaded and added to a compressed archive
        (tarball). If the ID of the gist was c78d925546e964b4b1df, the
        resulting archive would be,

            c78d925546e964b4b1df.tar.gz

        The archive is created in the directory where the command is invoked.

        Arguments:
            request: an initial request object
            id:      the gist identifier"
"def _plot_simple_fault(self, source, border=, border_width=1.0):
        
        
        trace_lons = np.array([pnt.longitude
                               for pnt in source.fault_trace.points])
        trace_lats = np.array([pnt.latitude
                               for pnt in source.fault_trace.points])
        surface_projection = _fault_polygon_from_mesh(source)
        
        x, y = self.m(surface_projection[:, 0], surface_projection[:, 1])
        self.m.plot(x, y, border, linewidth=border_width)
        
        x, y = self.m(trace_lons, trace_lats)
        self.m.plot(x, y, border, linewidth=1.3 * border_width)","Plots the simple fault source as a composite of the fault trace
        and the surface projection of the fault.
        :param source:
            Fault source as instance of :class: mtkSimpleFaultSource
        :param str border:
            Line properties of border (see matplotlib documentation for detail)
        :param float border_width:
            Line width of border (see matplotlib documentation for detail)"
"def simplify(source, kink=20):
    
    source_coord = map(lambda o: {""lng"": o.coordinates[0], ""lat"": o.coordinates[1]}, source)

    
    
    
    F = (math.pi / 180.0) * 0.5
    index = [] 
    sig_start = [] 
    sig_end = []

    
    count = len(source_coord)
    if count < 3:
        return source_coord 

    

    band_sqr = kink * 360.0 / (2.0 * math.pi * 6378137.0) 
    band_sqr *= band_sqr
    n_dest = 0
    sig_start[0] = 0
    sig_end[0] = count - 1
    n_stack = 1

    
    while n_stack > 0:
        
        start = sig_start[n_stack - 1]
        end = sig_end[n_stack - 1]
        n_stack -= 1

        if (end - start) > 1: 
            
            x12 = source[end][""lng""] - source[start][""lng""]
            y12 = source[end][""lat""] - source[start][""lat""]
            if math.fabs(x12) > 180.0:
                x12 = 360.0 - math.fabs(x12)
            x12 *= math.cos(F * (source[end][""lat""] + source[start][""lat""])) 
            d12 = (x12 * x12) + (y12 * y12)

            i = start + 1
            sig = start
            max_dev_sqr = -1.0
            while i < end:
                x13 = source[i][""lng""] - source[start][""lng""]
                y13 = source[i][""lat""] - source[start][""lat""]
                if math.fabs(x13) > 180.0:
                    x13 = 360.0 - math.fabs(x13)
                x13 *= math.cos(F * (source[i][""lat""] + source[start][""lat""]))
                d13 = (x13 * x13) + (y13 * y13)
                x23 = source[i][""lng""] - source[end][""lng""]
                y23 = source[i][""lat""] - source[end][""lat""]
                if math.fabs(x23) > 180.0:
                    x23 = 360.0 - math.fabs(x23)
                x23 *= math.cos(F * (source[i][""lat""] + source[end][""lat""]))
                d23 = (x23 * x23) + (y23 * y23)

                if d13 >= (d12 + d23):
                    dev_sqr = d23
                elif d23 >= (d12 + d13):
                    dev_sqr = d13
                else:
                    dev_sqr = (x13 * y12 - y13 * x12) * (x13 * y12 - y13 * x12) / d12 
                if dev_sqr > max_dev_sqr:
                    sig = i
                    max_dev_sqr = dev_sqr
                i += 1


            if max_dev_sqr < band_sqr: 
            
                index[n_dest] = start
                n_dest += 1
            else: 
                n_stack += 1
                sig_start[n_stack - 1] = sig
                sig_end[n_stack - 1] = end
                n_stack += 1
                sig_start[n_stack - 1] = start
                sig_end[n_stack - 1] = sig

        else:  
            index[n_dest] = start
            n_dest += 1

    
    index[n_dest] = count - 1
    n_dest += 1

    
    r = []
    for i in range(0, n_dest):
        r.append(source_coord[index[i]])

    return map(lambda o:  {""type"": ""Point"",""coordinates"": [o.lng, o.lat]}, r)","source[] array of geojson points
     kink	in metres, kinks above this depth kept
     kink depth is the height of the triangle abc where a-b and b-c are two consecutive line segments"
"def disconnect(self):
        
        if self.connected and self.channel:
            logging.debug(""Disconnecting KNX/IP tunnel..."")

            frame = KNXIPFrame(KNXIPFrame.DISCONNECT_REQUEST)
            frame.body = self.hpai_body()

            
            
            if self.seq < 0xff:
                self.seq += 1
            else:
                self.seq = 0

            self.control_socket.sendto(
                bytes(frame.to_frame()), (self.remote_ip, self.remote_port))
            
            

        else:
            logging.debug(""Disconnect - no connection, nothing to do"")

        self.channel = None
        self.connected = False",Disconnect an open tunnel connection
"def get_peer_ips(self):
        
        presponse = [ord(i) for i in self.tracker_response[]]
        while presponse:
            peer_ip = ((.join(str(x) for x in presponse[0:4]),
                       256 * presponse[4] + presponse[5]))
            if peer_ip not in self.peer_ips:
                self.peer_ips.append(peer_ip)
            presponse = presponse[6:]","Generates list of peer IPs from tracker response. Note: not all of
        these IPs might be good, which is why we only init peer objects for
        the subset that respond to handshake"
"def add_collection(self, property_name, use_context=True):
        
        if self.has_property(property_name):
            err = 
            raise PropertyExists(err.format(property_name))

        prop = CollectionProperty(use_context=bool(use_context))
        self.collections[property_name] = prop
        return prop","Add collection property to schema
        :param property_name: str, property name
        :param property_name: str, property name
        :return: shiftschema.property.CollectionProperty"
"def forwards(apps, schema_editor):
    
    Movie = apps.get_model(, )
    Work = apps.get_model(, )
    WorkRole = apps.get_model(, )
    WorkSelection = apps.get_model(, )

    for m in Movie.objects.all():

        work = Work.objects.create(
            kind=,
            title=m.title,
            title_sort=m.title_sort,
            year=m.year,
            imdb_id=m.imdb_id
        )

        for role in m.roles.all():
            WorkRole.objects.create(
                creator=role.creator,
                work=work,
                role_name=role.role_name,
                role_order=role.role_order
            )

        for selection in m.events.all():
            WorkSelection.objects.create(
                event=selection.event,
                work=work,
                order=selection.order
            )

        m.delete()","Change all Movie objects into Work objects, and their associated
    data into WorkRole and WorkSelection models, then delete the Movie."
"def get_vmpolicy_macaddr_input_datacenter(self, **kwargs):
        
        config = ET.Element(""config"")
        get_vmpolicy_macaddr = ET.Element(""get_vmpolicy_macaddr"")
        config = get_vmpolicy_macaddr
        input = ET.SubElement(get_vmpolicy_macaddr, ""input"")
        datacenter = ET.SubElement(input, ""datacenter"")
        datacenter.text = kwargs.pop()

        callback = kwargs.pop(, self._callback)
        return callback(config)",Auto Generated Code
"def get(self, didMethodName, required=True) -> DidMethod:
        
        dm = self.d.get(didMethodName) if didMethodName else self.default
        if not dm and required:
            raise DidMethodNotFound
        return dm",":param didMethodName: name of DID Method
        :param required: if not found and True, throws an exception, else None
        :return: DID Method"
"def style_layout(style_less,
                 theme=,
                 cursorwidth=2,
                 cursorcolor=,
                 cellwidth=,
                 lineheight=170,
                 margins=,
                 vimext=False,
                 toolbar=False,
                 nbname=False,
                 kernellogo=False,
                 altprompt=False,
                 altmd=False,
                 altout=False,
                 hideprompt=False):
    

    
    with fileOpen(theme_name_file, ) as f:
        f.write(theme)

    if (os.path.isdir(styles_dir_user) and
            .format(theme) in os.listdir(styles_dir_user)):
        theme_relpath = os.path.relpath(
            os.path.join(styles_dir_user, theme), package_dir)
    else:
        theme_relpath = os.path.relpath(
            os.path.join(styles_dir, theme), package_dir)

    style_less += .format(theme_relpath)

    textcell_bg = 
    promptText = 
    promptBG = 
    promptPadding = 
    promptBorder = 
    tcPromptBorder = 
    promptMinWidth = 11.5
    outpromptMinWidth = promptMinWidth 
    tcPromptWidth = promptMinWidth + 3
    tcPromptFontsize = ""@prompt-fontsize""
    ccOutputBG = 

    if theme == :
        textcell_bg = 
    if altprompt:
        promptPadding = 
        promptMinWidth = 8
        outpromptMinWidth = promptMinWidth + 3
        tcPromptWidth = promptMinWidth + 3
        promptText = 
        tcPromptBorder = 
    if altmd:
        textcell_bg = 
        tcPromptBorder = 
    if altout:
        ccOutputBG = 
    if margins != :
        margins = .format(margins)
    if  not in cellwidth:
        cellwidth = str(cellwidth) + 

    style_less += .format(margins)
    style_less += .format(cellwidth)
    style_less += .format(lineheight)
    style_less += .format(textcell_bg)
    style_less += .format(promptMinWidth)
    style_less += .format(promptBG)
    style_less += .format(ccOutputBG)
    style_less += .format(promptText)
    style_less += .format(promptPadding)
    style_less += .format(promptBorder)
    style_less += .format(promptMinWidth)
    style_less += .format(outpromptMinWidth)
    style_less += .format(tcPromptWidth)
    style_less += .format(tcPromptBorder)
    style_less += .format(cursorwidth)
    style_less += .format(
        cursorcolor)
    style_less += .format(tcPromptFontsize)
    style_less += 

    
    with fileOpen(nb_style, ) as notebook:
        style_less += notebook.read() + 

    
    with fileOpen(cl_style, ) as cells:
        style_less += cells.read() + 

    
    with fileOpen(ex_style, ) as extras:
        style_less += extras.read() + 

    
    with fileOpen(cm_style, ) as codemirror:
        style_less += codemirror.read() + 
    with fileOpen(comp_style, ) as codemirror:
        style_less += codemirror.read() + 

    style_less += toggle_settings(
        toolbar, nbname, hideprompt, kernellogo) + 
    if vimext:
        set_vim_style(theme)

    return style_less",Set general layout and style properties of text and code cells
"def _init_options(self, kwargs):
        
        self.options = self.task_config.options
        if self.options is None:
            self.options = {}
        if kwargs:
            self.options.update(kwargs)

        
        for option, value in list(self.options.items()):
            try:
                if value.startswith(""$project_config.""):
                    attr = value.replace(""$project_config."", """", 1)
                    self.options[option] = getattr(self.project_config, attr, None)
            except AttributeError:
                pass",Initializes self.options
"def cp_handler(self, args):
    

    self.validate(, args)
    source = args[1]
    target = args[2]
    self.s3handler().cp_files(source, target)",Handler for cp command
"def read_int8(self, little_endian=True):
        
        if little_endian:
            endian = ""<""
        else:
            endian = "">""
        return self.unpack( % endian)","Read 1 byte as a signed integer value from the stream.

        Args:
            little_endian (bool): specify the endianness. (Default) Little endian.

        Returns:
            int:"
"def fromimage(im, flatten=False, mode=None):
    
    if not Image.isImageType(im):
        raise TypeError(""Input is not a PIL image."")

    if mode is not None:
        if mode != im.mode:
            im = im.convert(mode)
    elif im.mode == :
        
        
        
        
        if  in im.info:
            im = im.convert()
        else:
            im = im.convert()

    if flatten:
        im = im.convert()
    elif im.mode == :
        
        
        
        
        
        
        im = im.convert()

    a = array(im)
    return a","Return a copy of a PIL image as a numpy array.

    Parameters
    ----------
    im : PIL image
        Input image.
    flatten : bool
        If true, convert the output to grey-scale.
    mode : str, optional
        Mode to convert image to, e.g. ``'RGB'``.  See the Notes of the
        `imread` docstring for more details.

    Returns
    -------
    fromimage : ndarray
        The different colour bands/channels are stored in the
        third dimension, such that a grey-image is MxN, an
        RGB-image MxNx3 and an RGBA-image MxNx4."
"def get_validators_description(view):
    
    action = getattr(view, , None)
    if action is None:
        return 

    description = 
    validators = getattr(view, action + , [])
    for validator in validators:
        validator_description = get_entity_description(validator)
        description +=  + validator_description if description else validator_description

    return  + description if description else ","Returns validators description in format:
    ### Validators:
    * validator1 name
     * validator1 docstring
    * validator2 name
     * validator2 docstring"
"def on_ok(self):
        
        self.parentApp.repo_value[] = {}
        self.parentApp.repo_value[] = {}
        for branch in self.branch_cb:
            if self.branch_cb[branch].value:
                
                self.parentApp.repo_value[][branch] = self.commit_tc[branch].values[self.commit_tc[branch].value]
                self.parentApp.repo_value[][branch] = True
        if self.error:
            self.quit()
        self.parentApp.addForm(,
                               ChooseToolsForm,
                               name=
                               ,
                               color=)
        self.parentApp.change_form()","Take the branch, commit, and build selection and add them as plugins"
"def Parse(self, persistence, knowledge_base, download_pathtype):
    
    pathspecs = []

    if isinstance(persistence, rdf_client.WindowsServiceInformation):
      if persistence.HasField(""binary""):
        pathspecs.append(persistence.binary.pathspec)
      elif persistence.HasField(""image_path""):
        pathspecs = self._GetFilePaths(persistence.image_path,
                                       download_pathtype, knowledge_base)

    if isinstance(
        persistence,
        rdf_client_fs.StatEntry) and persistence.HasField(""registry_type""):
      pathspecs = self._GetFilePaths(persistence.registry_data.string,
                                     download_pathtype, knowledge_base)

    for pathspec in pathspecs:
      yield rdf_standard.PersistenceFile(pathspec=pathspec)",Convert persistence collector output to downloadable rdfvalues.
"def _advapi32_create_blob(key_info, key_type, algo, signing=True):
    

    if key_type == :
        blob_type = Advapi32Const.PUBLICKEYBLOB
    else:
        blob_type = Advapi32Const.PRIVATEKEYBLOB

    if algo == :
        struct_type = 
        if signing:
            algorithm_id = Advapi32Const.CALG_RSA_SIGN
        else:
            algorithm_id = Advapi32Const.CALG_RSA_KEYX
    else:
        struct_type = 
        algorithm_id = Advapi32Const.CALG_DSS_SIGN

    blob_header_pointer = struct(advapi32, )
    blob_header = unwrap(blob_header_pointer)
    blob_header.bType = blob_type
    blob_header.bVersion = Advapi32Const.CUR_BLOB_VERSION
    blob_header.reserved = 0
    blob_header.aiKeyAlg = algorithm_id

    blob_struct_pointer = struct(advapi32, struct_type)
    blob_struct = unwrap(blob_struct_pointer)
    blob_struct.publickeystruc = blob_header

    bit_size = key_info.bit_size
    len1 = bit_size // 8
    len2 = bit_size // 16

    if algo == :
        pubkey_pointer = struct(advapi32, )
        pubkey = unwrap(pubkey_pointer)
        pubkey.bitlen = bit_size
        if key_type == :
            parsed_key_info = key_info[].parsed
            pubkey.magic = Advapi32Const.RSA1
            pubkey.pubexp = parsed_key_info[].native
            blob_data = int_to_bytes(parsed_key_info[].native, signed=False, width=len1)[::-1]
        else:
            parsed_key_info = key_info[].parsed
            pubkey.magic = Advapi32Const.RSA2
            pubkey.pubexp = parsed_key_info[].native
            blob_data = int_to_bytes(parsed_key_info[].native, signed=False, width=len1)[::-1]
            blob_data += int_to_bytes(parsed_key_info[].native, signed=False, width=len2)[::-1]
            blob_data += int_to_bytes(parsed_key_info[].native, signed=False, width=len2)[::-1]
            blob_data += int_to_bytes(parsed_key_info[].native, signed=False, width=len2)[::-1]
            blob_data += int_to_bytes(parsed_key_info[].native, signed=False, width=len2)[::-1]
            blob_data += int_to_bytes(parsed_key_info[].native, signed=False, width=len2)[::-1]
            blob_data += int_to_bytes(parsed_key_info[].native, signed=False, width=len1)[::-1]
        blob_struct.rsapubkey = pubkey

    else:
        pubkey_pointer = struct(advapi32, )
        pubkey = unwrap(pubkey_pointer)
        pubkey.bitlen = bit_size

        if key_type == :
            pubkey.magic = Advapi32Const.DSS1
            params = key_info[][].native
            key_data = int_to_bytes(key_info[].parsed.native, signed=False, width=len1)[::-1]
        else:
            pubkey.magic = Advapi32Const.DSS2
            params = key_info[][].native
            key_data = int_to_bytes(key_info[].parsed.native, signed=False, width=20)[::-1]
        blob_struct.dsspubkey = pubkey

        blob_data = int_to_bytes(params[], signed=False, width=len1)[::-1]
        blob_data += int_to_bytes(params[], signed=False, width=20)[::-1]
        blob_data += int_to_bytes(params[], signed=False, width=len1)[::-1]
        blob_data += key_data

        dssseed_pointer = struct(advapi32, )
        dssseed = unwrap(dssseed_pointer)
        
        dssseed.counter = 0xffffffff

        blob_data += struct_bytes(dssseed_pointer)

    return struct_bytes(blob_struct_pointer) + blob_data","Generates a blob for importing a key to CryptoAPI

    :param key_info:
        An asn1crypto.keys.PublicKeyInfo or asn1crypto.keys.PrivateKeyInfo
        object

    :param key_type:
        A unicode string of ""public"" or ""private""

    :param algo:
        A unicode string of ""rsa"" or ""dsa""

    :param signing:
        If the key handle is for signing - may only be False for rsa keys

    :return:
        A byte string of a blob to pass to advapi32.CryptImportKey()"
"def _apply_incoming_copying_manipulators(self, son, collection):
        
        for manipulator in self.__incoming_copying_manipulators:
            son = manipulator.transform_incoming(son, collection)
        return son",Apply incoming copying manipulators to `son`.
"def render_to_response(self, context, *args, **kwargs):
        
        preview_outputs = []
        file_outputs = []
        bast_ctx = context
        added = set()

        for output_group, output_files in context[][].items():
            for output_file_content in output_files:
                if output_group:
                    bast_ctx.update({
                        : context[],
                        : output_group,
                        : output_file_content,
                    })
                    preview = render_to_string( % output_group, bast_ctx)
                    preview_outputs.append(preview)

        for file_info in context[][]:
            if file_info and file_info.get() not in added:
                row_ctx = dict(
                    file=file_info,
                    **context
                )
                table_row = render_to_string(, row_ctx)
                file_outputs.append(table_row)
                added.add(file_info.get())

        return JsonResponse({
            : context[][].lower(),
            : context[][].command,
            : context[][].get_stdout(),
            : context[][].get_stderr(),
            : preview_outputs,
            : file_outputs,
        })",Build dictionary of content
"def initialize_model(self, model):
        
        logging.info()
        self.fp16_model = model
        self.fp16_model.zero_grad()
        self.fp32_params = [param.to(torch.float32).detach()
                            for param in model.parameters()]

        for param in self.fp32_params:
            param.requires_grad = True","Initializes internal state and build fp32 master copy of weights.

        :param model: fp16 model"
"def read_ical(self, ical_file_location):  
        
        with open(ical_file_location, ) as ical_file:
            data = ical_file.read()
        self.cal = Calendar.from_ical(data)
        return self.cal",Read the ical file
"def get_user(self, username):
        
        ret = {}
        tmp = self._get_user(self._byte_p2(username), ALL_ATTRS)
        if tmp is None:
            raise UserDoesntExist(username, self.backend_name)
        attrs_tmp = tmp[1]
        for attr in attrs_tmp:
            value_tmp = attrs_tmp[attr]
            if len(value_tmp) == 1:
                ret[attr] = value_tmp[0]
            else:
                ret[attr] = value_tmp
        return ret",Gest a specific user
"def _get_template(self):
        
        loader = PackageLoader(self._package_name, )
        env = Environment(extensions=[], loader=loader)
        return env.get_template(.format(self._report_name))","Returns a template for this report.

        :rtype: `jinja2.Template`"
"def _prepare_reserved_tokens(reserved_tokens):
  
  reserved_tokens = [tf.compat.as_text(tok) for tok in reserved_tokens or []]
  dups = _find_duplicates(reserved_tokens)
  if dups:
    raise ValueError(""Duplicates found in tokens: %s"" % dups)
  reserved_tokens_re = _make_reserved_tokens_re(reserved_tokens)
  return reserved_tokens, reserved_tokens_re",Prepare reserved tokens and a regex for splitting them out of strings.
"def click_right(x = None, y = None, hold_time = 0):
    

    if not x or not y:
        cursor = win32api.GetCursorPos()
        if not x:
            x = cursor[0]
        if not y:
            y = cursor[1]
    move(x, y)
    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTDOWN, x, y, 0, 0)
    time.sleep(hold_time)
    win32api.mouse_event(win32con.MOUSEEVENTF_RIGHTUP, x, y, 0, 0)","Simulates a mouse right click on pixel (x,y) if x and y are provided
    If x and y are not passed to this function, a mouse click is simulated at the current (x,y)

    :param x: target x-ordinate
    :param y: target y-ordinate
    :param hold_time: length of time to hold the mouse's right button
    :return: None"
"def register_to_random_name(grad_f):
  
  grad_f_name = grad_f.__name__ + ""_"" + str(uuid.uuid4())
  tf.RegisterGradient(grad_f_name)(grad_f)
  return grad_f_name","Register a gradient function to a random string.

  In order to use a custom gradient in TensorFlow, it must be registered to a
  string. This is both a hassle, and -- because only one function can every be
  registered to a string -- annoying to iterate on in an interactive
  environemnt.

  This function registers a function to a unique random string of the form:

    {FUNCTION_NAME}_{RANDOM_SALT}

  And then returns the random string. This is a helper in creating more
  convenient gradient overrides.

  Args:
    grad_f: gradient function to register. Should map (op, grad) -> grad(s)

  Returns:
    String that gradient function was registered to."
"def _definition(self):
        
        headerReference = self._sectPr.get_headerReference(self._hdrftr_index)
        return self._document_part.header_part(headerReference.rId)",|HeaderPart| object containing content of this header.
"def clip_out_of_image(self):
        
        lss_cut = [ls_clipped
                   for ls in self.line_strings
                   for ls_clipped in ls.clip_out_of_image(self.shape)]
        return LineStringsOnImage(lss_cut, shape=self.shape)","Clip off all parts of the line strings that are outside of the image.

        Returns
        -------
        imgaug.augmentables.lines.LineStringsOnImage
            Line strings, clipped to fall within the image dimensions."
"def get_environment(self, field=):
        
        env = {
            : self.sim_params.arena,
            : self.sim_params.data_frequency,
            : self.sim_params.first_open,
            : self.sim_params.last_close,
            : self.sim_params.capital_base,
            : self._platform
        }
        if field == :
            return env
        else:
            try:
                return env[field]
            except KeyError:
                raise ValueError(
                     % field,
                )","Query the execution environment.

        Parameters
        ----------
        field : {'platform', 'arena', 'data_frequency',
                 'start', 'end', 'capital_base', 'platform', '*'}
            The field to query. The options have the following meanings:
              arena : str
                  The arena from the simulation parameters. This will normally
                  be ``'backtest'`` but some systems may use this distinguish
                  live trading from backtesting.
              data_frequency : {'daily', 'minute'}
                  data_frequency tells the algorithm if it is running with
                  daily data or minute data.
              start : datetime
                  The start date for the simulation.
              end : datetime
                  The end date for the simulation.
              capital_base : float
                  The starting capital for the simulation.
              platform : str
                  The platform that the code is running on. By default this
                  will be the string 'zipline'. This can allow algorithms to
                  know if they are running on the Quantopian platform instead.
              * : dict[str -> any]
                  Returns all of the fields in a dictionary.

        Returns
        -------
        val : any
            The value for the field queried. See above for more information.

        Raises
        ------
        ValueError
            Raised when ``field`` is not a valid option."
"def zb_encode(zone, area, user_code):
    
    if zone < 0:
        zone = 0
    elif zone > Max.ZONES.value:
        zone = 999
    else:
        zone += 1
    return MessageEncode(.format(
        zone=zone, area=area+1, code=user_code), )",zb: Zone bypass. Zone < 0 unbypass all; Zone > Max bypass all.
"def get_bios(self):
        
        uri = ""{}/bios"".format(self.data[""uri""])
        return self._helper.do_get(uri)","Gets the list of BIOS/UEFI values currently set on the physical server.

        Returns:
            dict: Dictionary of BIOS/UEFI values."
"def local_bind_ports(self):
        
        self._check_is_started()
        return [_server.local_port for _server in self._server_list if
                _server.local_port is not None]",Return a list containing the ports of local side of the TCP tunnels
"def add_file(self, name="""", hashalg="""", hash="""", comClasses=None, 
                 typelibs=None, comInterfaceProxyStubs=None, 
                 windowClasses=None):
        
        self.files.append(File(name, hashalg, hash, comClasses, 
                          typelibs, comInterfaceProxyStubs, windowClasses))",Shortcut for manifest.files.append
"def get_default(__func: Callable, __arg: str) -> str:
    
    return signature(__func).parameters[__arg].default","Fetch default value for a function argument

    Args:
        __func: Function to inspect
        __arg: Argument to extract default value for"
"def default_repository(self):
        
        default_dict_repo = {}
        for line in self.default_repositories_list.splitlines():
            line = line.lstrip()
            if not line.startswith(""
                if line.split()[0] in self.DEFAULT_REPOS_NAMES:
                    default_dict_repo[line.split()[0]] = line.split()[1]
                else:
                    print(""\nslpkg: Error: Repository name  is not ""
                          ""default.\n              Please check file: ""
                          ""/etc/slpkg/default-repositories\n"".format(
                              line.split()[0]))
                    raise SystemExit()
        return default_dict_repo",Return dictionary with default repo name and url
"def _pypsa_generator_timeseries(network, timesteps, mode=None):
    

    mv_gen_timeseries_q = []
    mv_gen_timeseries_p = []
    lv_gen_timeseries_q = []
    lv_gen_timeseries_p = []

    
    if mode is  or mode is None:
        for gen in network.mv_grid.generators:
            mv_gen_timeseries_q.append(gen.pypsa_timeseries().rename(
                repr(gen)).to_frame().loc[timesteps])
            mv_gen_timeseries_p.append(gen.pypsa_timeseries().rename(
                repr(gen)).to_frame().loc[timesteps])
        if mode is :
            lv_gen_timeseries_p, lv_gen_timeseries_q = \
                _pypsa_generator_timeseries_aggregated_at_lv_station(
                    network, timesteps)

    
    if mode is  or mode is None:
        for lv_grid in network.mv_grid.lv_grids:
            for gen in lv_grid.generators:
                lv_gen_timeseries_q.append(gen.pypsa_timeseries().rename(
                    repr(gen)).to_frame().loc[timesteps])
                lv_gen_timeseries_p.append(gen.pypsa_timeseries().rename(
                    repr(gen)).to_frame().loc[timesteps])

    gen_df_p = pd.concat(mv_gen_timeseries_p + lv_gen_timeseries_p, axis=1)
    gen_df_q = pd.concat(mv_gen_timeseries_q + lv_gen_timeseries_q, axis=1)

    return gen_df_p, gen_df_q","Timeseries in PyPSA compatible format for generator instances

    Parameters
    ----------
    network : Network
        The eDisGo grid topology model overall container
    timesteps : array_like
        Timesteps is an array-like object with entries of type
        :pandas:`pandas.Timestamp<timestamp>` specifying which time steps
        to export to pypsa representation and use in power flow analysis.
    mode : str, optional
        Specifically retrieve generator time series for MV or LV grid level or
        both. Either choose 'mv' or 'lv'.
        Defaults to None, which returns both timeseries for MV and LV in a
        single DataFrame.

    Returns
    -------
    :pandas:`pandas.DataFrame<dataframe>`
        Time series table in PyPSA format"
"def to_query_parameters(parameters):
    
    if parameters is None:
        return []

    if isinstance(parameters, collections_abc.Mapping):
        return to_query_parameters_dict(parameters)

    return to_query_parameters_list(parameters)","Converts DB-API parameter values into query parameters.

    :type parameters: Mapping[str, Any] or Sequence[Any]
    :param parameters: A dictionary or sequence of query parameter values.

    :rtype: List[google.cloud.bigquery.query._AbstractQueryParameter]
    :returns: A list of query parameters."
"def _set_lsp_select_path(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=lsp_select_path.lsp_select_path, is_container=, presence=False, yang_name=""lsp-select-path"", rest_name=""select-path"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u: {u: u, u: None, u: None, u: None, u: None, u: u}}, namespace=, defining_module=, yang_type=, is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          : ,
          : ""container"",
          : ,
        })

    self.__lsp_select_path = t
    if hasattr(self, ):
      self._set()","Setter method for lsp_select_path, mapped from YANG variable /mpls_config/router/mpls/mpls_cmds_holder/lsp/lsp_select_path (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_lsp_select_path is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_lsp_select_path() directly."
"def delete_nic(self, instance_id, port_id):
        
        self.client.servers.interface_detach(instance_id, port_id)
        return True",Delete a Network Interface Controller
"def get_as_datadict(self):
        
        return dict(type=self.__class__.__name__, tags=list(self.tags))","Get information about this object as a dictionary.  Used by WebSocket interface to pass some
            relevant information to client applications."
"def page(self, log=values.unset, message_date_before=values.unset,
             message_date=values.unset, message_date_after=values.unset,
             page_token=values.unset, page_number=values.unset,
             page_size=values.unset):
        
        params = values.of({
            : log,
            : serialize.iso8601_date(message_date_before),
            : serialize.iso8601_date(message_date),
            : serialize.iso8601_date(message_date_after),
            : page_token,
            : page_number,
            : page_size,
        })

        response = self._version.page(
            ,
            self._uri,
            params=params,
        )

        return NotificationPage(self._version, response, self._solution)","Retrieve a single page of NotificationInstance records from the API.
        Request is executed immediately

        :param unicode log: Filter by log level
        :param date message_date_before: Filter by date
        :param date message_date: Filter by date
        :param date message_date_after: Filter by date
        :param str page_token: PageToken provided by the API
        :param int page_number: Page Number, this value is simply for client state
        :param int page_size: Number of records to return, defaults to 50

        :returns: Page of NotificationInstance
        :rtype: twilio.rest.api.v2010.account.call.notification.NotificationPage"
"def _extract_id_token(id_token):
    
    if type(id_token) == bytes:
        segments = id_token.split(b)
    else:
        segments = id_token.split(u)

    if len(segments) != 3:
        raise VerifyJwtTokenError(
            .format(id_token))

    return json.loads(
        _helpers._from_bytes(_helpers._urlsafe_b64decode(segments[1])))","Extract the JSON payload from a JWT.

    Does the extraction w/o checking the signature.

    Args:
        id_token: string or bytestring, OAuth 2.0 id_token.

    Returns:
        object, The deserialized JSON payload."
"def auto_instantiate(*classes):
    
    def decorator(f):
        
        sig = signature(f)

        @wraps(f)
        def _(*args, **kwargs):
            bvals = sig.bind(*args, **kwargs)

            
            for varname, val in bvals.arguments.items():
                anno = sig.parameters[varname].annotation

                if anno in classes or (len(classes) == 0 and anno != _empty):
                    bvals.arguments[varname] = anno(val)

            return f(*bvals.args, **bvals.kwargs)

        
        
        return FunctionMaker.create(
            f, , dict(_=_, __wrapped__=f)
        )

    return decorator","Creates a decorator that will instantiate objects based on function
    parameter annotations.

    The decorator will check every argument passed into ``f``. If ``f`` has an
    annotation for the specified parameter and the annotation is found in
    ``classes``, the parameter value passed in will be used to construct a new
    instance of the expression that is the annotation.

    An example (Python 3):

    .. code-block:: python

        @auto_instantiate(int)
        def foo(a: int, b: float):
            pass

    Any value passed in as ``b`` is left unchanged. Anything passed as the
    parameter for ``a`` will be converted to :class:`int` before calling the
    function.

    Since Python 2 does not support annotations, the
    :func:`~data.decorators.annotate` function should can be used:

    .. code-block:: python

        @auto_instantiate(int)
        @annotate(a=int)
        def foo(a, b):
            pass


    :param classes: Any number of classes/callables for which
                    auto-instantiation should be performed. If empty, perform
                    for all.

    :note: When dealing with data, it is almost always more convenient to use
           the :func:`~data.decorators.data` decorator instead."
"def truncate_database(self, database=None):
        
        
        if database in self.databases and database is not self.database:
            self.change_db(database)

        
        tables = self.tables if isinstance(self.tables, list) else [self.tables]
        if len(tables) > 0:
            self.drop(tables)
            self._printer( + str(len(tables)), , database)
        return tables",Drop all tables in a database.
"async def proxy(self, port, proxied_path):
        

        if  in self.request.headers:
            del self.request.headers[]

        self._record_activity()

        if self.request.headers.get(""Upgrade"", """").lower() == :
            
            
            self.log.info(""we wanna websocket, but we donPOSTContent-LengthTransfer-EncodingContent-EncodingConnectionSet-Cookie'
                    self.add_header(header, v)

            if response.body:
                self.write(response.body)","This serverextension handles:
            {base_url}/proxy/{port([0-9]+)}/{proxied_path}
            {base_url}/proxy/absolute/{port([0-9]+)}/{proxied_path}
            {base_url}/{proxy_base}/{proxied_path}"
"def invert(self):
        
        libfn = utils.get_lib_fn( % (self._libsuffix))
        inv_tx_ptr = libfn(self.pointer)

        new_tx = ANTsTransform(precision=self.precision, dimension=self.dimension,
                                transform_type=self.transform_type, pointer=inv_tx_ptr)

        return new_tx",Invert the transform
"def initRnaQuantificationSet(self):
        
        store = rnaseq2ga.RnaSqliteStore(self._args.filePath)
        store.createTables()",Initialize an empty RNA quantification set
"def compile_dir(env, src_path, dst_path, pattern=r, encoding=, base_dir=None):
  
  from os import path, listdir, mkdir
  file_re = re.compile(pattern)

  if base_dir is None:
    base_dir = src_path

  for filename in listdir(src_path):
    src_name = path.join(src_path, filename)
    dst_name = path.join(dst_path, filename)

    if path.isdir(src_name):
      mkdir(dst_name)
      compile_dir(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)
    elif path.isfile(src_name) and file_re.match(filename):
      compile_file(env, src_name, dst_name, encoding=encoding, base_dir=base_dir)","Compiles a directory of Jinja2 templates to python code.
  
  :param env: a Jinja2 Environment instance.
  :param src_path: path to the source directory.
  :param dst_path: path to the destination directory.
  :param encoding: template encoding.
  :param base_dir: the base path to be removed from the compiled template filename."
"def _regexp(expr, item):
    
    reg = re.compile(expr)
    return reg.search(item) is not None",REGEXP function for Sqlite
"def findPrefix(self, uri, default=None):
        
        for item in self.nsprefixes.items():
            if item[1] == uri:
                prefix = item[0]
                return prefix
        for item in self.specialprefixes.items():
            if item[1] == uri:
                prefix = item[0]
                return prefix
        if self.parent is not None:
            return self.parent.findPrefix(uri, default)
        else:
            return default","Find the first prefix that has been mapped to a namespace URI.
        The local mapping is searched, then it walks up the tree until
        it reaches the top or finds a match.
        @param uri: A namespace URI.
        @type uri: basestring
        @param default: A default prefix when not found.
        @type default: basestring
        @return: A mapped prefix.
        @rtype: basestring"
"def elemgetter(path: str) -> t.Callable[[Element], Element]:
    
    return compose(
        partial(_raise_if_none, exc=LookupError(path)),
        methodcaller(, path)
    )",shortcut making an XML element getter
"def key_json(minion_id,
             pillar,  
             pillar_key=None):
    
    key_data = __salt__[](minion_id)
    
    if not key_data:
        return {}

    data = salt.utils.json.loads(key_data)
    
    if isinstance(data, dict) and not pillar_key:
        return data
    elif not pillar_key:
        return {: data}
    else:
        return {pillar_key: data}","Pulls a string from redis and deserializes it from json. Deserialized
    dictionary data loaded directly into top level if pillar_key is not set.

    pillar_key
        Pillar key to return data into"
"def divergence(u, v, dx, dy):
    r
    dudx = first_derivative(u, delta=dx, axis=-1)
    dvdy = first_derivative(v, delta=dy, axis=-2)
    return dudx + dvdy","r""""""Calculate the horizontal divergence of the horizontal wind.

    Parameters
    ----------
    u : (M, N) ndarray
        x component of the wind
    v : (M, N) ndarray
        y component of the wind
    dx : float or ndarray
        The grid spacing(s) in the x-direction. If an array, there should be one item less than
        the size of `u` along the applicable axis.
    dy : float or ndarray
        The grid spacing(s) in the y-direction. If an array, there should be one item less than
        the size of `u` along the applicable axis.

    Returns
    -------
    (M, N) ndarray
        The horizontal divergence

    See Also
    --------
    vorticity

    Notes
    -----
    If inputs have more than two dimensions, they are assumed to have either leading dimensions
    of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``."
"def list(self, **request_parameters):
        
        
        items = self._session.get_items(
            API_ENDPOINT,
            params=request_parameters
        )

        
        for item in items:
            yield self._object_factory(OBJECT_TYPE, item)","List all roles.

        Args:
            **request_parameters: Additional request parameters (provides
                support for parameters that may be added in the future).

        Returns:
            GeneratorContainer: A GeneratorContainer which, when iterated,
            yields the roles returned by the Webex Teams query.

        Raises:
            TypeError: If the parameter types are incorrect.
            ApiError: If the Webex Teams cloud returns an error."
"def urls(self):
        
        for base_url, mapping in self.routes.items():
            for url, _ in mapping.items():
                yield base_url + url",Returns a generator of all URLs attached to this API
"def evaluate_dir(sample_dir):
    
    results = []
    if sample_dir[-1] == ""/"":
        sample_dir = sample_dir[:-1]
    for filename in glob.glob(""%s/*.inkml"" % sample_dir):
        results.append(evaluate_inkml(filename))
    return results","Evaluate all recordings in `sample_dir`.

    Parameters
    ----------
    sample_dir : string
        The path to a directory with *.inkml files.

    Returns
    -------
    list of dictionaries
        Each dictionary contains the keys 'filename' and 'results', where
        'results' itself is a list of dictionaries. Each of the results has
        the keys 'latex' and 'probability'"
"def _determineLength(self, fObj):
        
        try:
            seek = fObj.seek
            tell = fObj.tell
        except AttributeError:
            return UNKNOWN_LENGTH
        originalPosition = tell()
        seek(0, self._SEEK_END)
        end = tell()
        seek(originalPosition, self._SEEK_SET)
        return end - originalPosition","Determine how many bytes can be read out of C{fObj} (assuming it is not
        modified from this point on).  If the determination cannot be made,
        return C{UNKNOWN_LENGTH}."
"def send_pgrp(cls, sock, pgrp):
    
    assert(isinstance(pgrp, IntegerForPid) and pgrp < 0)
    encoded_int = cls.encode_int(pgrp)
    cls.write_chunk(sock, ChunkType.PGRP, encoded_int)",Send the PGRP chunk over the specified socket.
"def send(self, stat, value, rate=1):
        
        if rate < 1:
            value = ""%s|@%s"" % (value, rate)
        return super().send(stat, value, rate)",Send message to backend.
"def _set_customized(self, v, load=False):
    
    if hasattr(v, ""_utype""):
      v = v._utype(v)
    try:
      t = YANGDynClass(v,base=customized.customized, is_container=, presence=False, yang_name=""customized"", rest_name=""custom-profile"", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions={u: {u: u, u: u, u: None}}, namespace=, defining_module=, yang_type=, is_config=True)
    except (TypeError, ValueError):
      raise ValueError({
          : ,
          : ""container"",
          : ,
        })

    self.__customized = t
    if hasattr(self, ):
      self._set()","Setter method for customized, mapped from YANG variable /rbridge_id/hardware_profile/kap/customized (container)
    If this variable is read-only (config: false) in the
    source YANG file, then _set_customized is considered as a private
    method. Backends looking to populate this variable should
    do so via calling thisObj._set_customized() directly."
"def buildCommand(self,fileName,count,args):
        
        
	fileNameWithPath = self.escapePlaceholders(fileName)

        
        commandParts = args.command.split()
        processedParts = []
        
        for part in commandParts:
            processedParts.append(self.buildPart(part,fileNameWithPath,count,args))
        
        return self.unescapePlaceholders(.join(processedParts))","This is an internal method, building the command for a particular file."
"def match_function_id(self, function_id, match):
        
        self._add_match(, str(function_id), bool(match))","Matches the function identified by the given ``Id``.

        arg:    function_id (osid.id.Id): the Id of the ``Function``
        arg:    match (boolean): ``true`` if a positive match, ``false``
                for a negative match
        raise:  NullArgument - ``function_id`` is ``null``
        *compliance: mandatory -- This method must be implemented.*"
"def send_location(self, chat_id, latitude, longitude, live_period=None, reply_to_message_id=None, reply_markup=None,
                      disable_notification=None):
        
        return types.Message.de_json(
            apihelper.send_location(self.token, chat_id, latitude, longitude, live_period, reply_to_message_id,
                                    reply_markup,
                                    disable_notification))","Use this method to send point on the map.
        :param chat_id:
        :param latitude:
        :param longitude:
        :param live_period
        :param reply_to_message_id:
        :param reply_markup:
        :return: API reply."
"def generate_plaintext_random(plain_vocab, distribution, train_samples,
                              length):
  
  if distribution is not None:
    assert len(distribution) == len(plain_vocab)

  train_indices = np.random.choice(
      range(len(plain_vocab)), (train_samples, length), p=distribution)

  return train_indices","Generates samples of text from the provided vocabulary.

  Args:
    plain_vocab: vocabulary.
    distribution: distribution.
    train_samples: samples for training.
    length: length.

  Returns:
    train_indices (np.array of Integers): random integers for training.
      shape = [num_samples, length]
    test_indices (np.array of Integers): random integers for testing.
      shape = [num_samples, length]
    plain_vocab   (list of Integers): unique vocabularies."
"def get_fields(model_class):
    
    return [
        attr for attr, value in model_class.__dict__.items()
        if issubclass(type(value), (mongo.base.BaseField, mongo.EmbeddedDocumentField))  
    ]","Pass in a mongo model class and extract all the attributes which
    are mongoengine fields

    Returns:
        list of strings of field attributes"
"def describe(self):
        
        response = {
            : self.state,
            : self.model_name,
            : self.name,
            : _UNUSED_ARN,
            : self.end_time,
            : self.start_time,
            : self.start_time,
            : {},
            : self.batch_strategy,
        }

        if self.transform_resources:
            response[] = self.transform_resources

        if self.output_data:
            response[] = self.output_data

        if self.input_data:
            response[] = self.input_data

        return response","Describe this _LocalTransformJob

        The response is a JSON-like dictionary that follows the response of the
        boto describe_transform_job() API.

        Returns:
            dict: description of this _LocalTransformJob"
"def set_col_width(self, col, tab, width):
        

        try:
            old_width = self.col_widths.pop((col, tab))

        except KeyError:
            old_width = None

        if width is not None:
            self.col_widths[(col, tab)] = float(width)",Sets column width
"def save_load(jid, clear_load, minion=None):
    
    cb_ = _get_connection()

    try:
        jid_doc = cb_.get(six.text_type(jid))
    except couchbase.exceptions.NotFoundError:
        cb_.add(six.text_type(jid), {}, ttl=_get_ttl())
        jid_doc = cb_.get(six.text_type(jid))

    jid_doc.value[] = clear_load
    cb_.replace(six.text_type(jid), jid_doc.value, cas=jid_doc.cas, ttl=_get_ttl())

    
    if  in clear_load and clear_load[] != :
        ckminions = salt.utils.minions.CkMinions(__opts__)
        
        _res = ckminions.check_minions(
            clear_load[],
            clear_load.get(, )
            )
        minions = _res[]
        save_minions(jid, minions)",Save the load to the specified jid
"def is_dominated(self, action, tol=None, method=None):
        
        if tol is None:
            tol = self.tol

        payoff_array = self.payoff_array

        if self.num_opponents == 0:
            return payoff_array.max() > payoff_array[action] + tol

        ind = np.ones(self.num_actions, dtype=bool)
        ind[action] = False
        D = payoff_array[ind]
        D -= payoff_array[action]
        if D.shape[0] == 0:  
            return False
        if self.num_opponents >= 2:
            D.shape = (D.shape[0], np.prod(D.shape[1:]))

        if method is None:
            from .lemke_howson import lemke_howson
            g_zero_sum = NormalFormGame([Player(D), Player(-D.T)])
            NE = lemke_howson(g_zero_sum)
            return NE[0] @ D @ NE[1] > tol
        elif method in [, ]:
            from scipy.optimize import linprog
            m, n = D.shape
            A = np.empty((n+2, m+1))
            A[:n, :m] = -D.T
            A[:n, -1] = 1  
            A[n, :m], A[n+1, :m] = 1, -1  
            A[n:, -1] = 0
            b = np.empty(n+2)
            b[:n] = 0
            b[n], b[n+1] = 1, -1
            c = np.zeros(m+1)
            c[-1] = -1
            res = linprog(c, A_ub=A, b_ub=b, method=method)
            if res.success:
                return res.x[-1] > tol
            elif res.status == 2:  
                return False
            else:  
                msg = .format(res.status)
                raise RuntimeError(msg)
        else:
            raise ValueError(.format(method))","Determine whether `action` is strictly dominated by some mixed
        action.

        Parameters
        ----------
        action : scalar(int)
            Integer representing a pure action.

        tol : scalar(float), optional(default=None)
            Tolerance level used in determining domination. If None,
            default to the value of the `tol` attribute.

        method : str, optional(default=None)
            If None, `lemke_howson` from `quantecon.game_theory` is used
            to solve for a Nash equilibrium of an auxiliary zero-sum
            game. If `method` is set to `'simplex'` or
            `'interior-point'`, `scipy.optimize.linprog` is used with
            the method as specified by `method`.

        Returns
        -------
        bool
            True if `action` is strictly dominated by some mixed action;
            False otherwise."
"def touch(self):
        
        column = self.get_related().get_updated_at_column()

        self.raw_update({column: self.get_related().fresh_timestamp()})",Touch all of the related models for the relationship.
"def _example_stock_basic(quote_ctx):
    
    ret_status, ret_data = quote_ctx.get_stock_basicinfo(ft.Market.HK, ft.SecurityType.STOCK)
    if ret_status != ft.RET_OK:
        print(ret_data)
        exit()
    print(""stock_basic"")
    print(ret_data)", 
"def create_snapshot(self, xml_bytes):
        
        root = XML(xml_bytes)
        snapshot_id = root.findtext(""snapshotId"")
        volume_id = root.findtext(""volumeId"")
        status = root.findtext(""status"")
        start_time = root.findtext(""startTime"")
        start_time = datetime.strptime(
            start_time[:19], ""%Y-%m-%dT%H:%M:%S"")
        progress = root.findtext(""progress"")[:-1]
        progress = float(progress or ""0"") / 100.
        return model.Snapshot(
            snapshot_id, volume_id, status, start_time, progress)","Parse the XML returned by the C{CreateSnapshot} function.

        @param xml_bytes: XML bytes with a C{CreateSnapshotResponse} root
            element.
        @return: The L{Snapshot} instance created.

        TODO: ownerId, volumeSize, description."
"def check_dates(self):
        
        if self.checkin_date >= self.checkout_date:
                raise ValidationError(_())
        if self.folio_id.date_order and self.checkin_date:
            if self.checkin_date <= self.folio_id.date_order:
                raise ValidationError(_())","This method is used to validate the checkin_date and checkout_date.
        -------------------------------------------------------------------
        @param self: object pointer
        @return: raise warning depending on the validation"
"def obo(self):
        
        def add_tags(stanza_list, tags):
            for tag in tags:
                if tag in self.other:
                    if isinstance(self.other[tag], list):
                        for attribute in self.other[tag]:
                            stanza_list.append(""{}: {}"".format(tag, attribute))
                    else:
                        stanza_list.append(""{}: {}"".format(tag, self.other[tag]))

        
        
        
        
        

        stanza_list = [""[Term]""]

        
        stanza_list.append(""id: {}"".format(self.id))


        
        if self.name is not None:
            stanza_list.append(""name: {}"".format(self.name))
        else:
            stanza_list.append(""name: "")

        add_tags(stanza_list, [, ])

        
        if self.desc:
            stanza_list.append(self.desc.obo)

        
        add_tags(stanza_list, [, ])

        
        for synonym in sorted(self.synonyms, key=str):
            stanza_list.append(synonym.obo)

        add_tags(stanza_list, [])

        
        if Relationship() in self.relations:
            for companion in self.relations[Relationship()]:
                stanza_list.append(""is_a: {} ! {}"".format(companion.id, companion.name))

        add_tags(stanza_list, [, , ])

        for relation in self.relations:
            if relation.direction==""bottomup"" and relation is not Relationship():
                stanza_list.extend(
                    ""relationship: {} {} ! {}"".format(
                        relation.obo_name, companion.id, companion.name
                    ) for companion in self.relations[relation]
                )

        add_tags(stanza_list, [, , ,
                               , , ])

        return ""\n"".join(stanza_list)","str: the `Term` serialized in an Obo ``[Term]`` stanza.

        Note:
            The following guide was used:
            ftp://ftp.geneontology.org/pub/go/www/GO.format.obo-1_4.shtml"
"def succ(cmd, check_stderr=True, stdout=None, stderr=None):
    
    code, out, err = run(cmd)

    
    if stdout is not None:
        stdout[:] = out
    if stderr is not None:
        stderr[:] = err

    if code != 0:
        for l in out:
            print(l)
    assert code == 0, .format(code, cmd, err)
    if check_stderr:
        assert err == [], .format(err, code)
    return code, out, err",Alias to run with check return code and stderr
"def complete_abstract_value(
    exe_context,  
    return_type,  
    field_asts,  
    info,  
    path,  
    result,  
):
    
    
    runtime_type = None  

    
    if isinstance(return_type, (GraphQLInterfaceType, GraphQLUnionType)):
        if return_type.resolve_type:
            runtime_type = return_type.resolve_type(result, info)
        else:
            runtime_type = get_default_resolve_type_fn(result, info, return_type)

    if isinstance(runtime_type, string_types):
        runtime_type = info.schema.get_type(runtime_type)  

    if not isinstance(runtime_type, GraphQLObjectType):
        raise GraphQLError(
            (
                ""Abstract type {} must resolve to an Object type at runtime ""
                + 
            ).format(
                return_type, info.parent_type, info.field_name, result, runtime_type
            ),
            field_asts,
        )

    if not exe_context.schema.is_possible_type(return_type, runtime_type):
        raise GraphQLError(
            u.format(
                runtime_type, return_type
            ),
            field_asts,
        )

    return complete_object_value(
        exe_context, runtime_type, field_asts, info, path, result
    )","Complete an value of an abstract type by determining the runtime type of that value, then completing based
    on that type."
"def import_locations(self, zone_file):
        
        self._zone_file = zone_file
        field_names = (, , , )

        data = utils.prepare_csv_read(zone_file, field_names, delimiter=r""	"")

        for row in (x for x in data if not x[].startswith()):
            if row[]:
                row[] = row[].split()
            self.append(Zone(**row))","Parse zoneinfo zone description data files.

        ``import_locations()`` returns a list of :class:`Zone` objects.

        It expects data files in one of the following formats::

            AN	+1211-06900	America/Curacao
            AO	-0848+01314	Africa/Luanda
            AQ	-7750+16636	Antarctica/McMurdo	McMurdo Station, Ross Island

        Files containing the data in this format can be found in the
        :file:`zone.tab` file that is normally found in
        :file:`/usr/share/zoneinfo` on UNIX-like systems, or from the `standard
        distribution site`_.

        When processed by ``import_locations()`` a ``list`` object of the
        following style will be returned::

            [Zone(None, None, ""AN"", ""America/Curacao"", None),
             Zone(None, None, ""AO"", ""Africa/Luanda"", None),
             Zone(None, None, ""AO"", ""Antartica/McMurdo"",
                  [""McMurdo Station"", ""Ross Island""])]

        Args:
            zone_file (iter): ``zone.tab`` data to read

        Returns:
            list: Locations as :class:`Zone` objects

        Raises:
            FileFormatError: Unknown file format

        .. _standard distribution site: ftp://elsie.nci.nih.gov/pub/"
"def change_node_subscriptions(self, jid, node, subscriptions_to_set):
        
        iq = aioxmpp.stanza.IQ(
            type_=aioxmpp.structs.IQType.SET,
            to=jid,
            payload=pubsub_xso.OwnerRequest(
                pubsub_xso.OwnerSubscriptions(
                    node,
                    subscriptions=[
                        pubsub_xso.OwnerSubscription(
                            jid,
                            subscription
                        )
                        for jid, subscription in subscriptions_to_set
                    ]
                )
            )
        )

        yield from self.client.send(iq)","Update the subscriptions at a node.

        :param jid: Address of the PubSub service.
        :type jid: :class:`aioxmpp.JID`
        :param node: Name of the node to modify
        :type node: :class:`str`
        :param subscriptions_to_set: The subscriptions to set at the node.
        :type subscriptions_to_set: :class:`~collections.abc.Iterable` of
            tuples consisting of the JID to (un)subscribe and the subscription
            level to use.
        :raises aioxmpp.errors.XMPPError: as returned by the service

        `subscriptions_to_set` must be an iterable of pairs (`jid`,
        `subscription`), where the `jid` indicates the JID for which the
        `subscription` is to be set."
"def group(self, key, condition, initial, reduce, finalize=None, **kwargs):
        
        warnings.warn(""The group method is deprecated and will be removed in ""
                      ""PyMongo 4.0. Use the aggregate method with the $group ""
                      ""stage or the map_reduce method instead."",
                      DeprecationWarning, stacklevel=2)
        group = {}
        if isinstance(key, string_type):
            group[""$keyf""] = Code(key)
        elif key is not None:
            group = {""key"": helpers._fields_list_to_dict(key, ""key"")}
        group[""ns""] = self.__name
        group[""$reduce""] = Code(reduce)
        group[""cond""] = condition
        group[""initial""] = initial
        if finalize is not None:
            group[""finalize""] = Code(finalize)

        cmd = SON([(""group"", group)])
        collation = validate_collation_or_none(kwargs.pop(, None))
        cmd.update(kwargs)

        with self._socket_for_reads(session=None) as (sock_info, slave_ok):
            return self._command(sock_info, cmd, slave_ok,
                                 collation=collation,
                                 user_fields={: 1})[""retval""]","Perform a query similar to an SQL *group by* operation.

        **DEPRECATED** - The group command was deprecated in MongoDB 3.4. The
        :meth:`~group` method is deprecated and will be removed in PyMongo 4.0.
        Use :meth:`~aggregate` with the `$group` stage or :meth:`~map_reduce`
        instead.

        .. versionchanged:: 3.5
           Deprecated the group method.
        .. versionchanged:: 3.4
           Added the `collation` option.
        .. versionchanged:: 2.2
           Removed deprecated argument: command"
"def validate(self):
        
        validation_code = self.setup_src +  + self.stmt
        validation_scope = {}
        exec(validation_code, validation_scope)
        
        if len(self.groups[self.group]) == 1:
            self.result = validation_scope[]
            logging.info(
                         .format(b=self))
        else:
            compare_against_benchmark = self.groups[self.group][0]
            test = [benchmark.result_validation for benchmark in self.groups[self.group]]
            if not all(test):
                raise ValueError()
            compare_result = compare_against_benchmark.result
            if self.validation_func:
                results_are_valid = self.validation_func(compare_result, validation_scope[])
            else:
                results_are_valid = compare_result == validation_scope[]
            if results_are_valid:
                logging.info(.format(self.callable.__name__))
            else:
                error = 
                raise ValidationError(error.format(compare_against_benchmark.callable.__name__, self.callable.__name__,
                                          compare_result, validation_scope[]))","Execute the code once to get it's results (to be used in function validation). Compare the result to the
        first function in the group."
"def switch(self, name):
        
        try:
            switch = self.storage[self.__namespaced(name)]
        except KeyError:
            if not self.autocreate:
                raise ValueError(""No switch named  registered in "" % (name, self.namespace))

            switch = self.__create_and_register_disabled_switch(name)

        switch.manager = self
        return switch","Returns the switch with the provided ``name``.

        If ``autocreate`` is set to ``True`` and no switch with that name
        exists, a ``DISABLED`` switch will be with that name.

        Keyword Arguments:
        name -- A name of a switch."
"def register_list(self):
        
        num_items = self.MAX_NUM_CPU_REGISTERS
        buf = (ctypes.c_uint32 * num_items)()
        num_regs = self._dll.JLINKARM_GetRegisterList(buf, num_items)
        return buf[:num_regs]","Returns a list of the indices for the CPU registers.

        The returned indices can be used to read the register content or grab
        the register name.

        Args:
          self (JLink): the ``JLink`` instance

        Returns:
          List of registers."
"def get_instance(self, payload):
        
        return TokenInstance(self._version, payload, account_sid=self._solution[], )","Build an instance of TokenInstance

        :param dict payload: Payload response from the API

        :returns: twilio.rest.api.v2010.account.token.TokenInstance
        :rtype: twilio.rest.api.v2010.account.token.TokenInstance"
"def _spec(self, name):
        ""Return the named spec.""
        for s in self._framespec:
            if s.name == name:
                return s
        raise ValueError(""Unknown spec: "" + name)",Return the named spec.
"def revoke_permission(user, permission_name):
    
    roles = get_user_roles(user)

    for role in roles:
        if permission_name in role.permission_names_list():
            permission = get_permission(permission_name)
            user.user_permissions.remove(permission)
            return

    raise RolePermissionScopeException(
        ""This permission isns roles."")","Revoke a specified permission from a user.

    Permissions are only revoked if they are in the scope any of the user's
    roles. If the permission is out of scope, a RolePermissionScopeException
    is raised."
"def fix_whitespace(tokens, start, result):
    
    for e in result:
        for child in e.iter():
            child.text = child.text.replace(, )
            for hyphen in HYPHENS:
                child.text = child.text.replace( % hyphen,  % hyphen)
            child.text = re.sub(r, r, child.text)
    return result",Fix whitespace around hyphens and commas. Can be used to remove whitespace tokenization artefacts.
"def arc_node_edit_add_missing_characters_and_locations_to_related_story_node(
        sender,
        instance,
        action,
        reverse,
        pk_set,
        *args,
        **kwargs
):
    s characters/locations are not already in the story node, add them.
    We don
    if action == :
        logger.debug(""Updating nodes after character or location change."")
        if reverse:
            logger.debug(""Searching backwards from character or location to arc node"")
            
            for arcnode in instance.arcelementnode_set.all().select_related():
                logger.debug(""Scanning arc node..."")
                if arcnode.story_element_node:
                    logger.debug(""Found story node to update..."")
                    story_node = arcnode.story_element_node
                    if isinstance(instance, CharacterInstance):
                        logger.debug(""Updating characters..."")
                        story_node.assoc_characters.add(instance)
                    if isinstance(instance, LocationInstance):
                        logger.debug(""updating locations..."")
                        story_node.assoc_locations.add(instance)
        else:
            
            logger.debug()
            if instance.story_element_node:
                logger.debug(""found story node to update..."")
                story_node = instance.story_element_node
                if sender == ArcElementNode.assoc_characters.through:
                    logger.debug()
                    story_node.assoc_characters.add(*pk_set)
                if sender == ArcElementNode.assoc_locations.through:
                    logger.debug()
                    story_node.assoc_locations.add(*pk_set)","If an arc_element is modified and it's characters/locations are not already in the story node, add them.
    We don't assume that removing the arc element would change the characters or locations as of yet.
    This takes up a little more space in the database, but the additional flexibility for users is
    worth it."
"def _parse(self):
        
        parser = None
        previous = None

        for line in self.lines:

            parser = self._get_mapping(line, parser, previous)

            
            if parser:
                parser(line)

            previous = line","parse is the base function for parsing the Dockerfile, and extracting
           elements into the correct data structures. Everything is parsed into
           lists or dictionaries that can be assembled again on demand. 

           Environment: Since Docker also exports environment as we go, 
                        we add environment to the environment section and 
                        install

           Labels: include anything that is a LABEL, ARG, or (deprecated)
                   maintainer.

           Add/Copy: are treated the same"
"def add_context(self, context, label=None):
        

        assert isinstance(context, dict)

        item = defaults[""common""].copy()
        item.update(defaults[""instance""])
        item.update(context)

        item[""family""] = None
        item[""label""] = context[""data""].get(""label"") or settings.ContextLabel
        item[""itemType""] = ""instance""
        item[""isToggled""] = True
        item[""optional""] = False
        item[""hasCompatible""] = True

        item = self.add_item(item)
        self.instances.append(item)","Append `context` to model

        Arguments:
            context (dict): Serialised to add

        Schema:
            context.json"
"def run(self, x):
        
        
        try:    
            x = np.array(x)
        except:
            raise ValueError()
        N = len(x)   
        
        if self.outputs == 1:
            y = np.zeros(N)
        else:
            y = np.zeros((N, self.outputs))
        
        for k in range(N):
            y[k] = self.predict(x[k])
        return y","Function for batch usage of already trained and tested MLP.

        **Args:**

        * `x` : input array (2-dimensional array).
            Every row represents one input vector (features).

        **Returns:**
        
        * `y`: output vector (n-dimensional array). Every row represents
            output (outputs) for an input vector."
"def has_running_jobs(self) -> bool:
        
        return self.jobs.exclude(status__status__in=ExperimentLifeCycle.DONE_STATUS).exists()",Return a boolean indicating if the experiment has any running jobs
"def materialize_dict(bundle: dict, separator: str = ) -> t.List[t.Tuple[str, t.Any]]:
    

    def _matkeysort(tup: t.Tuple[str, t.Any]):
        return len(tup[0].split(separator))

    s1 = sorted(_materialize_dict(bundle, separator=separator), key=lambda x: x[0])
    return sorted(s1, key=_matkeysort, reverse=True)","Transforms a given ``bundle`` into a *sorted* list of tuples with materialized value paths and values:
    ``('path.to.value', <value>)``. Output is ordered by depth: the deepest element first.

    :param bundle: a dict to materialize
    :param separator: build paths with a given separator
    :return: a depth descending and alphabetically ascending sorted list (-deep, asc), the longest first

    ::

        sample = {
            'a': 1,
            'aa': 1,
            'b': {
                'c': 1,
                'b': 1,
                'a': 1,
                'aa': 1,
                'aaa': {
                    'a': 1
                }
            }
        }
        materialize_dict(sample, '/')
        [
            ('b/aaa/a', 1),
            ('b/a', 1),
            ('b/aa', 1),
            ('b/b', 1),
            ('b/c', 1),
            ('a', 1),
            ('aa', 1)
        ]"
"def pack(o, stream, **kwargs):
    
    packer = Packer(**kwargs)
    stream.write(packer.pack(o))","Pack object `o` and write it to `stream`

    See :class:`Packer` for options."
"def createpath(path, mode, exists_ok=True):
    
    try:
        os.makedirs(path, mode)
    except OSError, e:
        if e.errno != errno.EEXIST or not exists_ok:
            raise e","Create directories in the indicated path.
    :param path:
    :param mode:
    :param exists_ok:
    :return:"
"def get_model_abbrev(model_obj):
    
    
    model_type = model_obj.model_type
    
    for key in model_type_to_display_name:
        if model_type_to_display_name[key] == model_type:
            return key
    
    
    msg = ""Model object has an unknown or incorrect model type.""
    raise ValueError(msg)","Extract the string used to specify the model type of this model object in
    `pylogit.create_chohice_model`.

    Parameters
    ----------
    model_obj : An MNDC_Model instance.

    Returns
    -------
    str. The internal abbreviation used for the particular type of MNDC_Model."
"def _send(self, messages):
        
        if len(messages) == 1:
            to_send = self._build_message(messages[0])
            if to_send is False:
                
                
                return False
        else:
            pm_messages = list(map(self._build_message, messages))
            pm_messages = [m for m in pm_messages if m]
            if len(pm_messages) == 0:
                
                
                return False
            to_send = PMBatchMail(messages=pm_messages)
        try:
            to_send.send(test=self.test_mode)
        except:
            if self.fail_silently:
                return False
            raise
        return True",A helper method that does the actual sending.
"def setup(self):
        
        r = self.call_ext_prog(self.get_option(""script""))
        if r[] == 0:
            tarfile = """"
            for line in r[]:
                line = line.strip()
                tarfile = self.do_regex_find_all(r""ftp (.*tar.gz)"", line)
            if len(tarfile) == 1:
                self.add_copy_spec(tarfile[0])",interface with vrtsexplorer to capture veritas related data
"def is_visible(self, pos: Union[Point2, Point3, Unit]) -> bool:
        
        
        assert isinstance(pos, (Point2, Point3, Unit))
        pos = pos.position.to2.rounded
        return self.state.visibility[pos] == 2",Returns True if you have vision on a grid point.
"def sparse_dot_product_attention_truncated(
    q,
    k,
    v,
    bi,  
    experts_params,
    use_map_fn=False,  
    mask_right=False,
):  
  
  
  batch_size, nb_heads, _, depth = common_layers.shape_list(q)

  total_loss = 0.0

  
  list_lsh = [LshGating(depth=depth, **experts_params) for _ in range(nb_heads)]

  @expert_utils.add_name_scope()
  def get_gates_head(x, add_first=False):
    
    length = common_layers.shape_list(x)[2]

    
    x = tf.transpose(x, perm=[1, 0, 2, 3])
    x = tf.reshape(x, [nb_heads, batch_size * length, depth])

    list_x = tf.unstack(x)  

    
    list_gates = []
    
    for lsh, single_x in zip(list_lsh, list_x):
      
      gates = lsh.get_gates(single_x)
      nb_buckets = gates.get_shape().as_list()[-1]
      
      
      gates = tf.reshape(gates, [batch_size, length, nb_buckets])
      list_gates.append(gates)

    gates = tf.stack(list_gates)

    
    gates = tf.reshape(gates, [nb_heads, batch_size, length, nb_buckets])
    gates = tf.transpose(gates, [1, 0, 2, 3])

    
    if add_first:
      gates = tf.maximum(gates,
                         tf.reshape(tf.one_hot([0], length), [1, 1, length, 1]))

    return gates

  gates_q = get_gates_head(q)
  gates_k = get_gates_head(k, add_first=True)

  
  q, k, v, gates_q, gates_k = [
      combine_first_two_dimensions(t) for t in (q, k, v, gates_q, gates_k)
  ]

  v_out = dot_product_batched_head(q, k, v, gates_q, gates_k, mask_right)

  
  v_out = tf.reshape(v_out, [batch_size, nb_heads, -1, depth])

  return v_out, total_loss / nb_heads","Sparse multihead self attention.

  Perform an approximation of the full multihead attention by dispatching
  the tokens using their keys/values. Thus the attention matrix are only
  computed each times on a subset of the tokens.

  Notes:
   * The function don't perform scaling here (multihead_attention does
  the /sqrt(depth)).
   * The padding should have been removed (so batch size should be 1 but length
   contains the elements from all different batches)
   * Right now, only self attention is supported so length_q and length_kv
   should be identical and the function will add triangular mask.
   * If bi.order is not None, The bias is added inside this function to
   prevent attention to the future.

  Args:
    q (tf.Tensor): Queries of shape [batch, heads, length_q, depth_k]
    k (tf.Tensor): Keys of shape [batch, heads, length_q, depth_k]
    v (tf.Tensor): Values of shape [batch, heads, length_kv, depth_v]
    bi (BatchInfo): Contains the batch coordinates and sequence order
    experts_params (dict): Additional params for the local expert
    use_map_fn (bool): Use either tf.map_fn of python for loop to compute the
      heads separately
    mask_right (bool):
  Returns:
    tf.Tensor: Approximation of Softmax(Q.K) * V, of shape
      [batch, heads, length_q, depth_v]"
"def member_add(self, repl_id, params):
        
        repl = self[repl_id]
        member_id = repl.repl_member_add(params)
        self[repl_id] = repl
        return member_id","create instance and add it to existing replcia
        Args:
            repl_id - replica set identity
            params - member params

        return True if operation success otherwise False"
"def fetch_page_async(self, page_size, **q_options):
    
    qry = self._fix_namespace()
    return qry._fetch_page_async(page_size, **q_options)","Fetch a page of results.

    This is the asynchronous version of Query.fetch_page()."
"def lookup_expand(self, stmt, names):
        
        if not names: return []
        todo = [stmt]
        while todo:
            pst = todo.pop()
            for sub in pst.substmts:
                if sub.keyword in self.schema_nodes:
                    qname = self.qname(sub)
                    if qname in names:
                        names.remove(qname)
                        par = sub.parent
                        while hasattr(par,""d_ref""): 
                            par.d_ref.d_expand = True
                            par = par.d_ref.parent
                        if not names: return [] 
                elif sub.keyword == ""uses"":
                    g = sub.i_grouping
                    g.d_ref = sub
                    todo.append(g)
        return names","Find schema nodes under `stmt`, also in used groupings.

        `names` is a list with qualified names of the schema nodes to
        look up. All 'uses'/'grouping' pairs between `stmt` and found
        schema nodes are marked for expansion."
"def canonical_key(self, key):
        
        if key.startswith():
            return urlparse.urljoin(self.base_uri, key)
        else:
            return self.curies.expand(key)",Returns the canonical key for the given ``key``.
"def save_local_scope(
        self,
        line_number,
        saved_function_call_index
    ):
        
        saved_variables = list()
        saved_variables_so_far = set()
        first_node = None

        
        for assignment in [node for node in self.nodes
                           if (type(node) == AssignmentNode or
                               type(node) == AssignmentCallNode or
                               type(Node) == BBorBInode)]:  
            if assignment.left_hand_side in saved_variables_so_far:
                continue
            saved_variables_so_far.add(assignment.left_hand_side)
            save_name = .format(saved_function_call_index, assignment.left_hand_side)

            previous_node = self.nodes[-1]

            saved_scope_node = RestoreNode(
                save_name +  + assignment.left_hand_side,
                save_name,
                [assignment.left_hand_side],
                line_number=line_number,
                path=self.filenames[-1]
            )
            if not first_node:
                first_node = saved_scope_node

            self.nodes.append(saved_scope_node)
            
            saved_variables.append(SavedVariable(LHS=save_name,
                                                 RHS=assignment.left_hand_side))
            self.connect_if_allowed(previous_node, saved_scope_node)

        return (saved_variables, first_node)","Save the local scope before entering a function call by saving all the LHS's of assignments so far.

        Args:
            line_number(int): Of the def of the function call about to be entered into.
            saved_function_call_index(int): Unique number for each call.

        Returns:
            saved_variables(list[SavedVariable])
            first_node(EntryOrExitNode or None or RestoreNode): Used to connect previous statements to this function."
"def read(self, size=-1):
        
        if self.left is not None:
            size = min(size, self.left)
        if self.closed:
            raise ValueError()
        if size < 0:
            return .join(self)
        elif not size:
            chunk = 
        elif self.buf:
            chunk = self.buf
            self.buf = None
        else:
            try:
                chunk = next(self.iterator)
            except StopIteration:
                return 
        if len(chunk) > size:
            self.buf = chunk[size:]
            chunk = chunk[:size]
        if self.left is not None:
            self.left -= len(chunk)
        return chunk","read([size]) -> read at most size bytes, returned as a string.

        If the size argument is negative or omitted, read until EOF is reached.
        Notice that when in non-blocking mode, less data than what was
        requested may be returned, even if no size parameter was given."
"def __diff_internal(self):
        
        assert self.p > 0, ""order of Bspline must be > 0""  

        
        
        t    = self.knot_vector
        p    = self.p
        Bi   = Bspline( t[:-1], p-1 )
        Bip1 = Bspline( t[1:],  p-1 )

        numer1 = +p
        numer2 = -p
        denom1 = t[p:-1]   - t[:-(p+1)]
        denom2 = t[(p+1):] - t[1:-p]

        with np.errstate(divide=, invalid=):
            ci   = np.where(denom1 != 0., (numer1 / denom1), 0.)
            cip1 = np.where(denom2 != 0., (numer2 / denom2), 0.)

        return ( (ci,Bi), (cip1,Bip1) )","Differentiate a B-spline once, and return the resulting coefficients and Bspline objects.

This preserves the Bspline object nature of the data, enabling recursive implementation
of higher-order differentiation (see `diff`).

The value of the first derivative of `B` at a point `x` can be obtained as::

    def diff1(B, x):
        terms = B.__diff_internal()
        return sum( ci*Bi(x) for ci,Bi in terms )

Returns:
    tuple of tuples, where each item is (coefficient, Bspline object).

See:
    `diff`: differentiation of any order >= 0"
"def AddMethod(self, interface, name, in_sig, out_sig, code):
        s main
                   interface (as specified on construction).
        name: Name of the method
        in_sig: Signature of input arguments; for example ""ias"" for a method
                that takes an int32 and a string array as arguments; see
                http://dbus.freedesktop.org/doc/dbus-specification.html
        out_sig: Signature of output arguments; for example ""s"" for a method
                 that returns a string; use  for methods that do not return
                 anything.
        code: Python 3 code to run in the method call; you have access to the
              arguments through the ""args"" list, and can set the return value
              by assigning a value to the ""ret"" variable. You can also read the
              global ""objects"" variable, which is a dictionary mapping object
              paths to DBusMockObject instances.

              For keeping state across method calls, you are free to use normal
              Python members of the ""self"" object, which will be persistent for
              the whole mock
        if not interface:
            interface = self.interface
        n_args = len(dbus.Signature(in_sig))

        
        
        
        
        method = lambda self, *args, **kwargs: DBusMockObject.mock_method(
            self, interface, name, in_sig, *args, **kwargs)

        
        
        dbus_method = dbus.service.method(interface,
                                          out_signature=out_sig)(method)
        dbus_method.__name__ = str(name)
        dbus_method._dbus_in_signature = in_sig
        dbus_method._dbus_args = [ % i for i in range(1, n_args + 1)]

        
        
        if interface == self.interface:
            setattr(self.__class__, name, dbus_method)

        self.methods.setdefault(interface, {})[str(name)] = (in_sig, out_sig, code, dbus_method)","Add a method to this object

        interface: D-Bus interface to add this to. For convenience you can
                   specify '' here to add the method to the object's main
                   interface (as specified on construction).
        name: Name of the method
        in_sig: Signature of input arguments; for example ""ias"" for a method
                that takes an int32 and a string array as arguments; see
                http://dbus.freedesktop.org/doc/dbus-specification.html#message-protocol-signatures
        out_sig: Signature of output arguments; for example ""s"" for a method
                 that returns a string; use '' for methods that do not return
                 anything.
        code: Python 3 code to run in the method call; you have access to the
              arguments through the ""args"" list, and can set the return value
              by assigning a value to the ""ret"" variable. You can also read the
              global ""objects"" variable, which is a dictionary mapping object
              paths to DBusMockObject instances.

              For keeping state across method calls, you are free to use normal
              Python members of the ""self"" object, which will be persistent for
              the whole mock's life time. E. g. you can have a method with
              ""self.my_state = True"", and another method that returns it with
              ""ret = self.my_state"".

              When specifying '', the method will not do anything (except
              logging) and return None."
"def move_right(self, keep_anchor=False, nb_chars=1):
        
        text_cursor = self._editor.textCursor()
        text_cursor.movePosition(
            text_cursor.Right, text_cursor.KeepAnchor if keep_anchor else
            text_cursor.MoveAnchor, nb_chars)
        self._editor.setTextCursor(text_cursor)","Moves the cursor on the right.

        :param keep_anchor: True to keep anchor (to select text) or False to
            move the anchor (no selection)
        :param nb_chars: Number of characters to move."
"def add_index(self, mode, blob_id, path):
        
        self.command_exec([, , , mode, blob_id, path])","Add new entry to the current index
        :param tree: 
        :return:"
"def get_sequence_rule_enabler_form(self, *args, **kwargs):
        
        
        
        
        if isinstance(args[-1], list) or  in kwargs:
            return self.get_sequence_rule_enabler_form_for_create(*args, **kwargs)
        else:
            return self.get_sequence_rule_enabler_form_for_update(*args, **kwargs)",Pass through to provider SequenceRuleEnablerAdminSession.get_sequence_rule_enabler_form_for_update
"def do_reverse(value):
    
    if isinstance(value, string_types):
        return value[::-1]
    try:
        return reversed(value)
    except TypeError:
        try:
            rv = list(value)
            rv.reverse()
            return rv
        except TypeError:
            raise FilterArgumentError()","Reverse the object or return an iterator that iterates over it the other
    way round."
"def parse_quadrant_measurement(quad_azimuth):
    
    def rotation_direction(first, second):
        return np.cross(_azimuth2vec(first), _azimuth2vec(second))

    
    quad_azimuth = quad_azimuth.strip()
    try:
        first_dir = quadrantletter_to_azimuth(quad_azimuth[0].upper())
        sec_dir = quadrantletter_to_azimuth(quad_azimuth[-1].upper())
    except KeyError:
        raise ValueError(.format(quad_azimuth))

    angle = float(quad_azimuth[1:-1])

    
    direc = rotation_direction(first_dir, sec_dir)
    azi = first_dir + direc * angle

    
    if abs(direc) < 0.9:
        raise ValueError(.format(quad_azimuth))

    
    if azi < 0:
        azi += 360
    elif azi > 360:
        azi -= 360

    return azi","Parses a quadrant measurement of the form ""AxxB"", where A and B are cardinal
    directions and xx is an angle measured relative to those directions.

    In other words, it converts a measurement such as E30N into an azimuth of
    60 degrees, or W10S into an azimuth of 260 degrees.

    For ambiguous quadrant measurements such as ""N30S"", a ValueError is raised.

    Parameters
    -----------
    quad_azimuth : string
        An azimuth measurement in quadrant form.

    Returns
    -------
    azi : float
        An azimuth in degrees clockwise from north.

    See Also
    --------
    parse_azimuth"
"def do_scheduled_update(self, action, **kwargs):
        

        action = getattr(self, action, None)
        if callable(action):
            return action(**kwargs)
        else:
            for k, v in kwargs.items():
                setattr(self, k, v)
            self.save()","Do the actual update.

        action: if provided it will be looked up
        on the implementing class and called with
        **kwargs. If action is not provided each k/v pair
        in kwargs will be set on self and then self
        is saved.

        kwargs: any other you passed for this update
        passed along to whichever method performs
        the update."
"def get_task_module(feature):
    
    try:
        importlib.import_module(feature)
    except ImportError:
        raise FeatureNotFound(feature)

    tasks_module = None

    
    
    try:
        tasks_module = importlib.import_module(feature + )
    except ImportError:
        
        pass

    try:
        tasks_module = importlib.import_module(feature + )
    except ImportError:
        
        pass

    return tasks_module","Return imported task module of feature.

    This function first tries to import the feature and raises FeatureNotFound
    if that is not possible.
    Thereafter, it looks for a submodules called ``apetasks`` and ``tasks`` in that order.
    If such a submodule exists, it is imported and returned.

    :param feature: name of feature to fet task module for.
    :raises: FeatureNotFound if feature_module could not be imported.
    :return: imported module containing the ape tasks of feature or None,
                if module cannot be imported."
"def _is_ndb(self):
        
        
        
        
        if isinstance(self._model, type):
            if _NDB_MODEL is not None and issubclass(self._model, _NDB_MODEL):
                return True
            elif issubclass(self._model, db.Model):
                return False

        raise TypeError(
            .format(self._model))","Determine whether the model of the instance is an NDB model.

        Returns:
            Boolean indicating whether or not the model is an NDB or DB model."
"def get_business_rule_output(self, hosts, services, macromodulations, timeperiods):
        
        
        got_business_rule = getattr(self, , False)
        
        if got_business_rule is False or self.business_rule is None:
            return """"
        
        output_template = self.business_rule_output_template
        if not output_template:
            return """"
        macroresolver = MacroResolver()

        
        elts = re.findall(r""\$\((.*)\)\$"", output_template)
        if not elts:
            child_template_string = """"
        else:
            child_template_string = elts[0]

        
        children_output = """"
        ok_count = 0
        
        items = self.business_rule.list_all_elements()
        for item_uuid in items:
            if item_uuid in hosts:
                item = hosts[item_uuid]
            elif item_uuid in services:
                item = services[item_uuid]

            
            
            if item.last_hard_state_id == 0:
                ok_count += 1
                continue
            data = item.get_data_for_checks(hosts)
            children_output += macroresolver.resolve_simple_macros_in_string(child_template_string,
                                                                             data,
                                                                             macromodulations,
                                                                             timeperiods)

        if ok_count == len(items):
            children_output = ""all checks were successful.""

        
        template_string = re.sub(r""\$\(.*\)\$"", children_output, output_template)
        data = self.get_data_for_checks(hosts)
        output = macroresolver.resolve_simple_macros_in_string(template_string, data,
                                                               macromodulations, timeperiods)
        return output.strip()","Returns a status string for business rules based items formatted
        using business_rule_output_template attribute as template.

        The template may embed output formatting for itself, and for its child
        (dependent) items. Child format string is expanded into the $( and )$,
        using the string between brackets as format string.

        Any business rule based item or child macro may be used. In addition,
        the $STATUS$, $SHORTSTATUS$ and $FULLNAME$ macro which name is common
        to hosts and services may be used to ease template writing.

        Caution: only children in state not OK are displayed.

        Example:
          A business rule with a format string looking like
              ""$STATUS$ [ $($TATUS$: $HOSTNAME$,$SERVICEDESC$ )$ ]""
          Would return
              ""CRITICAL [ CRITICAL: host1,srv1 WARNING: host2,srv2  ]""

        :param hosts: Hosts object to look for objects
        :type hosts: alignak.objects.host.Hosts
        :param services: Services object to look for objects
        :type services: alignak.objects.service.Services
        :param macromodulations: Macromodulations object to look for objects
        :type macromodulations: alignak.objects.macromodulation.Macromodulations
        :param timeperiods: Timeperiods object to look for objects
        :type timeperiods: alignak.objects.timeperiod.Timeperiods
        :return: status for business rules
        :rtype: str"
"def update_gradebook(self, gradebook_form):
        
        
        
        if self._catalog_session is not None:
            return self._catalog_session.update_catalog(catalog_form=gradebook_form)
        collection = JSONClientValidated(,
                                         collection=,
                                         runtime=self._runtime)
        if not isinstance(gradebook_form, ABCGradebookForm):
            raise errors.InvalidArgument()
        if not gradebook_form.is_for_update():
            raise errors.InvalidArgument()
        try:
            if self._forms[gradebook_form.get_id().get_identifier()] == UPDATED:
                raise errors.IllegalState()
        except KeyError:
            raise errors.Unsupported()
        if not gradebook_form.is_valid():
            raise errors.InvalidArgument()
        collection.save(gradebook_form._my_map)  

        self._forms[gradebook_form.get_id().get_identifier()] = UPDATED

        
        return objects.Gradebook(osid_object_map=gradebook_form._my_map, runtime=self._runtime, proxy=self._proxy)","Updates an existing gradebook.

        arg:    gradebook_form (osid.grading.GradebookForm): the form
                containing the elements to be updated
        raise:  IllegalState - ``gradebook_form`` already used in an
                update transaction
        raise:  InvalidArgument - the form contains an invalid value
        raise:  NullArgument - ``gradebook_form`` is ``null``
        raise:  OperationFailed - unable to complete request
        raise:  PermissionDenied - authorization failure
        raise:  Unsupported - ``gradebook_form did not originate from
                get_gradebook_form_for_update()``
        *compliance: mandatory -- This method must be implemented.*"
"def deserialise(self, element_json: str) -> Element:
        

        return self.deserialise_dict(json.loads(element_json))","Deserialises the given JSON into an element.

        >>> json = '{""element"": ""string"", ""content"": ""Hello""'
        >>> JSONDeserialiser().deserialise(json)
        String(content='Hello')"
"def rebase(self, text, char=):
        
        regexp = re.compile(r % .join(self.collection),
                            re.IGNORECASE | re.UNICODE)

        def replace(m):
            word = m.group(1)
            return char * len(word)

        return regexp.sub(replace, text)",Rebases text with stop words removed.
"def _get_vispy_font_filename(face, bold, italic):
    
    name = face + 
    name +=  if not bold and not italic else 
    name +=  if bold else 
    name +=  if italic else 
    name += 
    return load_data_file( % name)",Fetch a remote vispy font
"def get_size(cls):
        
        return sum([getattr(cls, name).length
                    for name in cls.get_fields_names()])","Total byte size of fields in this structure => total byte size of
            the structure on the file"
"def find_or_new(self, id, columns=None):
        
        if columns is None:
            columns = [""*""]

        instance = self._query.find(id, columns)

        if instance is None:
            instance = self._related.new_instance()
            instance.set_attribute(self.get_plain_foreign_key(), self.get_parent_key())

        return instance","Find a model by its primary key or return new instance of the related model.

        :param id: The primary key
        :type id: mixed

        :param columns:  The columns to retrieve
        :type columns: list

        :rtype: Collection or Model"
"def ftp_download(ftp, ftp_path, local_path):
    
    with open(local_path, ) as _f:
        ftp.retrbinary( % ftp_path, _f.write)","Download the master database
    :param ftp: ftp connection
    :param ftp_path: path to file on the ftp server
    :param local_path: local path to download file
    :return:"
"def safe_remove(path):
    
    if not os.path.exists(path):
        return

    try:
        if os.path.isdir(path) and not os.path.islink(path):
            shutil.rmtree(path)
        else:
            os.remove(path)
    except OSError:
        if os.path.exists(path):
            raise","Safely remove the given file or directory.

    Works in a multithreaded scenario."
"async def create(
            cls, name: str, architecture: str, content: io.IOBase, *,
            title: str = """",
            filetype: BootResourceFileType = BootResourceFileType.TGZ,
            chunk_size=(1 << 22), progress_callback=None):
        
        if  not in name:
            raise ValueError(
                ""name must be in format os/release; missing "")
        if  not in architecture:
            raise ValueError(
                ""architecture must be in format arch/subarch; missing "")
        if not content.readable():
            raise ValueError(""content must be readable"")
        elif not content.seekable():
            raise ValueError(""content must be seekable"")
        if chunk_size <= 0:
            raise ValueError(
                ""chunk_size must be greater than 0, not %d"" % chunk_size)

        size, sha256 = calc_size_and_sha265(content, chunk_size)
        resource = cls._object(await cls._handler.create(
            name=name, architecture=architecture, title=title,
            filetype=filetype.value, size=str(size), sha256=sha256))
        newest_set = max(resource.sets, default=None)
        assert newest_set is not None
        resource_set = resource.sets[newest_set]
        assert len(resource_set.files) == 1
        rfile = list(resource_set.files.values())[0]
        if rfile.complete:
            
            return resource
        else:
            
            await cls._upload_chunks(
                rfile, content, chunk_size, progress_callback)
            return cls._object.read(resource.id)","Create a `BootResource`.

        Creates an uploaded boot resource with `content`. The `content` is
        uploaded in chunks of `chunk_size`. `content` must be seekable as the
        first pass through the `content` will calculate the size and sha256
        value then the second pass will perform the actual upload.

        :param name: Name of the boot resource. Must be in format 'os/release'.
        :type name: `str`
        :param architecture: Architecture of the boot resource. Must be in
            format 'arch/subarch'.
        :type architecture: `str`
        :param content: Content of the boot resource.
        :type content: `io.IOBase`
        :param title: Title of the boot resource.
        :type title: `str`
        :param filetype: Type of file in content.
        :type filetype: `str`
        :param chunk_size: Size in bytes to upload to MAAS in chunks.
            (Default is 4 MiB).
        :type chunk_size: `int`
        :param progress_callback: Called to inform the current progress of the
            upload. One argument is passed with the progress as a precentage.
            If the resource was already complete and no content
            needed to be uploaded then this callback will never be called.
        :type progress_callback: Callable
        :returns: Create boot resource.
        :rtype: `BootResource`."
"def process_files(self, path, recursive=False):
        
        self._logger.info(, path)

        for (path, file) in files_generator(path, recursive):
            if not file.endswith(BATCH_EXTENSION):
                self.process_file(os.path.join(path, file))","Apply normalizations over all files in the given directory.

        Iterate over all files in a given directory. Normalizations
        will be applied to each file, storing the result in a new file.
        The extension for the new file will be the one defined in
        BATCH_EXTENSION.

        Args:
            path: Path to the directory.
            recursive: Whether to find files recursively or not."
"def accept(self, node, **kwargs):
        
        if node is None:
            return
        
        for v in self.visitors:
            v.enter(node)
        
        name =  + node.__class__.__name__
        fn = getattr(self, name, self.default_accept)
        r = fn(node, **kwargs)
        
        for v in self.visitors:
            v.leave(node)
        
        return r","Invoke the visitors before and after decending down the tree. 
        The walker will also try to invoke a method matching the pattern 
        *accept_<type name>*, where <type name> is the name of the accepted
        *node*."
"def cmd_condition_yaw(self, args):
        
        if ( len(args) != 3):
            print(""Usage: yaw ANGLE ANGULAR_SPEED MODE:[0 absolute / 1 relative]"")
            return

        if (len(args) == 3):
            angle = float(args[0])
            angular_speed = float(args[1])
            angle_mode = float(args[2])
            print(""ANGLE %s"" % (str(angle)))

            self.master.mav.command_long_send(
                self.settings.target_system,  
                mavutil.mavlink.MAV_COMP_ID_SYSTEM_CONTROL, 
                mavutil.mavlink.MAV_CMD_CONDITION_YAW, 
                0, 
                angle, 
                angular_speed, 
                0, 
                angle_mode, 
                0, 
                0, 
                0)",yaw angle angular_speed angle_mode
"def optimize(self, timeSeries, forecastingMethods=None, startingPercentage=0.0, endPercentage=100.0):
        

        if forecastingMethods is None or len(forecastingMethods) == 0:
            raise ValueError(""forecastingMethods cannot be empty."")

        self._startingPercentage = startingPercentage
        self._endPercentage      = endPercentage

        results = []
        for forecastingMethod in forecastingMethods:
            results.append([forecastingMethod] + self.optimize_forecasting_method(timeSeries, forecastingMethod))

        
        bestForecastingMethod = min(results, key=lambda item: item[1].get_error(self._startingPercentage, self._endPercentage))

        for parameter in bestForecastingMethod[2]:
            bestForecastingMethod[0].set_parameter(parameter, bestForecastingMethod[2][parameter])

        return bestForecastingMethod","Runs the optimization of the given TimeSeries.

        :param TimeSeries timeSeries:    TimeSeries instance that requires an optimized forecast.
        :param list forecastingMethods:    List of forecastingMethods that will be used for optimization.
        :param float startingPercentage: Defines the start of the interval. This has to be a value in [0.0, 100.0].
            It represents the value, where the error calculation should be started.
            25.0 for example means that the first 25% of all calculated errors will be ignored.
        :param float endPercentage:    Defines the end of the interval. This has to be a value in [0.0, 100.0].
            It represents the value, after which all error values will be ignored. 90.0 for example means that
            the last 10% of all local errors will be ignored.

        :return:    Returns the optimized forecasting method, the corresponding error measure and the forecasting methods
            parameters.
        :rtype:     [BaseForecastingMethod, BaseErrorMeasure, Dictionary]

        :raise:    Raises a :py:exc:`ValueError` ValueError if no forecastingMethods is empty."
"def get_module_names(package_path, pattern=""lazy_*.py*""):
  
  package_contents = glob(os.path.join(package_path[0], pattern))
  relative_path_names = (os.path.split(name)[1] for name in package_contents)
  no_ext_names = (os.path.splitext(name)[0] for name in relative_path_names)
  return sorted(set(no_ext_names))","All names in the package directory that matches the given glob, without
  their extension. Repeated names should appear only once."
"def capability_installed(name,
                         source=None,
                         limit_access=False,
                         image=None,
                         restart=False):
    
    ret = {: name,
           : True,
           : ,
           : {}}

    old = __salt__[]()

    if name in old:
        ret[] = .format(name)
        return ret

    if __opts__[]:
        ret[][] = .format(name)
        ret[] = None
        return ret

    
    status = __salt__[](
        name, source, limit_access, image, restart)

    if status[] not in [0, 1641, 3010]:
        ret[] = \
            .format(name, status[])
        ret[] = False

    new = __salt__[]()
    changes = salt.utils.data.compare_lists(old, new)

    if changes:
        ret[] = .format(name)
        ret[] = status
        ret[][] = changes

    return ret","Install a DISM capability

    Args:
        name (str): The capability to install
        source (str): The optional source of the capability
        limit_access (bool): Prevent DISM from contacting Windows Update for
            online images
        image (Optional[str]): The path to the root directory of an offline
            Windows image. If `None` is passed, the running operating system is
            targeted. Default is None.
        restart (Optional[bool]): Reboot the machine if required by the install

    Example:
        Run ``dism.available_capabilities`` to get a list of available
        capabilities. This will help you get the proper name to use.

        .. code-block:: yaml

            install_dotnet35:
              dism.capability_installed:
                - name: NetFX3~~~~"
"def reset(self):
        
        self.overall = {
            : 0.0,
            : 0.0,
            : 0.0
        }

        self.scene_wise = {}
        for label in self.scene_label_list:
            self.scene_wise[label] = {
                : 0.0,
                : 0.0,
                : 0.0
            }",Reset internal state
"def __getOrganizations(self, web):
        
        orgsElements = web.find_all(""a"", {""class"": ""avatar-group-item""})
        self.organizations = len(orgsElements)","Scrap the number of organizations from a GitHub profile.

        :param web: parsed web.
        :type web: BeautifulSoup node."
"def _load_mtdata(gzfile):
    
    path = os.path.join(os.path.dirname(__file__), , , gzfile)
    return np.loadtxt(gzip.open(path))","Simple helper function that finds the test data in the directory tree
    and loads it using :func:`gzip.open` and :func:`numpy.loadtxt`.

    :param gzfile: Filename
    :type gzfile: str
    :returns: data
    :rtype: numpy.ndarray"
"def enqueue_download(self, resource):
        
        worker = self.pick_sticky(resource.url_string)
        coro = worker.enqueue(enums.Task.DOWNLOAD, (resource,))
        asyncio.ensure_future(coro)","Enqueue the download of the given foreign resource.

        Deprecated: Use async version instead"
"def local_predict(training_dir, data):
  
  
  from .prediction import predict as predict_module

  
  tmp_dir = tempfile.mkdtemp()
  _, input_file_path = tempfile.mkstemp(dir=tmp_dir, suffix=,
                                        prefix=)

  try:
    if isinstance(data, pd.DataFrame):
      data.to_csv(input_file_path, header=False, index=False)
    else:
      with open(input_file_path, ) as f:
        for line in data:
          f.write(line + )

    model_dir = os.path.join(training_dir, )
    if not file_io.file_exists(model_dir):
      raise ValueError()

    cmd = [,
            % input_file_path,
            % model_dir,
            % tmp_dir,
           ,
           ,
           ,
           ]

    
    runner_results = predict_module.main(cmd)
    runner_results.wait_until_finish()

    
    schema_file = os.path.join(tmp_dir, )
    with open(schema_file, ) as f:
      schema = json.loads(f.read())

    
    errors_file = glob.glob(os.path.join(tmp_dir, ))
    if errors_file and os.path.getsize(errors_file[0]) > 0:
      print()
      with open(errors_file[0], ) as f:
        text = f.read()
        print(text)

    
    prediction_file = glob.glob(os.path.join(tmp_dir, ))
    if not prediction_file:
      raise FileNotFoundError()
    predictions = pd.read_csv(prediction_file[0],
                              header=None,
                              names=[col[] for col in schema])
    return predictions
  finally:
    shutil.rmtree(tmp_dir)","Runs local prediction on the prediction graph.

  Runs local prediction and returns the result in a Pandas DataFrame. For
  running prediction on a large dataset or saving the results, run
  local_batch_prediction or batch_prediction. Input data should fully match
  the schema that was used at training, except the target column should not
  exist.

  Args:
    training_dir: local path to the trained output folder.
    data: List of csv strings or a Pandas DataFrame that match the model schema.

  Raises:
    ValueError: if training_dir does not contain the folder 'model'.
    FileNotFoundError: if the prediction data is not found."
"def each(self, *funcs):
        

        funcs = list(map(_make_callable, funcs))

        if len(funcs) == 1:
            return Collection(map(funcs[0], self._items))

        tupler = lambda item: Scalar(
            tuple(_unwrap(func(item)) for func in funcs))
        return Collection(map(tupler, self._items))","Call `func` on each element in the collection.

        If multiple functions are provided, each item
        in the output will be a tuple of each
        func(item) in self.

        Returns a new Collection.

        Example:

            >>> col = Collection([Scalar(1), Scalar(2)])
            >>> col.each(Q * 10)
            Collection([Scalar(10), Scalar(20)])
            >>> col.each(Q * 10, Q - 1)
            Collection([Scalar((10, 0)), Scalar((20, 1))])"
"def run(self, *args):
        
        if self.running:
            return self

        self._mut_finished(False)  
        self._mut_running(True)

        stream = self.target(*args)

        
        def subr():
            self._mut_running(True)
            try:
                for each in stream:
                    self._product = each
                    desc = self.descriptor_mapping(each)
                    event = self.events.get(desc)
                    if event:
                        event(self, each, globals)
                self._mut_finished(True)
            except ThreadExit:
                pass
            finally:
                self._mut_running(False)

        self._thread = thread = threading.Thread(target=subr, args=())
        thread.start()
        return self",You can choose whether to use lock method when running threads.
"def map_set_properties(
    m: tcod.map.Map, x: int, y: int, isTrans: bool, isWalk: bool
) -> None:
    
    lib.TCOD_map_set_properties(m.map_c, x, y, isTrans, isWalk)","Set the properties of a single cell.

    .. note::
        This function is slow.
    .. deprecated:: 4.5
        Use :any:`tcod.map.Map.transparent` and :any:`tcod.map.Map.walkable`
        arrays to set these properties."
