# 強化方案

1. 換掉CodeBERT baseline，找到更好的embedding模型（都用 mean pooling）
嘗試使用不同的embedding模型，選擇其中Kaggle分數最高的（由於train_queries.csv作為訓練資料，沒有本地驗證資料）
	microsoft/codebert-base（baseline, 目前已試驗過）
	microsoft/graphcodebert-base
	Salesforce/codet5p-220m
	microsoft/unixcoder-base
先嘗試不微調的結果，若分數不高再嘗試進行微調


2. Hard Negative Mining 迭代
	* 先用TF-IDF找top-K（K = 50）個候選答案（為了讓情境更貼合真實環境，負樣本從code_snippets抽取，那是test_queries的語料庫，與用來訓練的train_queries的code樣本不重合）
	* 從top-K裡採樣作為training negative，可以取top-1（最hard），或取top-[1..5]（mix hard），或隨機挑top-K中的1個以增加多樣性
      混合策略：每個 batch 同時用部分 random negatives（簡單）+ 部分 tfidf hard negatives（困難）
	* 每個anchor建議至少1個 negative（最少）；若能給5~10個候選供模型在loss中使用，效果更好（但會增加運算）


